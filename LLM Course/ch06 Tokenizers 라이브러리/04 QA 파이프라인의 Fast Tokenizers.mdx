# QA íŒŒì´í”„ë¼ì¸ì˜ ë¹ ë¥¸ í† í¬ë‚˜ì´ì € (Fast tokenizers in the QA pipeline)

ì´ì œ **`question-answering`** íŒŒì´í”„ë¼ì¸ì„ ìì„¸íˆ ì‚´í´ë³´ê³ , ì´ì „ ì„¹ì…˜ì—ì„œ ê·¸ë£¹í™”ëœ ê°œì²´ì— ëŒ€í•´ í–ˆë˜ ê²ƒì²˜ëŸ¼ ì˜¤í”„ì…‹ì„ í™œìš©í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë‹¹ë©´í•œ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì„ ê°€ì ¸ì˜¤ëŠ” ë°©ë²•ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ ì˜ë¦´ ìˆ˜ ìˆëŠ” ë§¤ìš° ê¸´ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë³¼ ê²ƒì…ë‹ˆë‹¤. ì§ˆë¬¸ ë‹µë³€ ì‘ì—…ì— ê´€ì‹¬ì´ ì—†ë‹¤ë©´ ì´ ì„¹ì…˜ì„ ê±´ë„ˆë›¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<Youtube id="_wxyB3j3mk4"/>

## `question-answering` íŒŒì´í”„ë¼ì¸ ì‚¬ìš©í•˜ê¸° (Using the `question-answering` pipeline)

[ì±•í„° 1](/course/chapter1)ì—ì„œ ë³´ì•˜ë“¯ì´, ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì„ ì–»ê¸° ìœ„í•´ **`question-answering`** íŒŒì´í”„ë¼ì¸ì„ ë‹¤ìŒê³¼ ê°™ì´ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```py
from transformers import pipeline

question_answerer = pipeline("question-answering")
context = """
ğŸ¤— Transformers is backed by the three most popular deep learning libraries â€” Jax, PyTorch, and TensorFlow â€” with a seamless integration
between them. It's straightforward to train your models with one before loading them for inference with the other.
"""
question = "Which deep learning libraries back ğŸ¤— Transformers?"
question_answerer(question=question, context=context)
```

```python out
{'score': 0.97773,
Â 'start': 78,
Â 'end': 105,
Â 'answer': 'Jax, PyTorch and TensorFlow'}
```

ìµœëŒ€ ê¸¸ì´ê°€ í—ˆìš©í•˜ëŠ” ê²ƒë³´ë‹¤ ê¸´ í…ìŠ¤íŠ¸ë¥¼ ìë¥´ê³  ë¶„í• í•  ìˆ˜ ì—†ëŠ” ë‹¤ë¥¸ íŒŒì´í”„ë¼ì¸ê³¼ ë‹¬ë¦¬(ë”°ë¼ì„œ ë¬¸ì„œ ëì˜ ì •ë³´ë¥¼ ë†“ì¹  ìˆ˜ ìˆìŒ), ì´ íŒŒì´í”„ë¼ì¸ì€ ë§¤ìš° ê¸´ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìœ¼ë©° ë‹µì´ ëì— ìˆë”ë¼ë„ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì„ ë°˜í™˜í•©ë‹ˆë‹¤:

```py
long_context = """
ğŸ¤— Transformers: State of the Art NLP

ğŸ¤— Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,
question answering, summarization, translation, text generation and more in over 100 languages.
Its aim is to make cutting-edge NLP easier to use for everyone.

ğŸ¤— Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and
then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and
can be modified to enable quick research experiments.

Why should I use transformers?

1. Easy-to-use state-of-the-art models:
Â  - High performance on NLU and NLG tasks.
Â  - Low barrier to entry for educators and practitioners.
Â  - Few user-facing abstractions with just three classes to learn.
Â  - A unified API for using all our pretrained models.
Â  - Lower compute costs, smaller carbon footprint:

2. Researchers can share trained models instead of always retraining.
Â  - Practitioners can reduce compute time and production costs.
Â  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.

3. Choose the right framework for every part of a model's lifetime:
Â  - Train state-of-the-art models in 3 lines of code.
Â  - Move a single model between TF2.0/PyTorch frameworks at will.
Â  - Seamlessly pick the right framework for training, evaluation and production.

4. Easily customize a model or an example to your needs:
Â  - We provide examples for each architecture to reproduce the results published by its original authors.
Â  - Model internals are exposed as consistently as possible.
Â  - Model files can be used independently of the library for quick experiments.

ğŸ¤— Transformers is backed by the three most popular deep learning libraries â€” Jax, PyTorch and TensorFlow â€” with a seamless integration
between them. It's straightforward to train your models with one before loading them for inference with the other.
"""
question_answerer(question=question, context=long_context)
```

```python out
{'score': 0.97149,
Â 'start': 1892,
Â 'end': 1919,
Â 'answer': 'Jax, PyTorch and TensorFlow'}
```

ì´ ëª¨ë“  ê²ƒì„ ì–´ë–»ê²Œ ìˆ˜í–‰í•˜ëŠ”ì§€ ì‚´í´ë´…ì‹œë‹¤!

## ì§ˆë¬¸ ë‹µë³€ì„ ìœ„í•œ ëª¨ë¸ ì‚¬ìš© (Using a model for question answering)

ë‹¤ë¥¸ íŒŒì´í”„ë¼ì¸ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ, ì…ë ¥ì„ í† í°í™”í•œ ë‹¤ìŒ ëª¨ë¸ì„ í†µê³¼ì‹œí‚¤ëŠ” ê²ƒìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤. **`question-answering`** íŒŒì´í”„ë¼ì¸ì— ëŒ€í•´ ê¸°ë³¸ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ì²´í¬í¬ì¸íŠ¸ëŠ” [`distilbert-base-cased-distilled-squad`](https://huggingface.co/distilbert-base-cased-distilled-squad)ì…ë‹ˆë‹¤(ì´ë¦„ì˜ "squad"ëŠ” ëª¨ë¸ì´ ë¯¸ì„¸ ì¡°ì •ëœ ë°ì´í„°ì…‹ì—ì„œ ìœ ë˜í•©ë‹ˆë‹¤. [ì±•í„° 7](/course/chapter7/7)ì—ì„œ SQuAD ë°ì´í„°ì…‹ì— ëŒ€í•´ ë” ìì„¸íˆ ì´ì•¼ê¸°í•  ê²ƒì…ë‹ˆë‹¤):

```py
from transformers import AutoTokenizer, AutoModelForQuestionAnswering

model_checkpoint = "distilbert-base-cased-distilled-squad"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)

inputs = tokenizer(question, context, return_tensors="pt")
outputs = model(**inputs)
```

ì§ˆë¬¸ì„ ë¨¼ì € ë‘ê³  ì§ˆë¬¸ê³¼ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìŒìœ¼ë¡œ í† í°í™”í•œë‹¤ëŠ” ì ì— ìœ ì˜í•˜ì‹­ì‹œì˜¤.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/question_tokens.svg" alt="An example of tokenization of question and context"/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/question_tokens-dark.svg" alt="An example of tokenization of question and context"/>
</div>

ì§ˆë¬¸ ë‹µë³€ì„ ìœ„í•œ ëª¨ë¸ì€ ì§€ê¸ˆê¹Œì§€ ìš°ë¦¬ê°€ ë³´ì•˜ë˜ ëª¨ë¸ê³¼ ì•½ê°„ ë‹¤ë¥´ê²Œ ì‘ë™í•©ë‹ˆë‹¤. ìœ„ì˜ ê·¸ë¦¼ì„ ì˜ˆë¡œ ë“¤ë©´, ëª¨ë¸ì€ ë‹µë³€ì´ ì‹œì‘ë˜ëŠ” í† í°ì˜ ì¸ë±ìŠ¤(ì—¬ê¸°ì„œëŠ” 21)ì™€ ë‹µë³€ì´ ëë‚˜ëŠ” í† í°ì˜ ì¸ë±ìŠ¤(ì—¬ê¸°ì„œëŠ” 24)ë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ í›ˆë ¨ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ê²ƒì´ ì´ ëª¨ë¸ë“¤ì´ í•˜ë‚˜ì˜ ë¡œì§“ í…ì„œê°€ ì•„ë‹Œ ë‘ ê°œì˜ ë¡œì§“ í…ì„œë¥¼ ë°˜í™˜í•˜ëŠ” ì´ìœ ì…ë‹ˆë‹¤: í•˜ë‚˜ëŠ” ë‹µë³€ì˜ ì‹œì‘ í† í°ì— í•´ë‹¹í•˜ëŠ” ë¡œì§“ìš©ì´ê³ , ë‹¤ë¥¸ í•˜ë‚˜ëŠ” ë‹µë³€ì˜ ë í† í°ì— í•´ë‹¹í•˜ëŠ” ë¡œì§“ìš©ì…ë‹ˆë‹¤. ì´ ê²½ìš° 66ê°œì˜ í† í°ì„ í¬í•¨í•˜ëŠ” ì…ë ¥ì´ í•˜ë‚˜ë§Œ ìˆìœ¼ë¯€ë¡œ ë‹¤ìŒì„ ì–»ìŠµë‹ˆë‹¤:

```py
start_logits = outputs.start_logits
end_logits = outputs.end_logits
print(start_logits.shape, end_logits.shape)
```

```python out
torch.Size([1, 66]) torch.Size([1, 66])
```

ì´ëŸ¬í•œ ë¡œì§“ì„ í™•ë¥ ë¡œ ë³€í™˜í•˜ë ¤ë©´ ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ë¥¼ ì ìš©í•´ì•¼ í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê·¸ ì „ì— ì»¨í…ìŠ¤íŠ¸ì˜ ì¼ë¶€ê°€ ì•„ë‹Œ ì¸ë±ìŠ¤ë¥¼ ë§ˆìŠ¤í‚¹í•´ì•¼ í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ì…ë ¥ì€ **`[CLS] question [SEP] context [SEP]`**ì´ë¯€ë¡œ, ì§ˆë¬¸ì˜ í† í°ê³¼ **`[SEP]`** í† í°ì„ ë§ˆìŠ¤í‚¹í•´ì•¼ í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì¼ë¶€ ëª¨ë¸ì€ ë‹µë³€ì´ ì»¨í…ìŠ¤íŠ¸ì— ì—†ìŒì„ ë‚˜íƒ€ë‚´ëŠ” ë° ì‚¬ìš©í•˜ë¯€ë¡œ **`[CLS]`** í† í°ì€ ìœ ì§€í•©ë‹ˆë‹¤.

ì†Œí”„íŠ¸ë§¥ìŠ¤ë¥¼ ì ìš©í•  ê²ƒì´ë¯€ë¡œ, ë§ˆìŠ¤í‚¹í•˜ë ¤ëŠ” ë¡œì§“ì„ í° ìŒìˆ˜ë¡œ ëŒ€ì²´í•˜ê¸°ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” **`-10000`**ì„ ì‚¬ìš©í•©ë‹ˆë‹¤:

```py
import torch

sequence_ids = inputs.sequence_ids()
# ì»¨í…ìŠ¤íŠ¸ì˜ í† í°ì„ ì œì™¸í•œ ëª¨ë“  ê²ƒì„ ë§ˆìŠ¤í‚¹
mask = [i != 1 for i in sequence_ids]
# [CLS] í† í° ë§ˆìŠ¤í‚¹ í•´ì œ
mask[0] = False
mask = torch.tensor(mask)[None]

start_logits[mask] = -10000
end_logits[mask] = -10000
```

ì´ì œ ì˜ˆì¸¡í•˜ê³  ì‹¶ì§€ ì•Šì€ ìœ„ì¹˜ì— í•´ë‹¹í•˜ëŠ” ë¡œì§“ì„ ì œëŒ€ë¡œ ë§ˆìŠ¤í‚¹í–ˆìœ¼ë¯€ë¡œ, ì†Œí”„íŠ¸ë§¥ìŠ¤ë¥¼ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```py
start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)[0]
end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)[0]
```

ì´ ë‹¨ê³„ì—ì„œ ì‹œì‘ ë° ë í™•ë¥ ì˜ argmaxë¥¼ ì·¨í•  ìˆ˜ ìˆì§€ë§Œ, ì‹œì‘ ì¸ë±ìŠ¤ê°€ ë ì¸ë±ìŠ¤ë³´ë‹¤ ì»¤ì§€ëŠ” ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ëª‡ ê°€ì§€ ì˜ˆë°© ì¡°ì¹˜ë¥¼ ë” ì·¨í•´ì•¼ í•©ë‹ˆë‹¤. **`start_index <= end_index`**ì¸ ê° ê°€ëŠ¥í•œ **`start_index`**ì™€ **`end_index`**ì˜ í™•ë¥ ì„ ê³„ì‚°í•œ ë‹¤ìŒ, ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ íŠœí”Œ **`(start_index, end_index)`**ë¥¼ ì·¨í•  ê²ƒì…ë‹ˆë‹¤.

"ë‹µë³€ì´ **`start_index`**ì—ì„œ ì‹œì‘"í•˜ëŠ” ì´ë²¤íŠ¸ì™€ "ë‹µë³€ì´ **`end_index`**ì—ì„œ ë"ë‚˜ëŠ” ì´ë²¤íŠ¸ê°€ ë…ë¦½ì ì´ë¼ê³  ê°€ì •í•˜ë©´, ë‹µë³€ì´ **`start_index`**ì—ì„œ ì‹œì‘í•˜ê³  **`end_index`**ì—ì„œ ëë‚  í™•ë¥ ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

$$\mathrm{start\_probabilities}[\mathrm{start\_index}] \times \mathrm{end\_probabilities}[\mathrm{end\_index}]$$Â 

ë”°ë¼ì„œ ëª¨ë“  ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ë ¤ë©´ **`start_index <= end_index`**ì¸ ëª¨ë“  ê³±ì…ˆ \\(\mathrm{start\_probabilities}[\mathrm{start\_index}] \times \mathrm{end\_probabilities}[\mathrm{end\_index}]\\)ë¥¼ ê³„ì‚°í•˜ê¸°ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤.

ë¨¼ì € ê°€ëŠ¥í•œ ëª¨ë“  ê³±ì…ˆì„ ê³„ì‚°í•´ ë´…ì‹œë‹¤:

```py
scores = start_probabilities[:, None] * end_probabilities[None, :]
```

ê·¸ëŸ° ë‹¤ìŒ **`start_index > end_index`**ì¸ ê°’ì„ **`0`**ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ë§ˆìŠ¤í‚¹í•©ë‹ˆë‹¤(ë‹¤ë¥¸ í™•ë¥ ì€ ëª¨ë‘ ì–‘ìˆ˜ì…ë‹ˆë‹¤). **`torch.triu()`** í•¨ìˆ˜ëŠ” ì¸ìˆ˜ë¡œ ì „ë‹¬ëœ 2D í…ì„œì˜ ìƒìœ„ ì‚¼ê° ë¶€ë¶„ì„ ë°˜í™˜í•˜ë¯€ë¡œ, ìš°ë¦¬ë¥¼ ìœ„í•´ í•´ë‹¹ ë§ˆìŠ¤í‚¹ì„ ìˆ˜í–‰í•  ê²ƒì…ë‹ˆë‹¤:

```py
scores = torch.triu(scores)
```

ì´ì œ ìµœëŒ€ê°’ì˜ ì¸ë±ìŠ¤ë¥¼ ê°€ì ¸ì™€ì•¼ í•©ë‹ˆë‹¤. PyTorchëŠ” í‰íƒ„í™”ëœ í…ì„œì˜ ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ, **`start_index`**ì™€ **`end_index`**ë¥¼ ì–»ìœ¼ë ¤ë©´ ì •ìˆ˜ ë‚˜ëˆ„ê¸° **`//`** ë° ëª¨ë“ˆëŸ¬ìŠ¤ **`%`** ì—°ì‚°ì„ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤:

```py
max_index = scores.argmax().item()
start_index = max_index // scores.shape[1]
end_index = max_index % scores.shape[1]
print(scores[start_index, end_index])
```

ì•„ì§ ì™„ì „íˆ ì™„ë£Œë˜ì§€ ì•Šì•˜ì§€ë§Œ, ì ì–´ë„ ë‹µë³€ì— ëŒ€í•œ ì˜¬ë°”ë¥¸ ì ìˆ˜ëŠ” ì´ë¯¸ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤(ì´ì „ ì„¹ì…˜ì˜ ì²« ë²ˆì§¸ ê²°ê³¼ì™€ ë¹„êµí•˜ì—¬ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤):

```python out
0.97773
```

<Tip>

âœï¸ **ì§ì ‘ í•´ë³´ì„¸ìš”!** ê°€ì¥ ê°€ëŠ¥ì„±ì´ ë†’ì€ ë‹¤ì„¯ ê°€ì§€ ë‹µë³€ì— ëŒ€í•œ ì‹œì‘ ë° ë ì¸ë±ìŠ¤ë¥¼ ê³„ì‚°í•˜ì‹­ì‹œì˜¤.

</Tip>

í† í° ì¸¡ë©´ì—ì„œ ë‹µë³€ì˜ **`start_index`**ì™€ **`end_index`**ê°€ ìˆìœ¼ë¯€ë¡œ, ì´ì œ ì»¨í…ìŠ¤íŠ¸ì˜ ë¬¸ì ì¸ë±ìŠ¤ë¡œ ë³€í™˜í•˜ê¸°ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤. ì—¬ê¸°ì„œ ì˜¤í”„ì…‹ì´ ë§¤ìš° ìœ ìš©í•  ê²ƒì…ë‹ˆë‹¤. í† í° ë¶„ë¥˜ ì‘ì—…ì—ì„œ í–ˆë˜ ê²ƒì²˜ëŸ¼ ì˜¤í”„ì…‹ì„ ê°€ì ¸ì™€ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```py
inputs_with_offsets = tokenizer(question, context, return_offsets_mapping=True)
offsets = inputs_with_offsets["offset_mapping"]

start_char, _ = offsets[start_index]
_, end_char = offsets[end_index]
answer = context[start_char:end_char]
```

ì´ì œ ëª¨ë“  ê²ƒì„ í¬ë§·í•˜ì—¬ ê²°ê³¼ë¥¼ ì–»ê¸°ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤:

```py
result = {
Â  Â  "answer": answer,
Â  Â  "start": start_char,
Â  Â  "end": end_char,
Â  Â  "score": scores[start_index, end_index],
}
print(result)
```

```python out
{'answer': 'Jax, PyTorch and TensorFlow',
Â 'start': 78,
Â 'end': 105,
Â 'score': 0.97773}
```

í›Œë¥­í•©ë‹ˆë‹¤! ì´ëŠ” ì²« ë²ˆì§¸ ì˜ˆì œì™€ ë™ì¼í•©ë‹ˆë‹¤!

<Tip>

âœï¸ **ì§ì ‘ í•´ë³´ì„¸ìš”!** ì´ì „ì— ê³„ì‚°í•œ ê°€ì¥ ì¢‹ì€ ì ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°€ì¥ ê°€ëŠ¥ì„±ì´ ë†’ì€ ë‹¤ì„¯ ê°€ì§€ ë‹µë³€ì„ í‘œì‹œí•˜ì‹­ì‹œì˜¤(ì „ì²´ ì»¨í…ìŠ¤íŠ¸ì— ëŒ€í•´, ê° ì²­í¬ì— ëŒ€í•´ì„œê°€ ì•„ë‹˜). ê²°ê³¼ë¥¼ í™•ì¸í•˜ë ¤ë©´ ì²« ë²ˆì§¸ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ëŒì•„ê°€ì„œ í˜¸ì¶œí•  ë•Œ **`top_k=5`**ë¥¼ ì „ë‹¬í•˜ì‹­ì‹œì˜¤.

</Tip>

## ê¸´ ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬ (Handling long contexts)

ì´ì „ì— ì˜ˆì œë¡œ ì‚¬ìš©í•œ ì§ˆë¬¸ê³¼ ê¸´ ì»¨í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ë ¤ê³  í•˜ë©´, **`question-answering`** íŒŒì´í”„ë¼ì¸ì—ì„œ ì‚¬ìš©ë˜ëŠ” ìµœëŒ€ ê¸¸ì´(384)ë³´ë‹¤ ë†’ì€ í† í° ìˆ˜ë¥¼ ì–»ê²Œ ë©ë‹ˆë‹¤:

```py
inputs = tokenizer(question, long_context)
print(len(inputs["input_ids"]))
```

```python out
461
```

ë”°ë¼ì„œ ì…ë ¥ì„ í•´ë‹¹ ìµœëŒ€ ê¸¸ì´ë¡œ ì˜ë¼ì•¼ í•©ë‹ˆë‹¤. ì´ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ëª‡ ê°€ì§€ ë°©ë²•ì´ ìˆì§€ë§Œ, ì§ˆë¬¸ì´ ì•„ë‹Œ ì»¨í…ìŠ¤íŠ¸ë§Œ ìë¥´ê³  ì‹¶ìŠµë‹ˆë‹¤. ì»¨í…ìŠ¤íŠ¸ê°€ ë‘ ë²ˆì§¸ ë¬¸ì¥ì´ë¯€ë¡œ, **`"only_second"`** ìë¥´ê¸° ì „ëµì„ ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤. ì—¬ê¸°ì„œ ë°œìƒí•˜ëŠ” ë¬¸ì œëŠ” ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì´ ì˜ë¦° ì»¨í…ìŠ¤íŠ¸ì— ì—†ì„ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì—¬ê¸°ì„œëŠ” ë‹µì´ ì»¨í…ìŠ¤íŠ¸ì˜ ë ë¶€ë¶„ì— ìˆëŠ” ì§ˆë¬¸ì„ ì„ íƒí–ˆìœ¼ë©°, ì´ë¥¼ ìë¥´ë©´ í•´ë‹¹ ë‹µì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤:

```py
inputs = tokenizer(question, long_context, max_length=384, truncation="only_second")
print(tokenizer.decode(inputs["input_ids"]))
```

```python out
"""
[CLS] Which deep learning libraries back [UNK] Transformers? [SEP] [UNK] Transformers : State of the Art NLP

[UNK] Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,
question answering, summarization, translation, text generation and more in over 100 languages.
Its aim is to make cutting-edge NLP easier to use for everyone.

[UNK] Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and
then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and
can be modified to enable quick research experiments.

Why should I use transformers?

1. Easy-to-use state-of-the-art models:
Â  - High performance on NLU and NLG tasks.
Â  - Low barrier to entry for educators and practitioners.
Â  - Few user-facing abstractions with just three classes to learn.
Â  - A unified API for using all our pretrained models.
Â  - Lower compute costs, smaller carbon footprint:

2. Researchers can share trained models instead of always retraining.
Â  - Practitioners can reduce compute time and production costs.
Â  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.

3. Choose the right framework for every part of a model's lifetime:
Â  - Train state-of-the-art models in 3 lines of code.
Â  - Move a single model between TF2.0/PyTorch frameworks at will.
Â  - Seamlessly pick the right framework for training, evaluation and production.

4. Easily customize a model or an example to your needs:
Â  - We provide examples for each architecture to reproduce the results published by its original authors.
Â  - Model internal [SEP]
"""
```

ì´ê²ƒì€ ëª¨ë¸ì´ ì˜¬ë°”ë¥¸ ë‹µì„ ì„ íƒí•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªì„ ê²ƒì„ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ **`question-answering`** íŒŒì´í”„ë¼ì¸ì„ ì‚¬ìš©í•˜ë©´ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë” ì‘ì€ ì²­í¬ë¡œ ë¶„í• í•˜ê³  ìµœëŒ€ ê¸¸ì´ë¥¼ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ë„ë¡ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì •í™•íˆ ì˜ëª»ëœ ìœ„ì¹˜ì—ì„œ ë¶„í• í•˜ì§€ ì•Šë„ë¡ í•˜ê¸° ìœ„í•´ ì²­í¬ ê°„ì— ì¼ë¶€ ì˜¤ë²„ë©ë„ í¬í•¨í•©ë‹ˆë‹¤.

**`return_overflowing_tokens=True`**ë¥¼ ì¶”ê°€í•˜ì—¬ í† í¬ë‚˜ì´ì €(ë¹ ë¥´ë“  ëŠë¦¬ë“ )ê°€ ì´ ì‘ì—…ì„ ìˆ˜í–‰í•˜ë„ë¡ í•  ìˆ˜ ìˆìœ¼ë©°, **`stride`** ì¸ìˆ˜ë¡œ ì›í•˜ëŠ” ì˜¤ë²„ë©ì„ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ìŒì€ ë” ì‘ì€ ë¬¸ì¥ì„ ì‚¬ìš©í•˜ëŠ” ì˜ˆì…ë‹ˆë‹¤:

```py
sentence = "This sentence is not too long but we are going to split it anyway."
inputs = tokenizer(
Â  Â  sentence, truncation=True, return_overflowing_tokens=True, max_length=6, stride=2
)

for ids in inputs["input_ids"]:
Â  Â  print(tokenizer.decode(ids))
```

```python out
'[CLS] This sentence is not [SEP]'
'[CLS] is not too long [SEP]'
'[CLS] too long but we [SEP]'
'[CLS] but we are going [SEP]'
'[CLS] are going to split [SEP]'
'[CLS] to split it anyway [SEP]'
'[CLS] it anyway. [SEP]'
```

ë³´ì‹œë‹¤ì‹œí”¼, ë¬¸ì¥ì´ ì²­í¬ë¡œ ë¶„í• ë˜ì–´ **`inputs["input_ids"]`**ì˜ ê° í•­ëª©ì—ëŠ” ìµœëŒ€ 6ê°œì˜ í† í°ì´ ìˆê³ (ë§ˆì§€ë§‰ í•­ëª©ì´ ë‹¤ë¥¸ í•­ëª©ê³¼ ë™ì¼í•œ í¬ê¸°ë¥¼ ê°–ë„ë¡ íŒ¨ë”©ì„ ì¶”ê°€í•´ì•¼ í•¨) ê° í•­ëª© ì‚¬ì´ì— 2ê°œì˜ í† í° ì˜¤ë²„ë©ì´ ìˆìŠµë‹ˆë‹¤.

í† í°í™” ê²°ê³¼ë¥¼ ìì„¸íˆ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤:

```py
print(inputs.keys())
```

```python out
dict_keys(['input_ids', 'attention_mask', 'overflow_to_sample_mapping'])
```

ì˜ˆìƒëŒ€ë¡œ, ì…ë ¥ IDì™€ ì–´í…ì…˜ ë§ˆìŠ¤í¬ë¥¼ ì–»ìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ í‚¤ì¸ **`overflow_to_sample_mapping`**ì€ ê²°ê³¼ ê°ê°ì´ ì–´ë–¤ ë¬¸ì¥ì— í•´ë‹¹í•˜ëŠ”ì§€ ì•Œë ¤ì£¼ëŠ” ë§µì…ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” í† í¬ë‚˜ì´ì €ì— ì „ë‹¬í•œ (ìœ ì¼í•œ) ë¬¸ì¥ì—ì„œ 7ê°œì˜ ê²°ê³¼ê°€ ëª¨ë‘ íŒŒìƒë©ë‹ˆë‹¤:

```py
print(inputs["overflow_to_sample_mapping"])
```

```python out
[0, 0, 0, 0, 0, 0, 0]
```

ì—¬ëŸ¬ ë¬¸ì¥ì„ í•¨ê»˜ í† í°í™”í•  ë•Œ ë” ìœ ìš©í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ë‹¤ìŒì€:

```py
sentences = [
Â  Â  "This sentence is not too long but we are going to split it anyway.",
Â  Â  "This sentence is shorter but will still get split.",
]
inputs = tokenizer(
Â  Â  sentences, truncation=True, return_overflowing_tokens=True, max_length=6, stride=2
)

print(inputs["overflow_to_sample_mapping"])
```

ë‹¤ìŒ ê²°ê³¼ë¥¼ ì–»ìŠµë‹ˆë‹¤:

```python out
[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]
```

ì´ëŠ” ì²« ë²ˆì§¸ ë¬¸ì¥ì´ ì´ì „ê³¼ ê°™ì´ 7ê°œì˜ ì²­í¬ë¡œ ë¶„í• ë˜ê³ , ë‹¤ìŒ 4ê°œì˜ ì²­í¬ëŠ” ë‘ ë²ˆì§¸ ë¬¸ì¥ì—ì„œ íŒŒìƒë¨ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

ì´ì œ ê¸´ ì»¨í…ìŠ¤íŠ¸ë¡œ ëŒì•„ê°‘ì‹œë‹¤. ê¸°ë³¸ì ìœ¼ë¡œ **`question-answering`** íŒŒì´í”„ë¼ì¸ì€ ì´ì „ì— ì–¸ê¸‰í–ˆë“¯ì´ ìµœëŒ€ ê¸¸ì´ 384ì™€ ìŠ¤íŠ¸ë¼ì´ë“œ 128ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì´ ë¯¸ì„¸ ì¡°ì •ëœ ë°©ì‹ì— í•´ë‹¹í•©ë‹ˆë‹¤(íŒŒì´í”„ë¼ì¸ì„ í˜¸ì¶œí•  ë•Œ **`max_seq_len`** ë° **`stride`** ì¸ìˆ˜ë¥¼ ì „ë‹¬í•˜ì—¬ ì´ ë§¤ê°œë³€ìˆ˜ë¥¼ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤). ë”°ë¼ì„œ í† í°í™”í•  ë•Œ í•´ë‹¹ ë§¤ê°œë³€ìˆ˜ë¥¼ ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤. ë˜í•œ íŒ¨ë”©(ë™ì¼í•œ ê¸¸ì´ì˜ ìƒ˜í”Œì„ ê°–ë„ë¡, í…ì„œë¥¼ êµ¬ì¶•í•  ìˆ˜ ìˆë„ë¡)ê³¼ ì˜¤í”„ì…‹ì„ ìš”ì²­í•  ê²ƒì…ë‹ˆë‹¤:

```py
inputs = tokenizer(
Â  Â  question,
Â  Â  long_context,
Â  Â  stride=128,
Â  Â  max_length=384,
Â  Â  padding="longest",
Â  Â  truncation="only_second",
Â  Â  return_overflowing_tokens=True,
Â  Â  return_offsets_mapping=True,
)
```

ì´ **`inputs`**ëŠ” ëª¨ë¸ì´ ì˜ˆìƒí•˜ëŠ” ì…ë ¥ IDì™€ ì–´í…ì…˜ ë§ˆìŠ¤í¬ë¿ë§Œ ì•„ë‹ˆë¼ ë°©ê¸ˆ ì´ì•¼ê¸°í•œ ì˜¤í”„ì…‹ê³¼ **`overflow_to_sample_mapping`**ì„ í¬í•¨í•  ê²ƒì…ë‹ˆë‹¤. ì´ ë‘ ê°€ì§€ëŠ” ëª¨ë¸ì—ì„œ ì‚¬ìš©ë˜ëŠ” ë§¤ê°œë³€ìˆ˜ê°€ ì•„ë‹ˆë¯€ë¡œ, í…ì„œë¡œ ë³€í™˜í•˜ê¸° ì „ì— **`inputs`**ì—ì„œ ì´ë“¤ì„ ì œê±°í•  ê²ƒì…ë‹ˆë‹¤(ê·¸ë¦¬ê³  ë§µì€ ì—¬ê¸°ì„œ ìœ ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì €ì¥í•˜ì§€ ì•Šì„ ê²ƒì…ë‹ˆë‹¤):

```py
_ = inputs.pop("overflow_to_sample_mapping")
offsets = inputs.pop("offset_mapping")

inputs = inputs.convert_to_tensors("pt")
print(inputs["input_ids"].shape)
```

```python out
torch.Size([2, 384])
```

ê¸´ ì»¨í…ìŠ¤íŠ¸ëŠ” ë‘ ê°œë¡œ ë¶„í• ë˜ì—ˆìœ¼ë¯€ë¡œ, ëª¨ë¸ì„ í†µê³¼í•œ í›„ì—ëŠ” ë‘ ì„¸íŠ¸ì˜ ì‹œì‘ ë° ë ë¡œì§“ì´ ìˆì„ ê²ƒì…ë‹ˆë‹¤:

```py
outputs = model(**inputs)

start_logits = outputs.start_logits
end_logits = outputs.end_logits
print(start_logits.shape, end_logits.shape)
```

```python out
torch.Size([2, 384]) torch.Size([2, 384])
```

ì´ì „ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ, ë¨¼ì € ì†Œí”„íŠ¸ë§¥ìŠ¤ë¥¼ ì·¨í•˜ê¸° ì „ì— ì»¨í…ìŠ¤íŠ¸ì˜ ì¼ë¶€ê°€ ì•„ë‹Œ í† í°ì„ ë§ˆìŠ¤í‚¹í•©ë‹ˆë‹¤. ë˜í•œ (ì–´í…ì…˜ ë§ˆìŠ¤í¬ë¡œ í”Œë˜ê·¸ê°€ ì§€ì •ëœ) ëª¨ë“  íŒ¨ë”© í† í°ì„ ë§ˆìŠ¤í‚¹í•©ë‹ˆë‹¤:

```py
sequence_ids = inputs.sequence_ids()
# ì»¨í…ìŠ¤íŠ¸ì˜ í† í°ì„ ì œì™¸í•œ ëª¨ë“  ê²ƒì„ ë§ˆìŠ¤í‚¹
mask = [i != 1 for i in sequence_ids]
# [CLS] í† í° ë§ˆìŠ¤í‚¹ í•´ì œ
mask[0] = False
# ëª¨ë“  [PAD] í† í° ë§ˆìŠ¤í‚¹
mask = torch.logical_or(torch.tensor(mask)[None], (inputs["attention_mask"] == 0))

start_logits[mask] = -10000
end_logits[mask] = -10000
```

ê·¸ëŸ° ë‹¤ìŒ ì†Œí”„íŠ¸ë§¥ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¡œì§“ì„ í™•ë¥ ë¡œ ë³€í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```py
start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)
end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)
```

ë‹¤ìŒ ë‹¨ê³„ëŠ” ì‘ì€ ì»¨í…ìŠ¤íŠ¸ì— ëŒ€í•´ í–ˆë˜ ê²ƒê³¼ ìœ ì‚¬í•˜ì§€ë§Œ, ë‘ ì²­í¬ ê°ê°ì— ëŒ€í•´ ë°˜ë³µí•©ë‹ˆë‹¤. ê°€ëŠ¥í•œ ëª¨ë“  ë‹µë³€ ë²”ìœ„ì— ì ìˆ˜ë¥¼ í• ë‹¹í•œ ë‹¤ìŒ, ê°€ì¥ ì¢‹ì€ ì ìˆ˜ë¥¼ ê°€ì§„ ë²”ìœ„ë¥¼ ì·¨í•©ë‹ˆë‹¤:

```py
candidates = []
for start_probs, end_probs in zip(start_probabilities, end_probabilities):
Â  Â  scores = start_probs[:, None] * end_probs[None, :]
Â  Â  idx = torch.triu(scores).argmax().item()

Â  Â  start_idx = idx // scores.shape[1]
Â  Â  end_idx = idx % scores.shape[1]
Â  Â  score = scores[start_idx, end_idx].item()
Â  Â  candidates.append((start_idx, end_idx, score))

print(candidates)
```

```python out
[(0, 18, 0.33867), (173, 184, 0.97149)]
```

ì´ ë‘ í›„ë³´ëŠ” ëª¨ë¸ì´ ê° ì²­í¬ì—ì„œ ì°¾ì„ ìˆ˜ ìˆì—ˆë˜ ìµœìƒì˜ ë‹µë³€ì— í•´ë‹¹í•©ë‹ˆë‹¤. ëª¨ë¸ì€ ì •ë‹µì´ ë‘ ë²ˆì§¸ ë¶€ë¶„ì— ìˆì„ ê²ƒì´ë¼ê³  í›¨ì”¬ ë” í™•ì‹ í•©ë‹ˆë‹¤(ì´ëŠ” ì¢‹ì€ ì§•ì¡°ì…ë‹ˆë‹¤!). ì´ì œ í•´ë‹¹ ë‘ í† í° ë²”ìœ„ë¥¼ ì»¨í…ìŠ¤íŠ¸ì˜ ë¬¸ì ë²”ìœ„ë¡œ ë§¤í•‘í•˜ê¸°ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤(ë‹µë³€ì„ ì–»ìœ¼ë ¤ë©´ ë‘ ë²ˆì§¸ ë²”ìœ„ë§Œ ë§¤í•‘í•˜ë©´ ë˜ì§€ë§Œ, ëª¨ë¸ì´ ì²« ë²ˆì§¸ ì²­í¬ì—ì„œ ë¬´ì—‡ì„ ì„ íƒí–ˆëŠ”ì§€ ë³´ëŠ” ê²ƒì€ í¥ë¯¸ë¡­ìŠµë‹ˆë‹¤).

<Tip>

âœï¸ **ì§ì ‘ í•´ë³´ì„¸ìš”!** ìœ„ì˜ ì½”ë“œë¥¼ ìˆ˜ì •í•˜ì—¬ ê°€ì¥ ê°€ëŠ¥ì„±ì´ ë†’ì€ ë‹¤ì„¯ ê°€ì§€ ë‹µë³€ì— ëŒ€í•œ ì ìˆ˜ì™€ ë²”ìœ„ë¥¼ ë°˜í™˜í•˜ë„ë¡ í•˜ì‹­ì‹œì˜¤(ì´ê³„ì— ëŒ€í•´, ì²­í¬ë³„ì´ ì•„ë‹˜).

</Tip>

ìš°ë¦¬ê°€ ì´ì „ì— ê°€ì ¸ì˜¨ **`offsets`**ëŠ” ì‹¤ì œë¡œ ì˜¤í”„ì…‹ ëª©ë¡ì´ë©°, í…ìŠ¤íŠ¸ ì²­í¬ë‹¹ í•˜ë‚˜ì˜ ëª©ë¡ì´ ìˆìŠµë‹ˆë‹¤:

```py
for candidate, offset in zip(candidates, offsets):
Â  Â  start_token, end_token, score = candidate
Â  Â  start_char, _ = offset[start_token]
Â  Â  _, end_char = offset[end_token]
Â  Â  answer = long_context[start_char:end_char]
Â  Â  result = {"answer": answer, "start": start_char, "end": end_char, "score": score}
Â  Â  print(result)
```

```python out
{'answer': '\nğŸ¤— Transformers: State of the Art NLP', 'start': 0, 'end': 37, 'score': 0.33867}
{'answer': 'Jax, PyTorch and TensorFlow', 'start': 1892, 'end': 1919, 'score': 0.97149}
```

ì²« ë²ˆì§¸ ê²°ê³¼ë¥¼ ë¬´ì‹œí•˜ë©´, ì´ ê¸´ ì»¨í…ìŠ¤íŠ¸ì— ëŒ€í•´ íŒŒì´í”„ë¼ì¸ê³¼ ë™ì¼í•œ ê²°ê³¼ë¥¼ ì–»ìŠµë‹ˆë‹¤. ì•¼í˜¸!

<Tip>

âœï¸ **ì§ì ‘ í•´ë³´ì„¸ìš”!** ì´ì „ì— ê³„ì‚°í•œ ê°€ì¥ ì¢‹ì€ ì ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°€ì¥ ê°€ëŠ¥ì„±ì´ ë†’ì€ ë‹¤ì„¯ ê°€ì§€ ë‹µë³€ì„ í‘œì‹œí•˜ì‹­ì‹œì˜¤(ì „ì²´ ì»¨í…ìŠ¤íŠ¸ì— ëŒ€í•´, ê° ì²­í¬ì— ëŒ€í•´ì„œê°€ ì•„ë‹˜). ê²°ê³¼ë¥¼ í™•ì¸í•˜ë ¤ë©´ ì²« ë²ˆì§¸ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ëŒì•„ê°€ì„œ í˜¸ì¶œí•  ë•Œ **`top_k=5`**ë¥¼ ì „ë‹¬í•˜ì‹­ì‹œì˜¤.

</Tip>

ì´ê²ƒìœ¼ë¡œ í† í¬ë‚˜ì´ì €ì˜ ê¸°ëŠ¥ì— ëŒ€í•œ ì‹¬ì¸µì ì¸ ë‚´ìš©ì´ ëë‚©ë‹ˆë‹¤. ë‹¤ìŒ ì±•í„°ì—ì„œëŠ” ì¼ë°˜ì ì¸ NLP ì‘ì—… ë²”ìœ„ì—ì„œ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤„ ë•Œ ì´ ëª¨ë“  ê²ƒì„ ë‹¤ì‹œ ì—°ìŠµí•  ê²ƒì…ë‹ˆë‹¤.


<EditOnGithub source="https://github.com/huggingface/course/blob/main/chapters/en/chapter6/3b.mdx" />

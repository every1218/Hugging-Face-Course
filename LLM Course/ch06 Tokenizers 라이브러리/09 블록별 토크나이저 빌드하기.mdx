# 토크나이저 구축하기, 블록별로 (Building a tokenizer, block by block)

이전 섹션에서 보았듯이, 토큰화는 여러 단계로 구성됩니다:

- 정규화 (Normalization) (공백이나 악센트 제거, 유니코드 정규화 등 필요하다고 판단되는 텍스트 정리)
- 사전 토큰화 (Pre-tokenization) (입력을 단어로 분할)
- 모델을 통한 입력 실행 (pre-tokenized된 단어를 사용하여 토큰 시퀀스 생성)
- 후처리 (Post-processing) (토크나이저의 특수 토큰 추가, 어텐션 마스크 및 토큰 유형 ID 생성)

요약하자면, 전체 프로세스를 다시 한 번 살펴보겠습니다:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline.svg" alt="The tokenization pipeline.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline-dark.svg" alt="The tokenization pipeline.">
</div>

🤗 Tokenizers 라이브러리는 이러한 각 단계에 대해 여러 옵션을 제공하도록 구축되었으며, 이를 혼합하고 일치시킬 수 있습니다. 이 섹션에서는 [섹션 2](/course/chapter6/2)에서 했던 것처럼 기존 토크나이저로부터 새 토크나이저를 훈련하는 것과 달리, **처음부터 토크나이저를 구축**하는 방법을 살펴볼 것입니다. 그러면 생각할 수 있는 모든 종류의 토크나이저를 구축할 수 있을 것입니다!

<Youtube id="MR8tZm5ViWU"/>

더 정확하게는, 라이브러리는 빌딩 블록이 서브모듈로 재그룹화된 중앙 **`Tokenizer`** 클래스를 중심으로 구축됩니다:

- **`normalizers`**에는 사용할 수 있는 모든 가능한 유형의 **`Normalizer`**가 포함되어 있습니다(전체 목록은 [여기](https://huggingface.co/docs/tokenizers/api/normalizers)에서 확인할 수 있습니다).
- **`pre_tokenizers`**에는 사용할 수 있는 모든 가능한 유형의 **`PreTokenizer`**가 포함되어 있습니다(전체 목록은 [여기](https://huggingface.co/docs/tokenizers/api/pre-tokenizers)에서 확인할 수 있습니다).
- **`models`**에는 **`BPE`**, **`WordPiece`**, 및 **`Unigram`**과 같이 사용할 수 있는 다양한 유형의 **`Model`**이 포함되어 있습니다(전체 목록은 [여기](https://huggingface.co/docs/tokenizers/api/models)에서 확인할 수 있습니다).
- **`trainers`**에는 코퍼스에서 모델을 훈련하는 데 사용할 수 있는 모든 다양한 유형의 **`Trainer`**가 포함되어 있습니다(모델 유형별로 하나씩, 전체 목록은 [여기](https://huggingface.co/docs/tokenizers/api/trainers)에서 확인할 수 있습니다).
- **`post_processors`**에는 사용할 수 있는 다양한 유형의 **`PostProcessor`**가 포함되어 있습니다(전체 목록은 [여기](https://huggingface.co/docs/tokenizers/api/post-processors)에서 확인할 수 있습니다).
- **`decoders`**에는 토큰화 출력을 디코딩하는 데 사용할 수 있는 다양한 유형의 **`Decoder`**가 포함되어 있습니다(전체 목록은 [여기](https://huggingface.co/docs/tokenizers/components#decoders)에서 확인할 수 있습니다).

전체 빌딩 블록 목록은 [여기](https://huggingface.co/docs/tokenizers/components)에서 찾을 수 있습니다.

## 코퍼스 확보 (Acquiring a corpus)

새 토크나이저를 훈련하기 위해, 작은 텍스트 코퍼스를 사용할 것입니다(예제가 빠르게 실행되도록). 코퍼스를 확보하는 단계는 [이 챕터의 시작 부분](/course/chapter6/2)에서 취한 단계와 유사하지만, 이번에는 [WikiText-2](https://huggingface.co/datasets/wikitext) 데이터셋을 사용할 것입니다:

```python
from datasets import load_dataset

dataset = load_dataset("wikitext", name="wikitext-2-raw-v1", split="train")


def get_training_corpus():
    for i in range(0, len(dataset), 1000):
        yield dataset[i : i + 1000]["text"]
```

**`get_training_corpus()`** 함수는 1,000개의 텍스트 배치를 생성하는 제너레이터이며, 이를 토크나이저를 훈련하는 데 사용할 것입니다.

🤗 Tokenizers는 텍스트 파일에서도 직접 훈련될 수 있습니다. 다음은 WikiText-2의 모든 텍스트/입력을 포함하는 텍스트 파일을 생성하는 방법입니다. 이 파일을 로컬에서 사용할 수 있습니다:

```python
with open("wikitext-2.txt", "w", encoding="utf-8") as f:
    for i in range(len(dataset)):
        f.write(dataset[i]["text"] + "\n")
```

다음으로, 처음부터 자신만의 BERT, GPT-2, 및 XLNet 토크나이저를 블록별로 구축하는 방법을 보여줄 것입니다. 이는 WordPiece, BPE, 및 Unigram이라는 세 가지 주요 토큰화 알고리즘 각각의 예시를 제공할 것입니다. BERT부터 시작하겠습니다!

## 처음부터 WordPiece 토크나이저 구축하기 (Building a WordPiece tokenizer from scratch)

🤗 Tokenizers 라이브러리로 토크나이저를 구축하기 위해, 먼저 **`model`**로 **`Tokenizer`** 객체를 인스턴스화한 다음, 해당 **`normalizer`**, **`pre_tokenizer`**, **`post_processor`**, 및 **`decoder`** 속성을 원하는 값으로 설정합니다.

이 예시에서는 WordPiece 모델로 **`Tokenizer`**를 생성할 것입니다:

```python
from tokenizers import (
    decoders,
    models,
    normalizers,
    pre_tokenizers,
    processors,
    trainers,
    Tokenizer,
)

tokenizer = Tokenizer(models.WordPiece(unk_token="[UNK]"))
```

이전에 보지 못한 문자를 만나면 무엇을 반환해야 하는지 모델이 알 수 있도록 **`unk_token`**을 지정해야 합니다. 여기서 설정할 수 있는 다른 인수에는 모델의 **`vocab`**(모델을 훈련할 것이므로 설정할 필요가 없습니다)와 각 단어의 최대 길이를 지정하는 **`max_input_chars_per_word`**(이 값을 초과하는 단어는 분할됨)가 포함됩니다.

토큰화의 첫 번째 단계는 정규화이므로, 그것부터 시작하겠습니다. BERT가 널리 사용되므로, BERT에 대해 설정할 수 있는 클래식 옵션이 포함된 **`BertNormalizer`**가 있습니다. **`lowercase`**와 **`strip_accents`**는 자명하며, **`clean_text`**는 모든 제어 문자를 제거하고 반복되는 공백을 단일 공백으로 대체합니다. **`handle_chinese_chars`**는 한자 주위에 공백을 배치합니다. **`bert-base-uncased`** 토크나이저를 복제하려면 이 정규화기를 설정하기만 하면 됩니다:

```python
tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)
```

그러나 일반적으로 새 토크나이저를 구축할 때 🤗 Tokenizers 라이브러리에 이미 구현된 이러한 편리한 정규화기에 액세스할 수 없을 것입니다. 따라서 BERT 정규화기를 수동으로 만드는 방법을 살펴봅시다. 라이브러리는 **`Lowercase`** 정규화기와 **`StripAccents`** 정규화기를 제공하며, **`Sequence`**를 사용하여 여러 정규화기를 구성할 수 있습니다:

```python
tokenizer.normalizer = normalizers.Sequence(
    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]
)
```

또한 **`NFD`** 유니코드 정규화기를 사용하고 있습니다. 그렇지 않으면 **`StripAccents`** 정규화기가 악센트 문자를 올바르게 인식하지 못하여 악센트를 제거하지 못할 것입니다.

이전에 보았듯이, **`normalizer`**의 **`normalize_str()`** 메서드를 사용하여 주어진 텍스트에 미치는 영향을 확인할 수 있습니다:

```python
print(tokenizer.normalizer.normalize_str("Héllò hôw are ü?"))
```

```python out
hello how are u?
```

<Tip>

**더 나아가기 (To go further)** `u"\u0085"` 유니코드 문자를 포함하는 문자열에서 이전 정규화기의 두 버전을 테스트하면 이 두 정규화기가 완전히 동일하지 않다는 것을 분명히 알 수 있습니다.
**`normalizers.Sequence`**를 사용하는 버전을 너무 복잡하게 만들지 않기 위해, `clean_text` 인수가 **`True`**로 설정될 때 **`BertNormalizer`**가 요구하는 Regex 대체를 포함하지 않았습니다. 하지만 걱정하지 마십시오. 편리한 **`BertNormalizer`**를 사용하지 않고도 두 **`normalizers.Replace`**를 정규화기 시퀀스에 추가하여 정확히 동일한 정규화를 얻을 수 있습니다.

</Tip>

다음은 사전 토큰화 단계입니다. 다시 말하지만, 사용할 수 있는 미리 구축된 **`BertPreTokenizer`**가 있습니다:

```python
tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()
```

또는 처음부터 구축할 수 있습니다:

```python
tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()
```

**`Whitespace`** 사전 토크나이저는 공백과 문자, 숫자 또는 밑줄 문자가 아닌 모든 문자를 기준으로 분할하므로, 기술적으로 공백과 구두점을 기준으로 분할한다는 점에 유의하십시오:

```python
tokenizer.pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer.")
```

```python out
[('Let', (0, 3)), ("'", (3, 4)), ('s', (4, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre', (14, 17)),
 ('-', (17, 18)), ('tokenizer', (18, 27)), ('.', (27, 28))]
```

공백으로만 분할하려면 대신 **`WhitespaceSplit`** 사전 토크나이저를 사용해야 합니다:

```python
pre_tokenizer = pre_tokenizers.WhitespaceSplit()
pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer.")
```

```python out
[("Let's", (0, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre-tokenizer.', (14, 28))]
```

정규화기와 마찬가지로, **`Sequence`**를 사용하여 여러 사전 토크나이저를 구성할 수 있습니다:

```python
pre_tokenizer = pre_tokenizers.Sequence(
    [pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()]
)
pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer.")
```

```python out
[('Let', (0, 3)), ("'", (3, 4)), ('s', (4, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre', (14, 17)),
 ('-', (17, 18)), ('tokenizer', (18, 27)), ('.', (27, 28))]
```

토큰화 파이프라인의 다음 단계는 입력을 모델을 통해 실행하는 것입니다. 초기화에서 모델을 이미 지정했지만, 여전히 훈련해야 하며, **`WordPieceTrainer`**가 필요합니다. 🤗 Tokenizers에서 트레이너를 인스턴스화할 때 기억해야 할 주요 사항은 사용하려는 모든 특수 토큰을 전달해야 한다는 것입니다. 그렇지 않으면 훈련 코퍼스에 없으므로 어휘에 추가하지 않습니다:

```python
special_tokens = ["[UNK]", "[PAD]", "[CLS]", "[SEP]", "[MASK]"]
trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)
```

**`vocab_size`**와 **`special_tokens`**를 지정하는 것 외에도, **`min_frequency`**(토큰이 어휘에 포함되려면 나타나야 하는 횟수)를 설정하거나 **`continuing_subword_prefix`**를 변경할 수 있습니다( **`##`**와 다른 것을 사용하려는 경우).

이전에 정의한 반복자를 사용하여 모델을 훈련하려면 다음 명령을 실행하기만 하면 됩니다:

```python
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)
```

텍스트 파일을 사용하여 토크나이저를 훈련할 수도 있으며, 이는 다음과 같이 보일 것입니다(이전에 빈 **`WordPiece`**로 모델을 다시 초기화합니다):

```python
tokenizer.model = models.WordPiece(unk_token="[UNK]")
tokenizer.train(["wikitext-2.txt"], trainer=trainer)
```

두 경우 모두 **`encode()`** 메서드를 호출하여 텍스트에서 토크나이저를 테스트할 수 있습니다:

```python
encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)
```

```python out
['let', "'", 's', 'test', 'this', 'tok', '##eni', '##zer', '.']
```

얻은 **`encoding`**은 다양한 속성에 토크나이저의 모든 필요한 출력을 포함하는 **`Encoding`**입니다: **`ids`**, **`type_ids`**, **`tokens`**, **`offsets`**, **`attention_mask`**, **`special_tokens_mask`**, 및 **`overflowing`**입니다.

토큰화 파이프라인의 마지막 단계는 후처리입니다. 시작 부분에 **`[CLS]`** 토큰을 추가하고 끝 부분에 **`[SEP]`** 토큰을 추가해야 합니다(또는 문장 쌍인 경우 각 문장 뒤에). 이를 위해 **`TemplateProcessor`**를 사용할 것이지만, 먼저 어휘에서 **`[CLS]`** 및 **`[SEP]`** 토큰의 ID를 알아야 합니다:

```python
cls_token_id = tokenizer.token_to_id("[CLS]")
sep_token_id = tokenizer.token_to_id("[SEP]")
print(cls_token_id, sep_token_id)
```

```python out
(2, 3)
```

**`TemplateProcessor`**에 대한 템플릿을 작성하려면, 단일 문장과 문장 쌍을 처리하는 방법을 지정해야 합니다. 둘 다에 대해 사용하려는 특수 토큰을 작성합니다. 첫 번째(또는 단일) 문장은 **`$A`**로 표시되고, 두 번째 문장(쌍을 인코딩하는 경우)은 **`$B`**로 표시됩니다. 이러한 각 항목(특수 토큰 및 문장)에 대해 콜론 뒤에 해당 토큰 유형 ID도 지정합니다.

클래식 BERT 템플릿은 다음과 같이 정의됩니다:

```python
tokenizer.post_processor = processors.TemplateProcessing(
    single=f"[CLS]:0 $A:0 [SEP]:0",
    pair=f"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1",
    special_tokens=[("[CLS]", cls_token_id), ("[SEP]", sep_token_id)],
)
```

토크나이저가 ID로 올바르게 변환할 수 있도록 특수 토큰의 ID를 함께 전달해야 한다는 점에 유의하십시오.

이것이 추가되면 이전 예제로 돌아가면 다음을 얻을 수 있습니다:

```python
encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)
```

```python out
['[CLS]', 'let', "'", 's', 'test', 'this', 'tok', '##eni', '##zer', '.', '[SEP]']
```

그리고 문장 쌍에서는 올바른 결과를 얻습니다:

```python
encoding = tokenizer.encode("Let's test this tokenizer...", "on a pair of sentences.")
print(encoding.tokens)
print(encoding.type_ids)
```

```python out
['[CLS]', 'let', "'", 's', 'test', 'this', 'tok', '##eni', '##zer', '...', '[SEP]', 'on', 'a', 'pair', 'of', 'sentences', '.', '[SEP]']
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]
```

이 토크나이저를 처음부터 구축하는 것을 거의 마쳤습니다. 마지막 단계는 디코더를 포함하는 것입니다:

```python
tokenizer.decoder = decoders.WordPiece(prefix="##")
```

이전 **`encoding`**에서 테스트해 봅시다:

```python
tokenizer.decode(encoding.ids)
```

```python out
"let's test this tokenizer... on a pair of sentences."
```

훌륭합니다! 다음과 같이 단일 JSON 파일에 토크나이저를 저장할 수 있습니다:

```python
tokenizer.save("tokenizer.json")
```

그런 다음 **`from_file()`** 메서드를 사용하여 **`Tokenizer`** 객체로 해당 파일을 다시 로드할 수 있습니다:

```python
new_tokenizer = Tokenizer.from_file("tokenizer.json")
```

이 토크나이저를 🤗 Transformers에서 사용하려면 **`PreTrainedTokenizerFast`**로 래핑해야 합니다. 일반 클래스를 사용하거나, 토크나이저가 기존 모델에 해당하는 경우 해당 클래스(여기서는 **`BertTokenizerFast`**)를 사용할 수 있습니다. 이 레슨을 적용하여 완전히 새로운 토크나이저를 구축하는 경우 첫 번째 옵션을 사용해야 합니다.

토크나이저를 **`PreTrainedTokenizerFast`**로 래핑하려면, 구축한 토크나이저를 **`tokenizer_object`**로 전달하거나 저장한 토크나이저 파일을 **`tokenizer_file`**로 전달할 수 있습니다. 기억해야 할 핵심은 **`mask_token`**, **`[CLS]`** 토큰 등 어떤 토큰이 마스크 토큰인지 **`tokenizer`** 객체에서 추론할 수 없으므로 모든 특수 토큰을 수동으로 설정해야 한다는 것입니다:

```python
from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    # tokenizer_file="tokenizer.json", # 대안으로 토크나이저 파일에서 로드할 수 있습니다.
    unk_token="[UNK]",
    pad_token="[PAD]",
    cls_token="[CLS]",
    sep_token="[SEP]",
    mask_token="[MASK]",
)
```

또는:

```python
from transformers import BertTokenizerFast

wrapped_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)
```

그런 다음 이 토크나이저를 다른 🤗 Transformers 토크나이저처럼 사용할 수 있습니다. **`save_pretrained()`** 메서드로 저장하거나 **`push_to_hub()`** 메서드로 Hub에 업로드할 수 있습니다.

이제 WordPiece 토크나이저를 구축하는 방법을 보았으므로, BPE 토크나이저에 대해서도 동일하게 수행해 봅시다. 모든 단계를 알고 있으므로 조금 더 빨리 진행하고 차이점만 강조하겠습니다.

## 처음부터 BPE 토크나이저 구축하기 (Building a BPE tokenizer from scratch)

이제 GPT-2 토크나이저를 구축해 봅시다. BERT 토크나이저와 마찬가지로 BPE 모델로 **`Tokenizer`**를 초기화하는 것으로 시작합니다:

```python
tokenizer = Tokenizer(models.BPE())
```

BERT와 마찬가지로, 어휘가 있는 경우 이 모델을 어휘로 초기화할 수 있지만(이 경우 **`vocab`**과 **`merges`**를 전달해야 함), 처음부터 훈련할 것이므로 그럴 필요는 없습니다. 또한 GPT-2는 바이트 수준 BPE를 사용하며 **`unk_token`**이 필요하지 않으므로 **`unk_token`**을 지정할 필요도 없습니다.

GPT-2는 정규화기를 사용하지 않으므로, 해당 단계를 건너뛰고 바로 사전 토큰화로 이동합니다:

```python
tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)
```

여기서 **`ByteLevel`**에 추가한 옵션은 문장 시작 부분에 공백을 추가하지 않는 것입니다(그렇지 않으면 기본값임). 이전처럼 예제 텍스트의 사전 토큰화를 살펴볼 수 있습니다:

```python
tokenizer.pre_tokenizer.pre_tokenize_str("Let's test pre-tokenization!")
```

```python out
[('Let', (0, 3)), ("'s", (3, 5)), ('Ġtest', (5, 10)), ('Ġpre', (10, 14)), ('-', (14, 15)),
 ('tokenization', (15, 27)), ('!', (27, 28))]
```

다음은 훈련이 필요한 모델입니다. GPT-2의 경우 유일한 특수 토큰은 텍스트 끝 토큰입니다:

```python
trainer = trainers.BpeTrainer(vocab_size=25000, special_tokens=["<|endoftext|>"])
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)
```

**`WordPieceTrainer`**와 마찬가지로, **`vocab_size`**와 **`special_tokens`** 외에도 **`min_frequency`**(토큰이 어휘에 포함되기 위해 나타나야 하는 횟수)를 지정하거나 **`end_of_word_suffix`**( **`</w>`**와 같은 것을 사용하려는 경우)를 설정할 수 있습니다.

이 토크나이저는 텍스트 파일에서도 훈련될 수 있습니다:

```python
tokenizer.model = models.BPE()
tokenizer.train(["wikitext-2.txt"], trainer=trainer)
```

샘플 텍스트의 토큰화를 살펴봅시다:

```python
encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)
```

```python out
['L', 'et', "'", 's', 'Ġtest', 'Ġthis', 'Ġto', 'ken', 'izer', '.']
```

GPT-2 토크나이저에 대한 바이트 수준 후처리를 다음과 같이 적용합니다:

```python
tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)
```

여기서 **`trim_offsets = False`** 옵션은 후처리기에게 **`'Ġ'`**로 시작하는 토큰의 오프셋을 그대로 두어야 함을 나타냅니다. 이렇게 하면 오프셋의 시작이 단어의 첫 번째 문자 대신 단어 앞의 공백을 가리킵니다(공백이 기술적으로 토큰의 일부이기 때문입니다). 방금 인코딩한 텍스트로 결과를 살펴봅시다. 여기서 **`'Ġtest'`**는 인덱스 4의 토큰입니다:

```python
sentence = "Let's test this tokenizer."
encoding = tokenizer.encode(sentence)
start, end = encoding.offsets[4]
sentence[start:end]
```

```python out
' test'
```

마지막으로, 바이트 수준 디코더를 추가합니다:

```python
tokenizer.decoder = decoders.ByteLevel()
```

그리고 제대로 작동하는지 다시 한 번 확인합니다:

```python
tokenizer.decode(encoding.ids)
```

```python out
"Let's test this tokenizer."
```

훌륭합니다! 이제 완료되었으므로 이전처럼 토크나이저를 저장하고, 🤗 Transformers에서 사용하려면 **`PreTrainedTokenizerFast`** 또는 **`GPT2TokenizerFast`**로 래핑할 수 있습니다:

```python
from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    bos_token="<|endoftext|>",
    eos_token="<|endoftext|>",
)
```

또는:

```python
from transformers import GPT2TokenizerFast

wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)
```

마지막 예로, 처음부터 Unigram 토크나이저를 구축하는 방법을 보여줄 것입니다.

## 처음부터 Unigram 토크나이저 구축하기 (Building a Unigram tokenizer from scratch)

이제 XLNet 토크나이저를 구축해 봅시다. 이전 토크나이저와 마찬가지로 Unigram 모델로 **`Tokenizer`**를 초기화하는 것으로 시작합니다:

```python
tokenizer = Tokenizer(models.Unigram())
```

다시 말하지만, 어휘가 있는 경우 이 모델을 어휘로 초기화할 수 있습니다.

정규화의 경우, XLNet은 몇 가지 대체(SentencePiece에서 파생됨)를 사용합니다:

```python
from tokenizers import Regex

tokenizer.normalizer = normalizers.Sequence(
    [
        normalizers.Replace("``", '"'),
        normalizers.Replace("''", '"'),
        normalizers.NFKD(),
        normalizers.StripAccents(),
        normalizers.Replace(Regex(" {2,}"), " "),
    ]
)
```

이는 **` `` `**와 **` '' `**를 **` " `**로 바꾸고, 두 개 이상의 공백 시퀀스를 단일 공백으로 바꾸며, 텍스트의 악센트를 제거합니다.

모든 SentencePiece 토크나이저에 사용할 사전 토크나이저는 **`Metaspace`**입니다:

```python
tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()
```

이전처럼 예제 텍스트의 사전 토큰화를 살펴볼 수 있습니다:

```python
tokenizer.pre_tokenizer.pre_tokenize_str("Let's test the pre-tokenizer!")
```

```python out
[(" Let's", (0, 5)), (' test', (5, 10)), (' the', (10, 14)), (' pre-tokenizer!', (14, 29))]
```

다음은 훈련이 필요한 모델입니다. XLNet에는 꽤 많은 특수 토큰이 있습니다:

```python
special_tokens = ["<cls>", "<sep>", "<unk>", "<pad>", "<mask>", "<s>", "</s>"]
trainer = trainers.UnigramTrainer(
    vocab_size=25000, special_tokens=special_tokens, unk_token="<unk>"
)
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)
```

**`UnigramTrainer`**에 대해 잊지 말아야 할 매우 중요한 인수는 **`unk_token`**입니다. 또한 토큰을 제거하는 각 단계에 대한 **`shrinking_factor`**(기본값은 0.75) 또는 주어진 토큰의 최대 길이를 지정하는 **`max_piece_length`**(기본값은 16)와 같은 Unigram 알고리즘에 특정한 다른 인수를 전달할 수 있습니다.

이 토크나이저는 텍스트 파일에서도 훈련될 수 있습니다:

```python
tokenizer.model = models.Unigram()
tokenizer.train(["wikitext-2.txt"], trainer=trainer)
```

샘플 텍스트의 토큰화를 살펴봅시다:

```python
encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)
```

```python out
[' Let', "'", 's', ' test', ' this', ' to', 'ken', 'izer', '.']
```

XLNet의 특이한 점은 **`<cls>`** 토큰을 문장의 끝에 배치하고, (다른 토큰과 구별하기 위해) 유형 ID 2를 부여한다는 것입니다. 결과적으로 왼쪽에 패딩을 합니다. BERT와 마찬가지로 템플릿을 사용하여 모든 특수 토큰과 토큰 유형 ID를 처리할 수 있지만, 먼저 **`<cls>`** 및 **`<sep>`** 토큰의 ID를 가져와야 합니다:

```python
cls_token_id = tokenizer.token_to_id("<cls>")
sep_token_id = tokenizer.token_to_id("<sep>")
print(cls_token_id, sep_token_id)
```

```python out
0 1
```

템플릿은 다음과 같습니다:

```python
tokenizer.post_processor = processors.TemplateProcessing(
    single="$A:0 <sep>:0 <cls>:2",
    pair="$A:0 <sep>:0 $B:1 <sep>:1 <cls>:2",
    special_tokens=[("<sep>", sep_token_id), ("<cls>", cls_token_id)],
)
```

그리고 문장 쌍을 인코딩하여 작동하는지 테스트할 수 있습니다:

```python
encoding = tokenizer.encode("Let's test this tokenizer...", "on a pair of sentences!")
print(encoding.tokens)
print(encoding.type_ids)
```

```python out
[' Let', "'", 's', ' test', ' this', ' to', 'ken', 'izer', '.', '.', '.', '<sep>', ' ', 'on', ' ', 'a', ' pair', 
  ' of', ' sentence', 's', '!', '<sep>', '<cls>']
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]
```

마지막으로, **`Metaspace`** 디코더를 추가합니다:

```python
tokenizer.decoder = decoders.Metaspace()
```

그리고 이 토크나이저를 마쳤습니다! 이전처럼 토크나이저를 저장하고, 🤗 Transformers에서 사용하려면 **`PreTrainedTokenizerFast`** 또는 **`XLNetTokenizerFast`**로 래핑할 수 있습니다. **`PreTrainedTokenizerFast`**를 사용할 때 한 가지 유의할 점은 특수 토큰 외에도 왼쪽에 패딩하도록 🤗 Transformers 라이브러리에 알려줘야 한다는 것입니다:

```python
from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    bos_token="<s>",
    eos_token="</s>",
    unk_token="<unk>",
    pad_token="<pad>",
    cls_token="<cls>",
    sep_token="<sep>",
    mask_token="<mask>",
    padding_side="left",
)
```

또는 대안으로:

```python
from transformers import XLNetTokenizerFast

wrapped_tokenizer = XLNetTokenizerFast(tokenizer_object=tokenizer)
```

이제 기존 토크나이저를 구축하는 데 다양한 빌딩 블록이 어떻게 사용되는지 보았으므로, 🤗 Tokenizers 라이브러리로 원하는 모든 토크나이저를 작성하고 🤗 Transformers에서 사용할 수 있어야 합니다.


<EditOnGithub source="https://github.com/huggingface/course/blob/main/chapters/en/chapter6/8.mdx" />

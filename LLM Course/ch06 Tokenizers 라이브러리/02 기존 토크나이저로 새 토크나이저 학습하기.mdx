# 기존 토크나이저로부터 새 토크나이저 훈련하기 (Training a new tokenizer from an old one)

관심 있는 언어로 된 언어 모델이 없거나, 언어 모델이 훈련된 코퍼스가 당신의 코퍼스와 매우 다른 경우, 당신의 데이터에 맞게 조정된 토크나이저를 사용하여 처음부터 모델을 재훈련하는 것을 원할 가능성이 높습니다. 그렇게 하려면 데이터셋에서 새 토크나이저를 훈련해야 합니다. 하지만 그것이 정확히 무엇을 의미할까요? [챕터 2](/course/chapter2)에서 토크나이저를 처음 살펴봤을 때, 대부분의 트랜스포머 모델은 *서브워드 토큰화 알고리즘*을 사용한다는 것을 알았습니다. 관심 있고 현재 코퍼스에서 가장 자주 발생하는 서브워드를 식별하기 위해, 토크나이저는 코퍼스의 모든 텍스트를 면밀히 살펴봐야 합니다. 우리는 이 과정을 *훈련*이라고 부릅니다. 이 훈련을 지배하는 정확한 규칙은 사용되는 토크나이저 유형에 따라 다르며, 이 챕터의 후반부에서 세 가지 주요 알고리즘을 살펴볼 것입니다.

<Youtube id="DJimQynXZsQ"/>

<Tip warning={true}>

⚠️ 토크나이저 훈련은 모델 훈련과 다릅니다! 모델 훈련은 확률적 경사 하강법을 사용하여 각 배치에 대한 손실을 조금씩 줄입니다. 이는 본질적으로 무작위적입니다(즉, 동일한 훈련을 두 번 수행할 때 동일한 결과를 얻으려면 일부 시드를 설정해야 합니다). 토크나이저 훈련은 주어진 코퍼스에 대해 어떤 서브워드를 선택하는 것이 가장 좋은지 식별하려고 시도하는 통계적 프로세스이며, 서브워드를 선택하는 데 사용되는 정확한 규칙은 토큰화 알고리즘에 따라 다릅니다. 이는 **결정론적**입니다. 즉, 동일한 코퍼스에서 동일한 알고리즘으로 훈련할 때 항상 동일한 결과를 얻습니다.

</Tip>

## 코퍼스 조립하기 (Assembling a corpus)

기존 토크나이저와 동일한 특성을 가진 새 토크나이저를 훈련하는 데 사용할 수 있는 🤗 Transformers의 매우 간단한 API가 있습니다. 바로 **`AutoTokenizer.train_new_from_iterator()`**입니다. 이것이 실제로 작동하는 것을 보기 위해, 처음부터 GPT-2를 훈련하고 싶지만 영어가 아닌 다른 언어로 훈련하고 싶다고 가정해 봅시다. 우리의 첫 번째 작업은 해당 언어로 된 많은 데이터를 훈련 코퍼스에 모으는 것입니다. 모두가 이해할 수 있는 예시를 제공하기 위해, 여기서는 러시아어나 중국어와 같은 언어를 사용하지 않고, 특화된 영어인 **Python 코드**를 사용할 것입니다.

[🤗 Datasets](https://github.com/huggingface/datasets) 라이브러리는 Python 소스 코드 코퍼스를 조립하는 데 도움이 될 수 있습니다. 일반적인 **`load_dataset()`** 함수를 사용하여 [CodeSearchNet](https://huggingface.co/datasets/code_search_net) 데이터셋을 다운로드하고 캐시할 것입니다. 이 데이터셋은 [CodeSearchNet 챌린지](https://wandb.ai/github/CodeSearchNet/benchmark)를 위해 만들어졌으며, 여러 프로그래밍 언어로 된 GitHub의 오픈 소스 라이브러리에서 수백만 개의 함수를 포함합니다. 여기서는 이 데이터셋의 Python 부분을 로드할 것입니다:

```py
from datasets import load_dataset

# 로드하는 데 몇 분이 걸릴 수 있으므로, 기다리는 동안 커피나 차를 마시세요!
raw_datasets = load_dataset("code_search_net", "python")
```

훈련 분할을 살펴보면 액세스할 수 있는 열을 확인할 수 있습니다:

```py
raw_datasets["train"]
```

```python out
Dataset({
    features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 
      'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 
      'func_code_url'
    ],
    num_rows: 412178
})
```

데이터셋이 독스트링과 코드를 분리하고 둘 다의 토큰화를 제안하는 것을 볼 수 있습니다. 여기서는 토크나이저를 훈련하는 데 **`whole_func_string`** 열만 사용할 것입니다. **`train`** 분할을 인덱싱하여 이 함수 중 하나의 예시를 볼 수 있습니다:

```py
print(raw_datasets["train"][123456]["whole_func_string"])
```

다음과 같이 출력되어야 합니다:

```out
def handle_simple_responses(
      self, timeout_ms=None, info_cb=DEFAULT_MESSAGE_CALLBACK):
    """Accepts normal responses from the device.

    Args:
      timeout_ms: Timeout in milliseconds to wait for each response.
      info_cb: Optional callback for text sent from the bootloader.

    Returns:
      OKAY packet's message.
    """
    return self._accept_responses('OKAY', info_cb, timeout_ms=timeout_ms)
```

가장 먼저 해야 할 일은 데이터셋을 텍스트 목록의 *반복자*(iterator)로 변환하는 것입니다. 예를 들어, 텍스트 목록의 목록입니다. 텍스트 목록을 사용하면 토크나이저가 더 빠르게 실행될 수 있으며(개별 텍스트를 하나씩 처리하는 대신 텍스트 배치를 훈련), 모든 것을 한 번에 메모리에 로드하는 것을 피하려면 반복자여야 합니다. 코퍼스가 방대하다면, 🤗 Datasets가 모든 것을 RAM에 로드하는 것이 아니라 데이터셋의 요소를 디스크에 저장한다는 사실을 활용해야 합니다.

다음과 같이 하면 각각 1,000개의 텍스트 목록의 목록이 생성되지만, 모든 것이 메모리에 로드됩니다:

```py
# 데이터셋이 작지 않다면 다음 줄의 주석을 해제하지 마십시오!
# training_corpus = [raw_datasets["train"][i: i + 1000]["whole_func_string"] for i in range(0, len(raw_datasets["train"]), 1000)]
```

Python **제너레이터**를 사용하면 실제로 필요할 때까지 Python이 아무것도 메모리에 로드하는 것을 피할 수 있습니다. 이러한 제너레이터를 만들려면 대괄호를 괄호로 바꾸기만 하면 됩니다:

```py
training_corpus = (
    raw_datasets["train"][i : i + 1000]["whole_func_string"]
    for i in range(0, len(raw_datasets["train"]), 1000)
)
```

이 코드 줄은 데이터셋의 요소를 가져오지 않습니다. Python **`for`** 루프에서 사용할 수 있는 객체만 생성합니다. 텍스트는 필요할 때만(즉, 텍스트가 필요한 **`for`** 루프의 단계에 있을 때) 로드되며, 한 번에 1,000개의 텍스트만 로드됩니다. 이렇게 하면 거대한 데이터셋을 처리하더라도 모든 메모리를 소진하지 않습니다.

제너레이터 객체의 문제는 한 번만 사용할 수 있다는 것입니다. 따라서 다음과 같이 하면 처음 10개 숫자의 목록을 두 번 제공하는 대신:

```py
gen = (i for i in range(10))
print(list(gen))
print(list(gen))
```

한 번 얻고 빈 목록을 얻습니다:

```python out
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
[]
```

이것이 제너레이터를 반환하는 함수를 정의하는 이유입니다:

```py
def get_training_corpus():
    return (
        raw_datasets["train"][i : i + 1000]["whole_func_string"]
        for i in range(0, len(raw_datasets["train"]), 1000)
    )


training_corpus = get_training_corpus()
```

**`yield`** 문을 사용하여 **`for`** 루프 내에서 제너레이터를 정의할 수도 있습니다:

```py
def get_training_corpus():
    dataset = raw_datasets["train"]
    for start_idx in range(0, len(dataset), 1000):
        samples = dataset[start_idx : start_idx + 1000]
        yield samples["whole_func_string"]
```

이는 이전과 정확히 동일한 제너레이터를 생성하지만, 목록 이해에서 사용할 수 있는 것보다 더 복잡한 로직을 사용할 수 있습니다.

## 새 토크나이저 훈련하기 (Training a new tokenizer)

이제 텍스트 배치의 반복자 형태로 코퍼스가 있으므로, 새 토크나이저를 훈련할 준비가 되었습니다. 이를 위해 먼저 모델과 쌍을 이루려는 토크나이저(여기서는 GPT-2)를 로드해야 합니다:

```py
from transformers import AutoTokenizer

old_tokenizer = AutoTokenizer.from_pretrained("gpt2")
```

새 토크나이저를 훈련할 것이지만, 완전히 처음부터 시작하는 것을 피하기 위해 이 작업을 수행하는 것이 좋습니다. 이렇게 하면 토큰화 알고리즘이나 사용하려는 특수 토큰에 대해 아무것도 지정할 필요가 없습니다. 새 토크나이저는 GPT-2와 정확히 동일하며, 변경될 유일한 것은 코퍼스에서의 훈련에 의해 결정될 어휘입니다.

먼저 이 토크나이저가 예제 함수를 어떻게 처리하는지 살펴봅시다:

```py
example = '''def add_numbers(a, b):
    """Add the two numbers `a` and `b`."""
    return a + b'''

tokens = old_tokenizer.tokenize(example)
tokens
```

```python out
['def', 'Ġadd', '_', 'n', 'umbers', '(', 'a', ',', 'Ġb', '):', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ"""', 'Add', 'Ġthe', 'Ġtwo',
 'Ġnumbers', 'Ġ`', 'a', '`', 'Ġand', 'Ġ`', 'b', '`', '."', '""', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġreturn', 'Ġa', 'Ġ+', 'Ġb']
```

이 토크나이저에는 각각 공백과 새 줄을 나타내는 **`Ġ`** 및 **`Ċ`**와 같은 몇 가지 특수 기호가 있습니다. 보시다시피, 이것은 그다지 효율적이지 않습니다. 코드가 들여쓰기 수준을 그룹화할 수 있을 때(4개 또는 8개의 공백 세트가 코드에서 매우 일반적일 것이므로) 토크나이저가 각 공백에 대해 개별 토큰을 반환합니다. 또한 **`_`** 문자가 있는 단어를 보는 데 익숙하지 않아 함수 이름을 약간 이상하게 분할했습니다.

새 토크나이저를 훈련하고 이러한 문제를 해결하는지 확인해 봅시다. 이를 위해 **`train_new_from_iterator()`** 메서드를 사용할 것입니다:

```py
tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)
```

코퍼스가 매우 크면 이 명령은 시간이 걸릴 수 있지만, 1.6GB의 텍스트가 있는 이 데이터셋의 경우 매우 빠릅니다(12코어가 있는 AMD Ryzen 9 3900X CPU에서 1분 16초).

**`AutoTokenizer.train_new_from_iterator()`**는 사용하는 토크나이저가 "빠른" 토크나이저인 경우에만 작동한다는 점에 유의하십시오. 다음 섹션에서 보겠지만, 🤗 Transformers 라이브러리에는 두 가지 유형의 토크나이저가 포함되어 있습니다. 일부는 순전히 Python으로 작성되었고 다른 토크나이저(빠른 토크나이저)는 [Rust](https://www.rust-lang.org) 프로그래밍 언어로 작성된 🤗 Tokenizers 라이브러리를 기반으로 합니다. Python은 데이터 과학 및 딥러닝 애플리케이션에 가장 자주 사용되는 언어이지만, 빠르게 실행되도록 병렬화해야 하는 모든 것은 다른 언어로 작성되어야 합니다. 예를 들어, 모델 계산의 핵심인 행렬 곱셈은 GPU용으로 최적화된 C 라이브러리인 CUDA로 작성됩니다.

순수 Python으로 완전히 새로운 토크나이저를 훈련하는 것은 극도로 느릴 것이므로, 우리는 🤗 Tokenizers 라이브러리를 개발했습니다. GPU에서 입력 배치에 대해 모델을 실행할 수 있도록 CUDA 언어를 배울 필요가 없었던 것처럼, 빠른 토크나이저를 사용하기 위해 Rust를 배울 필요는 없다는 점에 유의하십시오. 🤗 Tokenizers 라이브러리는 내부적으로 Rust의 일부 코드를 호출하는 많은 메서드에 대한 Python 바인딩을 제공합니다. 예를 들어, 새 토크나이저의 훈련을 병렬화하거나, [챕터 3](/course/chapter3)에서 보았듯이, 입력 배치의 토큰화를 병렬화하는 데 사용됩니다.

대부분의 트랜스포머 모델에는 사용 가능한 빠른 토크나이저가 있으며(예외는 [여기](https://huggingface.co/transformers/#supported-frameworks)에서 확인할 수 있습니다), **`AutoTokenizer`** API는 사용 가능한 경우 항상 빠른 토크나이저를 선택합니다. 다음 섹션에서는 빠른 토크나이저가 가진 다른 특별한 기능 중 일부를 살펴볼 것입니다. 이는 토큰 분류 및 질문 답변과 같은 작업에 정말 유용할 것입니다. 그러나 그 전에, 이전 예제에서 새로 만든 토크나이저를 시도해 봅시다:

```py
tokens = tokenizer.tokenize(example)
tokens
```

```python out
['def', 'Ġadd', '_', 'numbers', '(', 'a', ',', 'Ġb', '):', 'ĊĠĠĠ', 'Ġ"""', 'Add', 'Ġthe', 'Ġtwo', 'Ġnumbers', 'Ġ`',
 'a', '`', 'Ġand', 'Ġ`', 'b', '`."""', 'ĊĠĠĠ', 'Ġreturn', 'Ġa', 'Ġ+', 'Ġb']
```

여기서 다시 공백과 새 줄을 나타내는 특수 기호 **`Ġ`**와 **`Ċ`**를 보지만, 우리의 토크나이저가 Python 함수 코퍼스에 매우 특정한 토큰을 학습했다는 것도 볼 수 있습니다. 예를 들어, 들여쓰기를 나타내는 **`ĊĠĠĠ`** 토큰과 독스트링을 시작하는 세 개의 따옴표를 나타내는 **`Ġ"""`** 토큰이 있습니다. 토크나이저는 또한 **`_`**에서 함수 이름을 올바르게 분할했습니다. 이는 매우 **압축된 표현**입니다. 비교적, 동일한 예제에 일반 영어 토크나이저를 사용하면 더 긴 문장이 생성됩니다:

```py
print(len(tokens))
print(len(old_tokenizer.tokenize(example)))
```

```python out
27
36
```

다른 예시를 살펴봅시다:

```python
example = """class LinearLayer():
    def __init__(self, input_size, output_size):
        self.weight = torch.randn(input_size, output_size)
        self.bias = torch.zeros(output_size)

    def __call__(self, x):
        return x @ self.weights + self.bias
    """
tokenizer.tokenize(example)
```

```python out
['class', 'ĠLinear', 'Layer', '():', 'ĊĠĠĠ', 'Ġdef', 'Ġ__', 'init', '__(', 'self', ',', 'Ġinput', '_', 'size', ',',
 'Ġoutput', '_', 'size', '):', 'ĊĠĠĠĠĠĠĠ', 'Ġself', '.', 'weight', 'Ġ=', 'Ġtorch', '.', 'randn', '(', 'input', '_',
 'size', ',', 'Ġoutput', '_', 'size', ')', 'ĊĠĠĠĠĠĠĠ', 'Ġself', '.', 'bias', 'Ġ=', 'Ġtorch', '.', 'zeros', '(',
 'output', '_', 'size', ')', 'ĊĊĠĠĠ', 'Ġdef', 'Ġ__', 'call', '__(', 'self', ',', 'Ġx', '):', 'ĊĠĠĠĠĠĠĠ',
 'Ġreturn', 'Ġx', 'Ġ@', 'Ġself', '.', 'weights', 'Ġ+', 'Ġself', '.', 'bias', 'ĊĠĠĠĠ']
```

들여쓰기에 해당하는 토큰 외에도, 여기에서 이중 들여쓰기에 대한 토큰인 **`ĊĠĠĠĠĠĠĠ`**도 볼 수 있습니다. **`class`**, **`init`**, **`call`**, **`self`**, 및 **`return`**과 같은 특수 Python 단어는 각각 하나의 토큰으로 토큰화되며, 토크나이저가 **`_`**와 **`.`**에서 분할할 뿐만 아니라 카멜 케이스 이름도 올바르게 분할한다는 것을 알 수 있습니다. **`LinearLayer`**는 **`["ĠLinear", "Layer"]`**로 토큰화됩니다.

## 토크나이저 저장하기 (Saving the tokenizer)

나중에 사용할 수 있도록 새 토크나이저를 저장해야 합니다. 모델과 마찬가지로 **`save_pretrained()`** 메서드를 사용하여 이 작업을 수행합니다:

```py
tokenizer.save_pretrained("code-search-net-tokenizer")
```

이렇게 하면 **`code-search-net-tokenizer`**라는 새 폴더가 생성되며, 토크나이저를 다시 로드하는 데 필요한 모든 파일이 포함됩니다. 이 토크나이저를 동료 및 친구와 공유하려면 계정에 로그인하여 Hub에 업로드할 수 있습니다. 노트북에서 작업하는 경우, 이를 도와주는 편의 함수가 있습니다:

```python
from huggingface_hub import notebook_login

notebook_login()
```

이렇게 하면 Hugging Face 로그인 자격 증명을 입력할 수 있는 위젯이 표시됩니다. 노트북에서 작업하지 않는 경우, 터미널에 다음 줄을 입력하십시오:

```bash
huggingface-cli login
```

로그인했으면 다음 명령을 실행하여 토크나이저를 푸시할 수 있습니다:

```py
tokenizer.push_to_hub("code-search-net-tokenizer")
```

이렇게 하면 **`code-search-net-tokenizer`**라는 이름의 새 저장소가 당신의 네임스페이스에 생성되고 토크나이저 파일이 포함됩니다. 그런 다음 **`from_pretrained()`** 메서드를 사용하여 어디서든 토크나이저를 로드할 수 있습니다:

```py
# 자신의 토크나이저를 사용하려면 아래의 "huggingface-course"를 실제 네임스페이스로 바꾸십시오.
tokenizer = AutoTokenizer.from_pretrained("huggingface-course/code-search-net-tokenizer")
```

이제 처음부터 언어 모델을 훈련하고 당면한 작업에 대해 미세 조정할 준비가 모두 완료되었습니다! [챕터 7](/course/chapter7)에서 이 내용을 다룰 것이지만, 먼저 이 챕터의 나머지 부분에서는 빠른 토크나이저를 더 자세히 살펴보고 **`train_new_from_iterator()`** 메서드를 호출할 때 실제로 어떤 일이 발생하는지 자세히 탐구할 것입니다.


<EditOnGithub source="https://github.com/huggingface/course/blob/main/chapters/en/chapter6/2.mdx" />

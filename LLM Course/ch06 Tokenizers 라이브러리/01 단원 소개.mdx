# 서론 (Introduction)

[챕터 3](/course/chapter3)에서는 주어진 작업에 대해 모델을 미세 조정하는 방법을 살펴보았습니다. 이 작업을 수행할 때 모델이 사전 훈련된 것과 동일한 토크나이저를 사용합니다. 그러나 처음부터 모델을 훈련하려고 할 때는 어떻게 해야 할까요? 이러한 경우, 다른 도메인이나 언어의 코퍼스에서 사전 훈련된 토크나이저를 사용하는 것은 일반적으로 최적이 아닙니다. 예를 들어, 영어 코퍼스에서 훈련된 토크나이저는 일본어 텍스트 코퍼스에서는 공백 및 구두점 사용이 두 언어에서 매우 다르기 때문에 성능이 저하됩니다.

이 챕터에서는 텍스트 코퍼스에서 완전히 새로운 토크나이저를 훈련하는 방법을 배울 것입니다. 그러면 이 토크나이저를 사용하여 언어 모델을 사전 훈련할 수 있습니다. 이 모든 것은 [🤗 Tokenizers](https://github.com/huggingface/tokenizers) 라이브러리의 도움으로 이루어지며, 이 라이브러리는 [🤗 Transformers](https://github.com/huggingface/transformers) 라이브러리에서 "빠른" 토크나이저를 제공합니다. 이 라이브러리가 제공하는 기능을 자세히 살펴보고, 빠른 토크나이저가 "느린" 버전과 어떻게 다른지 탐색할 것입니다.

우리가 다룰 주제는 다음과 같습니다:

* 주어진 체크포인트에서 사용된 토크나이저와 유사한 새 토크나이저를 새 텍스트 코퍼스에서 훈련하는 방법
* 빠른 토크나이저의 특별한 기능
* 오늘날 NLP에서 사용되는 세 가지 주요 서브워드 토큰화 알고리즘의 차이점
* 🤗 Tokenizers 라이브러리로 토크나이저를 처음부터 구축하고 일부 데이터에서 훈련하는 방법

이 챕터에서 소개된 기술은 [챕터 7](/course/chapter7/6)에서 Python 소스 코드용 언어 모델을 만드는 방법을 살펴볼 섹션을 준비할 것입니다. 토크나이저를 "훈련"한다는 것이 무엇을 의미하는지부터 살펴보겠습니다.

<EditOnGithub source="https://github.com/huggingface/course/blob/main/chapters/en/chapter6/1.mdx" />

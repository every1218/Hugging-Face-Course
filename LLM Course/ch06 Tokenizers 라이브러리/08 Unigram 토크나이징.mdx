# Unigram 토큰화 (Unigram tokenization)

Unigram 알고리즘은 ALBERT, T5, mBART, Big Bird, 및 XLNet와 같은 모델에서 사용되는 토큰화 알고리즘인 [SentencePiece](https://huggingface.co/papers/1808.06226)와 함께 사용됩니다.

SentencePiece는 모든 언어가 단어를 분리하기 위해 공백을 사용하지 않는다는 사실을 다룹니다. 대신, SentencePiece는 입력을 사용할 문자 세트에 공백이 포함된 원시 입력 스트림으로 처리합니다. 그런 다음 Unigram 알고리즘을 사용하여 적절한 어휘를 구성할 수 있습니다.

<Youtube id="TGZfZVuF9Yc"/>

<Tip>

💡 이 섹션은 Unigram을 심층적으로 다루며, 전체 구현을 보여주는 것까지 진행합니다. 토큰화 알고리즘에 대한 일반적인 개요만 원하면 끝으로 건너뛸 수 있습니다.

</Tip>

## 훈련 알고리즘 (Training algorithm)

BPE 및 WordPiece와 비교하여 Unigram은 다른 방향으로 작동합니다. 큰 어휘에서 시작하여 원하는 어휘 크기에 도달할 때까지 어휘에서 토큰을 제거합니다. 이 기본 어휘를 구축하는 데 사용할 수 있는 몇 가지 옵션이 있습니다. 예를 들어, 사전 토큰화된 단어에서 가장 일반적인 부분 문자열을 가져오거나, 큰 어휘 크기로 초기 코퍼스에 BPE를 적용할 수 있습니다.

훈련의 각 단계에서 Unigram 알고리즘은 현재 어휘가 주어지면 코퍼스에 대한 손실을 계산합니다. 그런 다음 어휘의 각 기호에 대해 기호가 제거될 경우 전체 손실이 얼마나 증가하는지 계산하고, 가장 적게 증가시킬 기호를 찾습니다. 이러한 기호는 코퍼스에 대한 전체 손실에 미치는 영향이 적으므로 어떤 의미에서는 "덜 필요하며", 제거를 위한 최상의 후보입니다.

이것은 모두 비용이 많이 드는 작업이므로, 가장 낮은 손실 증가와 관련된 단일 기호만 제거하는 것이 아니라, 가장 낮은 손실 증가와 관련된 기호의 \\(p\\) 퍼센트(\\(\\in [10, 20]\\))를 제거합니다. 그런 다음 어휘가 원하는 크기에 도달할 때까지 이 프로세스를 반복합니다.

모든 단어를 토큰화할 수 있도록 기본 문자는 절대 제거하지 않습니다.

이제 이것은 여전히 ​​조금 모호합니다. 알고리즘의 주요 부분은 코퍼스에 대한 손실을 계산하고 어휘에서 일부 토큰을 제거할 때 어떻게 변하는지 확인하는 것이지만, 이를 수행하는 방법을 아직 설명하지 않았습니다. 이 단계는 Unigram 모델의 토큰화 알고리즘에 의존하므로, 다음으로 이를 자세히 살펴보겠습니다.

이전 예제의 코퍼스를 재사용할 것입니다:

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

그리고 이 예제에서는 초기 어휘에 대한 모든 엄격한 부분 문자열을 가져올 것입니다:

```
["h", "u", "g", "hu", "ug", "p", "pu", "n", "un", "b", "bu", "s", "hug", "gs", "ugs"]
```

## 토큰화 알고리즘 (Tokenization algorithm)

Unigram 모델은 각 토큰이 이전 토큰과 독립적이라고 간주하는 언어 모델 유형입니다. 토큰 X의 확률은 이전 컨텍스트가 주어지면 토큰 X의 확률일 뿐이라는 점에서 가장 간단한 언어 모델입니다. 따라서 Unigram 언어 모델을 사용하여 텍스트를 생성하면 항상 가장 일반적인 토큰을 예측할 것입니다.

주어진 토큰의 확률은 원래 코퍼스에서 해당 토큰의 빈도(찾는 횟수)를 어휘의 모든 토큰의 모든 빈도 합계로 나눈 값입니다(확률이 1로 합산되도록 보장). 예를 들어, **`"ug"`**는 **`"hug"`**, **`"pug"`**, 및 **`"hugs"`**에 존재하므로 코퍼스에서 20의 빈도를 가집니다.

어휘에서 가능한 모든 서브워드의 빈도는 다음과 같습니다:

```
("h", 15) ("u", 36) ("g", 20) ("hu", 15) ("ug", 20) ("p", 17) ("pu", 17) ("n", 16)
("un", 16) ("b", 4) ("bu", 4) ("s", 5) ("hug", 15) ("gs", 5) ("ugs", 5)
```

따라서 모든 빈도의 합계는 210이며, 서브워드 **`"ug"`**의 확률은 20/210입니다.

<Tip>

✏️ **이제 당신 차례입니다!** 위의 빈도를 계산하고 표시된 결과와 총 합계가 올바른지 다시 확인하는 코드를 작성하십시오.

</Tip>

이제 주어진 단어를 토큰화하기 위해 토큰으로의 가능한 모든 분할을 살펴보고 Unigram 모델에 따라 각 분할의 확률을 계산합니다. 모든 토큰은 독립적인 것으로 간주되므로, 이 확률은 각 토큰의 확률의 곱일 뿐입니다. 예를 들어, **`"pug"`**의 토큰화 **`["p", "u", "g"]`**는 다음 확률을 가집니다:

$$P([``p", ``u", ``g"]) = P(``p") \times P(``u") \times P(``g") = \frac{5}{210} \times \frac{36}{210} \times \frac{20}{210} = 0.000389$$

비교적으로 토큰화 **`["pu", "g"]`**는 다음 확률을 가집니다:

$$P([``pu", ``g"]) = P(``pu") \times P(``g") = \frac{5}{210} \times \frac{20}{210} = 0.0022676$$

따라서 이 토큰화가 훨씬 더 가능성이 높습니다. 일반적으로 토큰 수가 가장 적은 토큰화가 가장 높은 확률을 가질 것입니다(각 토큰에 대해 반복되는 210으로 나누기 때문). 이는 우리가 직관적으로 원하는 것, 즉 단어를 가능한 가장 적은 수의 토큰으로 분할하는 것에 해당합니다.

Unigram 모델을 사용한 단어의 토큰화는 가장 높은 확률을 가진 토큰화입니다. **`"pug"`**의 예에서 가능한 각 분할에 대해 얻을 확률은 다음과 같습니다:

```
["p", "u", "g"] : 0.000389
["p", "ug"] : 0.0022676
["pu", "g"] : 0.0022676
```

따라서 **`"pug"`**는 이러한 분할 중 먼저 발견되는 분할에 따라 **`["p", "ug"]`** 또는 **`["pu", "g"]`**로 토큰화될 것입니다(더 큰 코퍼스에서는 이와 같은 동일한 경우는 드물다는 점에 유의하십시오).

이 경우 가능한 모든 분할을 찾고 확률을 계산하는 것은 쉬웠지만, 일반적으로는 조금 더 어려울 것입니다. 이를 위해 사용되는 *Viterbi 알고리즘*이라는 고전적인 알고리즘이 있습니다. 본질적으로, 단어의 가능한 분할을 감지하기 위해 그래프를 구축할 수 있습니다. 여기서 _a_ 문자에서 _b_ 문자까지의 서브워드가 어휘에 있으면 _a_에서 _b_까지 분기가 있다고 말하고, 해당 분기에 서브워드의 확률을 할당합니다.

가장 좋은 점수를 가질 그래프의 경로를 찾기 위해 Viterbi 알고리즘은 단어의 각 위치에 대해 해당 위치에서 끝나는 가장 좋은 점수를 가진 분할을 결정합니다. 시작부터 끝까지 이동하므로, 이 가장 좋은 점수는 현재 위치에서 끝나는 모든 서브워프를 반복한 다음 이 서브워드가 시작되는 위치에서 가장 좋은 토큰화 점수를 사용하여 찾을 수 있습니다. 그런 다음 끝에 도달하기 위해 취한 경로를 펼치기만 하면 됩니다.

어휘와 단어 **`"unhug"`**를 사용하여 예제를 살펴보겠습니다. 각 위치에서 가장 좋은 점수를 가진 서브워드는 다음과 같습니다:

```
문자 0 (u): "u" (점수 0.171429)
문자 1 (n): "un" (점수 0.076191)
문자 2 (h): "un" "h" (점수 0.005442)
문자 3 (u): "un" "hu" (점수 0.005442)
문자 4 (g): "un" "hug" (점수 0.005442)
```

따라서 **`"unhug"`**는 **`["un", "hug"]`**로 토큰화될 것입니다.

<Tip>

✏️ **이제 당신 차례입니다!** 단어 **`"huggun"`**의 토큰화와 그 점수를 결정하십시오.

</Tip>

## 훈련으로 돌아가기 (Back to training)

이제 토큰화가 어떻게 작동하는지 보았으므로, 훈련 중에 사용되는 손실에 대해 조금 더 깊이 들어갈 수 있습니다. 주어진 단계에서 이 손실은 현재 어휘와 코퍼스에서 각 토큰의 빈도에 의해 결정되는 Unigram 모델을 사용하여 코퍼스의 모든 단어를 토큰화하여 계산됩니다(이전에 보았듯이).

코퍼스의 각 단어는 점수를 가지며, 손실은 이러한 점수의 음의 로그 우도(negative log likelihood)입니다. 즉, 코퍼스의 모든 단어에 대한 모든 **`-log(P(word))`**의 합계입니다.

다음 코퍼스를 사용하여 예제로 돌아갑시다:

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

각 단어의 토큰화와 해당 점수는 다음과 같습니다:

```
"hug": ["hug"] (점수 0.071428)
"pug": ["pu", "g"] (점수 0.007710)
"pun": ["pu", "n"] (점수 0.006168)
"bun": ["bu", "n"] (점수 0.001451)
"hugs": ["hug", "s"] (점수 0.001701)
```

따라서 손실은 다음과 같습니다:

$$10 \times (-\log(0.071428)) + 5 \times (-\log(0.007710)) + 12 \times (-\log(0.006168)) + 4 \times (-\log(0.001451)) + 5 \times (-\log(0.001701)) = 169.8$$

이제 각 토큰을 제거하는 것이 손실에 어떻게 영향을 미치는지 계산해야 합니다. 이것은 다소 지루한 작업이므로, 여기서는 두 토큰에 대해서만 수행하고 코드가 우리를 도울 때 전체 프로세스를 저장할 것입니다. 이 (매우) 특정한 경우, 모든 단어의 두 가지 동일한 토큰화가 있었습니다. 예를 들어, 이전에 보았듯이 **`"pug"`**는 동일한 점수로 **`["p", "ug"]`**로 토큰화될 수 있습니다. 따라서 어휘에서 **`"pu"`** 토큰을 제거하면 정확히 동일한 손실이 발생합니다.

반면에 **`"hug"`**를 제거하면 손실이 더 나빠질 것입니다. **`"hug"`**와 **`"hugs"`**의 토큰화가 다음과 같이 될 것이기 때문입니다:

```
"hug": ["hu", "g"] (점수 0.006802)
"hugs": ["hu", "gs"] (점수 0.001701)
```

이러한 변경으로 인해 손실은 다음과 같이 증가할 것입니다:

$$ - 10 \times (-\log(0.071428)) + 10 \times (-\log(0.006802)) = 23.5 $$

따라서 토큰 **`"pu"`**는 어휘에서 제거될 가능성이 높지만, **`"hug"`**는 제거되지 않을 것입니다.

## Unigram 구현하기 (Implementing Unigram)

이제 지금까지 보았던 모든 것을 코드로 구현해 봅시다. BPE 및 WordPiece와 마찬가지로, 이것은 Unigram 알고리즘의 효율적인 구현은 아니지만(오히려 그 반대), 이를 조금 더 잘 이해하는 데 도움이 될 것입니다.

예제로 이전과 동일한 코퍼스를 사용할 것입니다:

```python
corpus = [
    "This is the Hugging Face Course.",
    "This chapter is about tokenization.",
    "This section shows several tokenizer algorithms.",
    "Hopefully, you will be able to understand how they are trained and generate tokens.",
]
```

이번에는 모델로 **`xlnet-base-cased`**를 사용할 것입니다:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("xlnet-base-cased")
```

BPE 및 WordPiece와 마찬가지로, 코퍼스에서 각 단어의 발생 횟수를 세는 것으로 시작합니다:

```python
from collections import defaultdict

word_freqs = defaultdict(int)
for text in corpus:
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    new_words = [word for word, offset in words_with_offsets]
    for word in new_words:
        word_freqs[word] += 1

word_freqs
```

그런 다음, 어휘 크기보다 큰 것으로 초기 어휘를 초기화해야 합니다. 모든 기본 문자를 포함해야 하지만(그렇지 않으면 모든 단어를 토큰화할 수 없음), 더 큰 부분 문자열의 경우 가장 일반적인 문자열만 유지하므로 빈도순으로 정렬합니다:

```python
char_freqs = defaultdict(int)
subwords_freqs = defaultdict(int)
for word, freq in word_freqs.items():
    for i in range(len(word)):
        char_freqs[word[i]] += freq
        # 길이가 2 이상인 부분 문자열을 반복
        for j in range(i + 2, len(word) + 1):
            subwords_freqs[word[i:j]] += freq

# 빈도순으로 부분 문자열 정렬
sorted_subwords = sorted(subwords_freqs.items(), key=lambda x: x[1], reverse=True)
sorted_subwords[:10]
```

```python out
[(' t', 7), ('is', 5), ('er', 5), (' a', 5), (' to', 4), ('to', 4), ('en', 4), (' T', 3), (' Th', 3), (' Thi', 3)]
```

문자를 가장 좋은 부분 문자열과 그룹화하여 크기 300의 초기 어휘에 도달합니다:

```python
token_freqs = list(char_freqs.items()) + sorted_subwords[: 300 - len(char_freqs)]
token_freqs = {token: freq for token, freq in token_freqs}
```

<Tip>

💡 SentencePiece는 초기 어휘를 만들기 위해 ESA(Enhanced Suffix Array)라는 더 효율적인 알고리즘을 사용합니다.

</Tip>

다음으로, 모든 빈도의 합계를 계산하여 빈도를 확률로 변환합니다. 우리 모델의 경우 확률의 로그를 저장할 것입니다. 이는 작은 숫자를 곱하는 것보다 로그를 더하는 것이 더 수치적으로 안정적이며, 이는 모델의 손실 계산을 단순화할 것입니다:

```python
from math import log

total_sum = sum([freq for token, freq in token_freqs.items()])
model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}
```

이제 주요 함수는 Viterbi 알고리즘을 사용하여 단어를 토큰화하는 함수입니다. 이전에 보았듯이, 이 알고리즘은 단어의 각 부분 문자열의 최상의 분할을 계산하며, 이를 `best_segmentations`라는 변수에 저장할 것입니다. 단어의 각 위치(0부터 총 길이까지)에 대해 두 개의 키가 있는 딕셔너리를 저장할 것입니다. 최상의 분할에서 마지막 토큰의 시작 인덱스와 최상의 분할의 점수입니다. 마지막 토큰의 시작 인덱스를 사용하여 목록이 완전히 채워지면 전체 분할을 검색할 수 있습니다.

목록을 채우는 것은 두 개의 루프로 수행됩니다. 주 루프는 각 시작 위치를 반복하고, 두 번째 루프는 해당 시작 위치에서 시작하는 모든 부분 문자열을 시도합니다. 부분 문자열이 어휘에 있으면 해당 끝 위치까지의 단어에 대한 새로운 분할이 있으며, 이를 `best_segmentations`에 있는 것과 비교합니다.

주 루프가 완료되면 끝에서 시작하여 다음 시작 위치로 이동하고 단어의 시작 부분에 도달할 때까지 토큰을 기록하기만 하면 됩니다:

```python
def encode_word(word, model):
    best_segmentations = [{"start": 0, "score": 1}] + [
        {"start": None, "score": None} for _ in range(len(word))
    ]
    for start_idx in range(len(word)):
        # 이 부분은 루프의 이전 단계에서 제대로 채워져야 합니다.
        best_score_at_start = best_segmentations[start_idx]["score"]
        for end_idx in range(start_idx + 1, len(word) + 1):
            token = word[start_idx:end_idx]
            if token in model and best_score_at_start is not None:
                score = model[token] + best_score_at_start
                # end_idx에서 끝나는 더 나은 분할을 찾았다면 업데이트
                if (
                    best_segmentations[end_idx]["score"] is None
                    or best_segmentations[end_idx]["score"] > score
                ):
                    best_segmentations[end_idx] = {"start": start_idx, "score": score}

    segmentation = best_segmentations[-1]
    if segmentation["score"] is None:
        # 단어의 토큰화를 찾지 못함 -> unknown
        return ["<unk>"], None

    score = segmentation["score"]
    start = segmentation["start"]
    end = len(word)
    tokens = []
    while start != 0:
        tokens.insert(0, word[start:end])
        next_start = best_segmentations[start]["start"]
        end = start
        start = next_start
    tokens.insert(0, word[start:end])
    return tokens, score
```

이미 초기 모델로 일부 단어를 시도해 볼 수 있습니다:

```python
print(encode_word("Hopefully", model))
print(encode_word("This", model))
```

```python out
(['H', 'o', 'p', 'e', 'f', 'u', 'll', 'y'], 41.5157494601402)
(['This'], 6.288267030694535)
```

이제 코퍼스에 대한 모델의 손실을 계산하는 것은 쉽습니다!

```python
def compute_loss(model):
    loss = 0
    for word, freq in word_freqs.items():
        _, word_loss = encode_word(word, model)
        loss += freq * word_loss
    return loss
```

가지고 있는 모델에서 작동하는지 확인할 수 있습니다:

```python
compute_loss(model)
```

```python out
413.10377642940875
```

각 토큰에 대한 점수를 계산하는 것도 그리 어렵지 않습니다. 각 토큰을 삭제하여 얻은 모델의 손실을 계산하기만 하면 됩니다:

```python
import copy


def compute_scores(model):
    scores = {}
    model_loss = compute_loss(model)
    for token, score in model.items():
        # 길이가 1인 토큰은 항상 유지합니다.
        if len(token) == 1:
            continue
        model_without_token = copy.deepcopy(model)
        _ = model_without_token.pop(token)
        scores[token] = compute_loss(model_without_token) - model_loss
    return scores
```

주어진 토큰에 대해 시도할 수 있습니다:

```python
scores = compute_scores(model)
print(scores["ll"])
print(scores["his"])
```

**`"ll"`**는 **`"Hopefully"`**의 토큰화에 사용되며, 이를 제거하면 대신 토큰 **`"l"`**을 두 번 사용하게 될 가능성이 높으므로 양수 손실이 예상됩니다. **`"his"`**는 **`"This"`** 단어 내부에서만 사용되며, 이는 그 자체로 토큰화되므로 영(zero) 손실이 예상됩니다. 결과는 다음과 같습니다:

```python out
6.376412403623874
0.0
```

<Tip>

💡 이 접근 방식은 매우 비효율적이므로, SentencePiece는 토큰 X가 없는 모델의 손실에 대한 근사치를 사용합니다. 처음부터 시작하는 대신 토큰 X를 남아 있는 어휘의 분할로 대체합니다. 이 방식으로 모든 점수는 모델 손실과 동시에 한 번에 계산될 수 있습니다.

</Tip>

이 모든 것이 준비되었으므로, 우리가 해야 할 마지막 일은 모델에서 사용되는 특수 토큰을 어휘에 추가한 다음, 원하는 크기에 도달할 때까지 어휘에서 충분한 토큰을 가지치기(prune)할 때까지 루프를 돌리는 것입니다:

```python
percent_to_remove = 0.1
while len(model) > 100:
    scores = compute_scores(model)
    sorted_scores = sorted(scores.items(), key=lambda x: x[1])
    # 가장 낮은 점수를 가진 percent_to_remove 토큰을 제거합니다.
    for i in range(int(len(model) * percent_to_remove)):
        _ = token_freqs.pop(sorted_scores[i][0])

    total_sum = sum([freq for token, freq in token_freqs.items()])
    model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}
```

그런 다음 일부 텍스트를 토큰화하려면 사전 토큰화를 적용한 다음 **`encode_word()`** 함수를 사용하기만 하면 됩니다:

```python
def tokenize(text, model):
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word for word, offset in words_with_offsets]
    encoded_words = [encode_word(word, model)[0] for word in pre_tokenized_text]
    return sum(encoded_words, [])


tokenize("This is the Hugging Face course.", model)
```

```python out
[' This', ' is', ' the', ' Hugging', ' Face', ' ', 'c', 'ou', 'r', 's', 'e', '.']
```

<Tip>

XLNetTokenizer는 SentencePiece를 사용하므로 **`"_"`** 문자가 포함됩니다. SentencePiece로 디코딩하려면 모든 토큰을 연결하고 **`"_"`**를 공백으로 대체하십시오.

</Tip>

Unigram은 여기까지입니다! 이제 토크나이저에 대한 모든 것에 전문가가 된 것 같은 기분이 드셨기를 바랍니다. 다음 섹션에서는 🤗 Tokenizers 라이브러리의 빌딩 블록을 탐구하고 이를 사용하여 자신만의 토크나이저를 구축하는 방법을 보여줄 것입니다.


<EditOnGithub source="https://github.com/huggingface/course/blob/main/chapters/en/chapter6/7.mdx" />

# WordPiece 토큰화 (WordPiece tokenization)

WordPiece는 Google이 BERT를 사전 훈련하기 위해 개발한 토큰화 알고리즘입니다. 그 이후로 DistilBERT, MobileBERT, Funnel Transformers, 및 MPNET와 같이 BERT를 기반으로 하는 꽤 많은 트랜스포머 모델에서 재사용되었습니다. 훈련 측면에서는 BPE와 매우 유사하지만, 실제 토큰화는 다르게 수행됩니다.

<Youtube id="qpv6ms_t_1A"/>

<Tip>

💡 이 섹션은 WordPiece를 심층적으로 다루며, 전체 구현을 보여주는 것까지 진행합니다. 토큰화 알고리즘에 대한 일반적인 개요만 원하면 끝으로 건너뛸 수 있습니다.

</Tip>

## 훈련 알고리즘 (Training algorithm)

<Tip warning={true}>

⚠️ Google은 WordPiece의 훈련 알고리즘 구현을 오픈 소스로 공개한 적이 없으므로, 다음에 나오는 내용은 공개된 문헌을 기반으로 한 우리의 최선의 추측입니다. 100% 정확하지 않을 수 있습니다.

</Tip>

BPE와 마찬가지로 WordPiece는 모델에서 사용되는 특수 토큰과 초기 알파벳을 포함하는 작은 어휘에서 시작합니다. 서브워드를 접두사(**`##`**, BERT의 경우와 같은)를 추가하여 식별하므로, 각 단어는 처음에 단어 내의 모든 문자에 해당 접두사를 추가하여 분할됩니다. 예를 들어, **`"word"`**는 다음과 같이 분할됩니다:

```
w ##o ##r ##d
```

따라서 초기 알파벳에는 단어 시작 부분에 있는 모든 문자와 WordPiece 접두사가 붙은 단어 내부에 있는 문자가 포함됩니다.

그런 다음, BPE와 마찬가지로 WordPiece는 병합 규칙을 학습합니다. 주요 차이점은 병합할 쌍을 선택하는 방식입니다. 가장 자주 발생하는 쌍을 선택하는 대신, WordPiece는 다음 공식을 사용하여 각 쌍에 대한 **점수**를 계산합니다:

$$\mathrm{score} = (\mathrm{freq\_of\_pair}) / (\mathrm{freq\_of\_first\_element} \times \mathrm{freq\_of\_second\_element})$$

쌍의 빈도를 각 부분의 빈도 곱으로 나누어, 알고리즘은 개별 부분의 빈도가 어휘에서 덜 빈번한 쌍의 병합을 우선시합니다. 예를 들어, **`("un", "##able")`** 쌍이 어휘에서 매우 자주 발생하더라도, 두 쌍 **`"un"`**과 **`"##able"`**이 다른 많은 단어에도 나타나고 높은 빈도를 가질 가능성이 높기 때문에 반드시 병합되지는 않습니다. 대조적으로, **`("hu", "##gging")`**과 같은 쌍은 **`"hu"`**와 **`"##gging"`**이 개별적으로 덜 빈번할 가능성이 높기 때문에 더 빨리 병합될 것입니다(단어 "hugging"이 어휘에 자주 나타난다고 가정).

BPE 훈련 예제에서 사용한 것과 동일한 어휘를 살펴보겠습니다:

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

여기서 분할은 다음과 같습니다:

```
("h" "##u" "##g", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("h" "##u" "##g" "##s", 5)
```

따라서 초기 어휘는 **`["b", "h", "p", "##g", "##n", "##s", "##u"]`**가 될 것입니다(일단 특수 토큰은 잊어버리십시오). 가장 자주 발생하는 쌍은 **`("##u", "##g")`**(20번 존재)이지만, **`"##u"`**의 개별 빈도가 매우 높기 때문에 점수가 가장 높지는 않습니다(1/36입니다). 실제로 **`"##u"`**가 있는 모든 쌍은 동일한 점수(1/36)를 가지므로, 가장 좋은 점수는 **`"##u"`**가 없는 유일한 쌍인 **`("##g", "##s")`**에 주어지며(1/20), 학습된 첫 번째 병합 규칙은 **`("##g", "##s") -> ("##gs")`**입니다.

병합할 때 두 토큰 사이의 **`##`**를 제거하므로, **`"##gs"`**를 어휘에 추가하고 코퍼스의 단어에 병합을 적용합니다:

```
Vocabulary: ["b", "h", "p", "##g", "##n", "##s", "##u", "##gs"]
Corpus: ("h" "##u" "##g", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("h" "##u" "##gs", 5)
```

이 시점에서 **`"##u"`**는 가능한 모든 쌍에 있으므로, 모두 동일한 점수로 끝납니다. 이 경우 첫 번째 쌍이 병합된다고 가정하면 **`("h", "##u") -> "hu"`**가 됩니다. 이는 다음으로 이어집니다:

```
Vocabulary: ["b", "h", "p", "##g", "##n", "##s", "##u", "##gs", "hu"]
Corpus: ("hu" "##g", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("hu" "##gs", 5)
```

그런 다음 다음으로 가장 좋은 점수는 **`("hu", "##g")`**와 **`("hu", "##gs")`**가 공유하므로(다른 모든 쌍의 1/21에 비해 1/15), 가장 큰 점수를 가진 첫 번째 쌍이 병합됩니다:

```
Vocabulary: ["b", "h", "p", "##g", "##n", "##s", "##u", "##gs", "hu", "hug"]
Corpus: ("hug", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("hu" "##gs", 5)
```

그리고 원하는 어휘 크기에 도달할 때까지 이와 같이 계속합니다.

<Tip>

✏️ **이제 당신 차례입니다!** 다음 병합 규칙은 무엇이 될까요?

</Tip>

## 토큰화 알고리즘 (Tokenization algorithm)

WordPiece와 BPE의 토큰화는 WordPiece가 학습된 병합 규칙이 아닌 최종 어휘만 저장한다는 점에서 다릅니다. 토큰화할 단어에서 시작하여, WordPiece는 어휘에 있는 가장 긴 서브워드를 찾은 다음 분할합니다. 예를 들어, 위 예제에서 학습된 어휘를 사용하면 단어 **`"hugs"`**의 경우 어휘에 있는 단어 시작 부분의 가장 긴 서브워드는 **`"hug"`**이므로 거기서 분할하고 **`["hug", "##s"]`**를 얻습니다. 그런 다음 어휘에 있는 **`"##s"`**로 계속 진행하므로 **`"hugs"`**의 토큰화는 **`["hug", "##s"]`**입니다.

BPE의 경우, 학습된 병합을 순서대로 적용하여 **`["hu", "##gs"]`**로 토큰화했을 것이므로 인코딩이 다릅니다.

다른 예로, 단어 **`"bugs"`**가 어떻게 토큰화될지 살펴보겠습니다. **`"b"`**는 어휘에 있는 단어 시작 부분의 가장 긴 서브워드이므로 거기서 분할하고 **`["b", "##ugs"]`**를 얻습니다. 그런 다음 **`"##u"`**는 **`"##ugs"`** 시작 부분의 어휘에 있는 가장 긴 서브워드이므로 거기서 분할하고 **`["b", "##u", "##gs"]`**를 얻습니다. 마지막으로 **`"##gs"`**는 어휘에 있으므로 이 마지막 목록이 **`"bugs"`**의 토큰화입니다.

토큰화가 어휘에서 서브워드를 찾을 수 없는 단계에 도달하면 전체 단어가 unknown으로 토큰화됩니다. 따라서 예를 들어 **`"mug"`**는 **`["[UNK]"]`**로 토큰화되고, **`"bum"`**도 마찬가지입니다( **`"b"`**와 **`"##u"`**로 시작할 수 있더라도 **`"##m"`**은 어휘에 없으며, 결과 토큰화는 **`["b", "##u", "[UNK]"]`**가 아닌 **`["[UNK]"]`**일 뿐입니다). 이는 unknown으로 어휘에 없는 개별 문자만 분류하는 BPE와의 또 다른 차이점입니다.

<Tip>

✏️ **이제 당신 차례입니다!** 단어 **`"pugs"`**는 어떻게 토큰화될까요?

</Tip>

## WordPiece 구현하기 (Implementing WordPiece)

이제 WordPiece 알고리즘의 구현을 살펴보겠습니다. BPE와 마찬가지로, 이것은 단지 교육적이며, 큰 코퍼스에서 사용할 수는 없을 것입니다.

BPE 예제와 동일한 코퍼스를 사용할 것입니다:

```python
corpus = [
    "This is the Hugging Face Course.",
    "This chapter is about tokenization.",
    "This section shows several tokenizer algorithms.",
    "Hopefully, you will be able to understand how they are trained and generate tokens.",
]
```

먼저, 코퍼스를 단어로 사전 토큰화해야 합니다. WordPiece 토크나이저(BERT와 같은)를 복제하고 있으므로 사전 토큰화에 **`bert-base-cased`** 토크나이저를 사용할 것입니다:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
```

그런 다음 사전 토큰화를 수행할 때 코퍼스의 각 단어의 빈도를 계산합니다:

```python
from collections import defaultdict

word_freqs = defaultdict(int)
for text in corpus:
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    new_words = [word for word, offset in words_with_offsets]
    for word in new_words:
        word_freqs[word] += 1

word_freqs
```

```python out
defaultdict(
    int, {'This': 3, 'is': 2, 'the': 1, 'Hugging': 1, 'Face': 1, 'Course': 1, '.': 4, 'chapter': 1, 'about': 1,
    'tokenization': 1, 'section': 1, 'shows': 1, 'several': 1, 'tokenizer': 1, 'algorithms': 1, 'Hopefully': 1,
    ',': 1, 'you': 1, 'will': 1, 'be': 1, 'able': 1, 'to': 1, 'understand': 1, 'how': 1, 'they': 1, 'are': 1,
    'trained': 1, 'and': 1, 'generate': 1, 'tokens': 1})
```

이전에 보았듯이, 알파벳은 단어의 모든 첫 글자와 **`##`**가 접두사로 붙은 단어에 나타나는 다른 모든 글자로 구성된 고유한 세트입니다:

```python
alphabet = []
for word in word_freqs.keys():
    if word[0] not in alphabet:
        alphabet.append(word[0])
    for letter in word[1:]:
        if f"##{letter}" not in alphabet:
            alphabet.append(f"##{letter}")

alphabet.sort()
alphabet

print(alphabet)
```

```python out
['##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s',
 '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u',
 'w', 'y']
```

또한 해당 어휘의 시작 부분에 모델에서 사용되는 특수 토큰을 추가합니다. BERT의 경우, **`["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"]`** 목록입니다:

```python
vocab = ["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"] + alphabet.copy()
```

다음으로, 첫 번째 글자가 아닌 모든 글자에 **`##`**가 접두사로 붙도록 각 단어를 분할해야 합니다:

```python
splits = {
    word: [c if i == 0 else f"##{c}" for i, c in enumerate(word)]
    for word in word_freqs.keys()
}
```

이제 훈련할 준비가 되었으므로, 각 쌍의 점수를 계산하는 함수를 작성해 봅시다. 훈련의 각 단계에서 이 함수가 필요합니다:

```python
def compute_pair_scores(splits):
    letter_freqs = defaultdict(int)
    pair_freqs = defaultdict(int)
    for word, freq in word_freqs.items():
        split = splits[word]
        if len(split) == 1:
            letter_freqs[split[0]] += freq
            continue
        for i in range(len(split) - 1):
            pair = (split[i], split[i + 1])
            letter_freqs[split[i]] += freq
            pair_freqs[pair] += freq
        letter_freqs[split[-1]] += freq

    scores = {
        pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])
        for pair, freq in pair_freqs.items()
    }
    return scores
```

초기 분할 후 이 딕셔너리의 일부를 살펴봅시다:

```python
pair_scores = compute_pair_scores(splits)
for i, key in enumerate(pair_scores.keys()):
    print(f"{key}: {pair_scores[key]}")
    if i >= 5:
        break
```

```python out
('T', '##h'): 0.125
('##h', '##i'): 0.03409090909090909
('##i', '##s'): 0.02727272727272727
('i', '##s'): 0.1
('t', '##h'): 0.03571428571428571
('##h', '##e'): 0.011904761904761904
```

이제 가장 좋은 점수를 가진 쌍을 찾는 데는 빠른 루프만 있으면 됩니다:

```python
best_pair = ""
max_score = None
for pair, score in pair_scores.items():
    if max_score is None or max_score < score:
        best_pair = pair
        max_score = score

print(best_pair, max_score)
```

```python out
('a', '##b') 0.2
```

따라서 학습할 첫 번째 병합은 **`('a', '##b') -> 'ab'`**이며, **`'ab'`**를 어휘에 추가합니다:

```python
vocab.append("ab")
```

계속하려면 **`splits`** 딕셔너리에서 해당 병합을 적용해야 합니다. 이를 위한 또 다른 함수를 작성해 봅시다:

```python
def merge_pair(a, b, splits):
    for word in word_freqs:
        split = splits[word]
        if len(split) == 1:
            continue
        i = 0
        while i < len(split) - 1:
            if split[i] == a and split[i + 1] == b:
                merge = a + b[2:] if b.startswith("##") else a + b
                split = split[:i] + [merge] + split[i + 2 :]
            else:
                i += 1
        splits[word] = split
    return splits
```

그리고 첫 번째 병합의 결과를 살펴볼 수 있습니다:

```py
splits = merge_pair("a", "##b", splits)
splits["about"]
```

```python out
['ab', '##o', '##u', '##t']
```

이제 원하는 모든 병합을 학습할 때까지 루프를 돌리는 데 필요한 모든 것이 있습니다. 어휘 크기를 70으로 목표로 합시다:

```python
vocab_size = 70
while len(vocab) < vocab_size:
    scores = compute_pair_scores(splits)
    best_pair, max_score = "", None
    for pair, score in scores.items():
        if max_score is None or max_score < score:
            best_pair = pair
            max_score = score
    splits = merge_pair(*best_pair, splits)
    new_token = (
        best_pair[0] + best_pair[1][2:]
        if best_pair[1].startswith("##")
        else best_pair[0] + best_pair[1]
    )
    vocab.append(new_token)
```

그런 다음 생성된 어휘를 볼 수 있습니다:

```py
print(vocab)
```

```python out
['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k',
 '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H',
 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', 'ab', '##fu', 'Fa', 'Fac', '##ct', '##ful', '##full', '##fully',
 'Th', 'ch', '##hm', 'cha', 'chap', 'chapt', '##thm', 'Hu', 'Hug', 'Hugg', 'sh', 'th', 'is', '##thms', '##za', '##zat',
 '##ut']
```

보시다시피, BPE와 비교하여 이 토크나이저는 단어의 일부를 토큰으로 조금 더 빨리 학습합니다.

<Tip>

💡 동일한 코퍼스에서 **`train_new_from_iterator()`**를 사용하는 것은 정확히 동일한 어휘를 생성하지 않을 것입니다. 이는 🤗 Tokenizers 라이브러리가 (내부 작동 방식에 대해 완전히 확신하지 못하므로) 훈련을 위해 WordPiece를 구현하지 않고 대신 BPE를 사용하기 때문입니다.

</Tip>

새 텍스트를 토큰화하려면, 이를 사전 토큰화하고, 분할한 다음, 각 단어에 토큰화 알고리즘을 적용합니다. 즉, 첫 번째 단어의 시작 부분에서 어휘에 있는 가장 큰 서브워드를 찾고 분할한 다음, 두 번째 부분에 대해 프로세스를 반복하는 식이며, 텍스트의 나머지 단어에 대해서도 마찬가지입니다:

```python
def encode_word(word):
    tokens = []
    while len(word) > 0:
        i = len(word)
        while i > 0 and word[:i] not in vocab:
            i -= 1
        if i == 0:
            return ["[UNK]"]
        tokens.append(word[:i])
        word = word[i:]
        if len(word) > 0:
            word = f"##{word}"
    return tokens
```

어휘에 있는 단어와 없는 단어를 하나씩 테스트해 봅시다:

```python
print(encode_word("Hugging"))
print(encode_word("HOgging"))
```

```python out
['Hugg', '##i', '##n', '##g']
['[UNK]']
```

이제 텍스트를 토큰화하는 함수를 작성해 봅시다:

```python
def tokenize(text):
    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word for word, offset in pre_tokenize_result]
    encoded_words = [encode_word(word) for word in pre_tokenized_text]
    return sum(encoded_words, [])
```

모든 텍스트에서 시도할 수 있습니다:

```python
tokenize("This is the Hugging Face course!")
```

```python out
['Th', '##i', '##s', 'is', 'th', '##e', 'Hugg', '##i', '##n', '##g', 'Fac', '##e', 'c', '##o', '##u', '##r', '##s',
 '##e', '[UNK]']
```

WordPiece 알고리즘은 여기까지입니다! 이제 Unigram을 살펴보겠습니다.


<EditOnGithub source="https://github.com/huggingface/course/blob/main/chapters/en/chapter6/6.mdx" />

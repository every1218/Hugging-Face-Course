# 빠른 토크나이저의 특별한 능력 (Fast tokenizers' special powers)

이 섹션에서는 🤗 Transformers의 토크나이저 기능을 더 자세히 살펴볼 것입니다. 지금까지는 입력을 토큰화하거나 ID를 텍스트로 다시 디코딩하는 데만 사용했지만, 토크나이저, 특히 🤗 Tokenizers 라이브러리를 기반으로 하는 토크나이저는 훨씬 더 많은 작업을 수행할 수 있습니다. 이러한 추가 기능을 설명하기 위해, [챕터 1](/course/chapter1)에서 처음 접했던 **`token-classification`**(우리는 **`ner`**라고 불렀습니다) 및 **`question-answering`** 파이프라인의 결과를 재현하는 방법을 탐색할 것입니다.

<Youtube id="g8quOxoqhHQ"/>

다음 논의에서는 "느린" 토크나이저와 "빠른" 토크나이저를 자주 구별할 것입니다. 느린 토크나이저는 🤗 Transformers 라이브러리 내에서 Python으로 작성된 토크나이저인 반면, 빠른 버전은 Rust 프로그래밍 언어로 작성된 🤗 Tokenizers 라이브러리에서 제공하는 토크나이저입니다. [챕터 5](/course/chapter5/3)에서 빠른 토크나이저와 느린 토크나이저가 Drug Review Dataset을 토큰화하는 데 걸린 시간을 보고한 표를 기억한다면, 왜 우리가 그것들을 빠르고 느리다고 부르는지 짐작할 수 있을 것입니다:

|               | 빠른 토크나이저 | 느린 토크나이저 |
|:--------------:|:--------------:|:-------------:|
| `batched=True`  | 10.8초          | 4분 41초 |
| `batched=False` | 59.2초          | 5분 3초 |

<Tip warning={true}>

⚠️ 단일 문장을 토큰화할 때, 동일한 토크나이저의 느린 버전과 빠른 버전 간에 속도 차이를 항상 볼 수는 없습니다. 사실, 빠른 버전이 실제로 더 느릴 수도 있습니다! 많은 텍스트를 동시에 병렬로 토큰화할 때만 그 차이를 명확하게 볼 수 있습니다.

</Tip>

## 배치 인코딩 (Batch encoding)

<Youtube id="3umI3tm27Vw"/>

토크나이저의 출력은 단순한 Python 딕셔너리가 아닙니다. 우리가 얻는 것은 실제로는 특수한 **`BatchEncoding`** 객체입니다. 이는 딕셔너리의 서브클래스입니다(이전에 문제 없이 결과를 인덱싱할 수 있었던 이유입니다)만, 주로 빠른 토크나이저에서 사용되는 추가 메서드를 포함합니다.

병렬화 기능 외에도, 빠른 토크나이저의 핵심 기능은 최종 토큰이 파생된 원본 텍스트의 범위를 항상 추적한다는 것입니다. 우리는 이 기능을 *오프셋 매핑(offset mapping)*이라고 부릅니다. 이는 결국 각 단어를 생성한 토큰에 매핑하거나, 원본 텍스트의 각 문자를 토큰이 포함된 위치에 매핑하는 기능 등을 가능하게 합니다.

예시를 살펴보겠습니다:

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
example = "My name is Sylvain and I work at Hugging Face in Brooklyn."
encoding = tokenizer(example)
print(type(encoding))
```

이전에 언급했듯이, 토크나이저 출력에서 **`BatchEncoding`** 객체를 얻습니다:

```python out
<class 'transformers.tokenization_utils_base.BatchEncoding'>
```

**`AutoTokenizer`** 클래스는 기본적으로 빠른 토크나이저를 선택하므로, 이 **`BatchEncoding`** 객체가 제공하는 추가 메서드를 사용할 수 있습니다. 우리의 토크나이저가 빠른 토크나이저인지 느린 토크나이저인지 확인하는 두 가지 방법이 있습니다. 토크나이저의 **`is_fast`** 속성을 확인하거나:

```python
tokenizer.is_fast
```

```python out
True
```

**`encoding`**의 동일한 속성을 확인할 수 있습니다:

```python
encoding.is_fast
```

```python out
True
```

빠른 토크나이저가 무엇을 가능하게 하는지 살펴봅시다. 먼저, ID를 다시 토큰으로 변환할 필요 없이 토큰에 액세스할 수 있습니다:

```py
encoding.tokens()
```

```python out
['[CLS]', 'My', 'name', 'is', 'S', '##yl', '##va', '##in', 'and', 'I', 'work', 'at', 'Hu', '##gging', 'Face', 'in',
 'Brooklyn', '.', '[SEP]']
```

이 경우 인덱스 5의 토큰은 **`##yl`**이며, 이는 원본 문장의 "Sylvain"이라는 단어의 일부입니다. 또한 **`word_ids()`** 메서드를 사용하여 각 토큰이 파생된 단어의 인덱스를 얻을 수 있습니다:

```py
encoding.word_ids()
```

```python out
[None, 0, 1, 2, 3, 3, 3, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, None]
```

토크나이저의 특수 토큰인 **`[CLS]`**와 **`[SEP]`**는 **`None`**에 매핑되고, 각 토큰은 파생된 단어에 매핑됨을 알 수 있습니다. 이는 토큰이 단어의 시작 부분에 있는지 또는 두 토큰이 동일한 단어 내에 있는지 확인하는 데 특히 유용합니다. 이를 위해 **`##`** 접두사에 의존할 수도 있지만, 이는 BERT와 유사한 토크나이저에서만 작동합니다. 이 메서드는 토크나이저가 빠른 토크나이저인 한 모든 유형의 토크나이저에서 작동합니다. 다음 챕터에서는 이 기능을 사용하여 명명된 개체 인식(NER) 및 품사 태그 지정(POS)과 같은 작업에서 각 단어에 대해 가지고 있는 레이블을 토큰에 올바르게 적용하는 방법을 알아볼 것입니다. 또한 이를 사용하여 마스크된 언어 모델링에서 동일한 단어에서 파생된 모든 토큰을 마스킹하는 데 사용할 수 있습니다(*전체 단어 마스킹*이라는 기술).

<Tip>

단어의 개념은 복잡합니다. 예를 들어, "I'll"("I will"의 축약형)은 하나 또는 두 개의 단어로 간주됩니까? 이는 실제로 토크나이저와 적용하는 사전 토큰화 작업에 따라 다릅니다. 일부 토크나이저는 공백으로만 분할하므로 이를 하나의 단어로 간주합니다. 다른 토크나이저는 공백 외에도 구두점을 사용하므로 두 단어로 간주합니다.

✏️ **직접 해보세요!** **`bert-base-cased`** 및 **`roberta-base`** 체크포인트에서 토크나이저를 만들고 이들을 사용하여 "81s"를 토큰화하십시오. 무엇을 관찰합니까? 단어 ID는 무엇입니까?

</Tip>

마찬가지로, 토큰을 토큰이 파생된 문장에 매핑하는 데 사용할 수 있는 **`sentence_ids()`** 메서드가 있습니다(이 경우 토크나이저가 반환하는 **`token_type_ids`**가 동일한 정보를 제공할 수 있습니다).

마지막으로, **`word_to_chars()`** 또는 **`token_to_chars()`** 및 **`char_to_word()`** 또는 **`char_to_token()`** 메서드를 통해 모든 단어 또는 토큰을 원본 텍스트의 문자에 매핑하고 그 반대로도 매핑할 수 있습니다. 예를 들어, **`word_ids()`** 메서드는 **`##yl`**이 인덱스 3의 단어의 일부임을 알려주었지만, 문장에서 어떤 단어에 있습니까? 다음과 같이 확인할 수 있습니다:

```py
start, end = encoding.word_to_chars(3)
example[start:end]
```

우리는 다음을 얻습니다:

```python out
Sylvain
```

이전에 언급했듯이, 이 모든 것은 빠른 토크나이저가 각 토큰이 파생된 텍스트 범위를 *오프셋* 목록으로 추적한다는 사실에 의해 구동됩니다. 사용법을 설명하기 위해, 다음으로 **`token-classification`** 파이프라인의 결과를 수동으로 재현하는 방법을 보여줄 것입니다.

<Tip>

✏️ **직접 해보세요!** 자신만의 예제 텍스트를 만들고 어떤 토큰이 단어 ID와 연결되어 있는지, 그리고 단일 단어에 대한 문자 범위를 추출하는 방법을 이해할 수 있는지 확인해 보세요. 추가 점수를 위해 두 문장을 입력으로 사용하고 문장 ID가 당신에게 의미가 있는지 확인해 보세요.

</Tip>

## `token-classification` 파이프라인 내부 (Inside the `token-classification` pipeline)

[챕터 1](/course/chapter1)에서 🤗 Transformers **`pipeline()`** 함수를 사용하여 NER(작업은 사람, 위치 또는 조직과 같은 개체에 해당하는 텍스트 부분을 식별하는 것)을 적용하는 것을 처음 맛봤습니다. 그런 다음 [챕터 2](/course/chapter2)에서 파이프라인이 원시 텍스트에서 예측을 얻는 데 필요한 세 단계를 어떻게 그룹화하는지 보았습니다: 토큰화, 모델을 통한 입력 전달 및 후처리. **`token-classification`** 파이프라인의 처음 두 단계는 다른 파이프라인과 동일하지만, 후처리는 조금 더 복잡합니다. 어떻게 되는지 봅시다!

<Youtube id="0E7ltQB7fM8"/>

### 파이프라인으로 기본 결과 얻기 (Getting the base results with the pipeline)

먼저 토큰 분류 파이프라인을 가져와 수동으로 비교할 결과를 얻을 수 있습니다. 기본적으로 사용되는 모델은 [`dbmdz/bert-large-cased-finetuned-conll03-english`](https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english)입니다. 문장에서 NER을 수행합니다:

```py
from transformers import pipeline

token_classifier = pipeline("token-classification")
token_classifier("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

```python out
[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S', 'start': 11, 'end': 12},
 {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14},
 {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va', 'start': 14, 'end': 16},
 {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in', 'start': 16, 'end': 18},
 {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35},
 {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40},
 {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45},
 {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

모델은 "Sylvain"에서 생성된 각 토큰을 사람으로, "Hugging Face"에서 생성된 각 토큰을 조직으로, "Brooklyn" 토큰을 위치로 올바르게 식별했습니다. 또한 파이프라인에 동일한 개체에 해당하는 토큰을 그룹화하도록 요청할 수 있습니다:

```py
from transformers import pipeline

token_classifier = pipeline("token-classification", aggregation_strategy="simple")
token_classifier("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

```python out
[{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18},
 {'entity_group': 'ORG', 'score': 0.97960204, 'word': 'Hugging Face', 'start': 33, 'end': 45},
 {'entity_group': 'LOC', 'score': 0.99321055, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

선택된 **`aggregation_strategy`**는 각 그룹화된 개체에 대해 계산된 점수를 변경합니다. **`"simple"`**의 경우 점수는 주어진 개체의 각 토큰 점수의 평균입니다. 예를 들어, "Sylvain"의 점수는 이전 예제에서 토큰 **`S`**, **`##yl`**, **`##va`**, 및 **`##in`**에 대해 본 점수의 평균입니다. 사용 가능한 다른 전략은 다음과 같습니다:

- **`"first"`**: 각 개체의 점수는 해당 개체의 첫 번째 토큰 점수입니다(따라서 "Sylvain"의 경우 토큰 **`S`**의 점수인 0.993828이 될 것입니다).
- **`"max"`**: 각 개체의 점수는 해당 개체의 토큰에서 최대 점수입니다(따라서 "Hugging Face"의 경우 "Face"의 점수인 0.98879766이 될 것입니다).
- **`"average"`**: 각 개체의 점수는 해당 개체를 구성하는 단어 점수의 평균입니다(따라서 "Sylvain"의 경우 **`"simple"`** 전략과 차이가 없지만, "Hugging Face"는 "Hugging"의 점수 0.975와 "Face"의 점수 0.98879의 평균인 0.9819의 점수를 가질 것입니다).

이제 **`pipeline()`** 함수를 사용하지 않고 이러한 결과를 얻는 방법을 살펴봅시다!

### 입력에서 예측으로 (From inputs to predictions)

먼저 입력을 토큰화하고 모델을 통과시켜야 합니다. 이는 [챕터 2](/course/chapter2)에서와 같이 정확히 수행됩니다. **`AutoXxx`** 클래스를 사용하여 토크나이저와 모델을 인스턴스화한 다음, 예제에서 사용합니다:

```py
from transformers import AutoTokenizer, AutoModelForTokenClassification

model_checkpoint = "dbmdz/bert-large-cased-finetuned-conll03-english"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)

example = "My name is Sylvain and I work at Hugging Face in Brooklyn."
inputs = tokenizer(example, return_tensors="pt")
outputs = model(**inputs)
```

여기서 **`AutoModelForTokenClassification`**을 사용하고 있으므로, 입력 시퀀스의 각 토큰에 대해 하나의 로짓 세트를 얻습니다:

```py
print(inputs["input_ids"].shape)
print(outputs.logits.shape)
```

```python out
torch.Size([1, 19])
torch.Size([1, 19, 9])
```

19개의 토큰으로 구성된 1개의 시퀀스가 있는 배치와 모델에 9개의 다른 레이블이 있으므로, 모델의 출력은 1 x 19 x 9의 모양을 가집니다. 텍스트 분류 파이프라인에서와 마찬가지로, 소프트맥스 함수를 사용하여 이러한 로짓을 확률로 변환하고, argmax를 사용하여 예측을 얻습니다(소프트맥스는 순서를 변경하지 않으므로 로짓에서 argmax를 취할 수 있음에 유의하십시오):

```py
import torch

probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)[0].tolist()
predictions = outputs.logits.argmax(dim=-1)[0].tolist()
print(predictions)
```

```python out
[0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 6, 6, 6, 0, 8, 0, 0]
```

**`model.config.id2label`** 속성에는 예측을 이해하는 데 사용할 수 있는 인덱스-레이블 매핑이 포함되어 있습니다:

```py
model.config.id2label
```

```python out
{0: 'O',
 1: 'B-MISC',
 2: 'I-MISC',
 3: 'B-PER',
 4: 'I-PER',
 5: 'B-ORG',
 6: 'I-ORG',
 7: 'B-LOC',
 8: 'I-LOC'}
```

이전에 보았듯이 9개의 레이블이 있습니다. **`O`**는 명명된 개체에 속하지 않는 토큰에 대한 레이블이며("outside"를 의미합니다), 각 개체 유형(기타, 사람, 조직 및 위치)에 대해 두 개의 레이블이 있습니다. **`B-XXX`** 레이블은 토큰이 **`XXX`** 개체의 시작 부분에 있음을 나타내고, **`I-XXX`** 레이블은 토큰이 **`XXX`** 개체 내부에 있음을 나타냅니다. 예를 들어, 현재 예제에서는 모델이 토큰 **`S`**를 **`B-PER`**(사람 개체의 시작)로, 토큰 **`##yl`**, **`##va`**, 및 **`##in`**을 **`I-PER`**(사람 개체 내부)로 분류할 것으로 예상할 수 있습니다.

모델이 이 경우에 이 네 토큰 모두에 **`I-PER`** 레이블을 부여했기 때문에 모델이 틀렸다고 생각할 수 있지만, 이는 전적으로 사실은 아닙니다. 실제로 이러한 **`B-`** 및 **`I-`** 레이블에 대한 두 가지 형식인 *IOB1* 및 *IOB2*가 있습니다. IOB2 형식(아래 분홍색)은 우리가 도입한 형식이지만, IOB1 형식(파란색)에서는 **`B-`**로 시작하는 레이블은 동일한 유형의 인접한 두 개체를 분리하는 데만 사용됩니다. 우리가 사용하는 모델은 해당 형식을 사용하는 데이터셋에서 미세 조정되었으므로 **`S`** 토큰에 **`I-PER`** 레이블을 할당합니다.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/IOB_versions.svg" alt="IOB1 vs IOB2 format"/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/IOB_versions-dark.svg" alt="IOB1 vs IOB2 format"/>
</div>

이 맵을 사용하여 첫 번째 파이프라인의 결과를 (거의 완전히) 재현할 준비가 되었습니다. **`O`**로 분류되지 않은 각 토큰의 점수와 레이블을 가져올 수 있습니다:

```py
results = []
tokens = inputs.tokens()

for idx, pred in enumerate(predictions):
    label = model.config.id2label[pred]
    if label != "O":
        results.append(
            {"entity": label, "score": probabilities[idx][pred], "word": tokens[idx]}
        )

print(results)
```

```python out
[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S'},
 {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl'},
 {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va'},
 {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in'},
 {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu'},
 {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging'},
 {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face'},
 {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn'}]
```

이는 이전과 매우 유사하지만, 한 가지 예외가 있습니다. 파이프라인은 원본 문장에서 각 개체의 **`start`**와 **`end`**에 대한 정보도 제공했습니다. 여기서 오프셋 매핑이 작동합니다. 오프셋을 얻으려면 입력을 토크나이저에 적용할 때 **`return_offsets_mapping=True`**를 설정하기만 하면 됩니다:

```py
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
inputs_with_offsets["offset_mapping"]
```

```python out
[(0, 0), (0, 2), (3, 7), (8, 10), (11, 12), (12, 14), (14, 16), (16, 18), (19, 22), (23, 24), (25, 29), (30, 32),
 (33, 35), (35, 40), (41, 45), (46, 48), (49, 57), (57, 58), (0, 0)]
```

각 튜플은 각 토큰에 해당하는 텍스트의 범위이며, **`(0, 0)`**은 특수 토큰용으로 예약되어 있습니다. 이전에 인덱스 5의 토큰이 **`##yl`**이며, 여기에 오프셋 **`(12, 14)`**가 있음을 보았습니다. 예제에서 해당 슬라이스를 가져오면:

```py
example[12:14]
```

**`##`** 없이 올바른 텍스트 범위를 얻습니다:

```python out
yl
```

이를 사용하여 이제 이전 결과를 완성할 수 있습니다:

```py
results = []
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
tokens = inputs_with_offsets.tokens()
offsets = inputs_with_offsets["offset_mapping"]

for idx, pred in enumerate(predictions):
    label = model.config.id2label[pred]
    if label != "O":
        start, end = offsets[idx]
        results.append(
            {
                "entity": label,
                "score": probabilities[idx][pred],
                "word": tokens[idx],
                "start": start,
                "end": end,
            }
        )

print(results)
```

```python out
[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S', 'start': 11, 'end': 12},
 {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14},
 {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va', 'start': 14, 'end': 16},
 {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in', 'start': 16, 'end': 18},
 {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35},
 {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40},
 {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45},
 {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

이는 첫 번째 파이프라인에서 얻은 것과 동일합니다!

### 개체 그룹화 (Grouping entities)

오프셋을 사용하여 각 개체에 대한 시작 및 끝 키를 결정하는 것은 편리하지만, 이 정보가 엄격하게 필요한 것은 아닙니다. 그러나 개체를 함께 그룹화하려면 오프셋이 많은 복잡한 코드를 절약해 줄 것입니다. 예를 들어, **`Hu`**, **`##gging`**, 및 **`Face`** 토큰을 함께 그룹화하려면, 처음 두 개는 **`##`**를 제거하면서 연결해야 하고, **`Face`**는 **`##`**로 시작하지 않으므로 공백으로 추가해야 한다는 특별한 규칙을 만들 수 있습니다. 그러나 이는 이 특정 유형의 토크나이저에서만 작동할 것입니다. SentencePiece 또는 Byte-Pair-Encoding 토크나이저(이 챕터 후반부에 논의됨)에 대해서는 또 다른 규칙 세트를 작성해야 합니다.

오프셋을 사용하면 사용자 정의 코드가 모두 사라집니다. 첫 번째 토큰으로 시작하고 마지막 토큰으로 끝나는 원본 텍스트의 범위를 가져올 수 있습니다. 따라서 **`Hu`**, **`##gging`**, 및 **`Face`** 토큰의 경우 문자 33( **`Hu`**의 시작)에서 시작하고 문자 45( **`Face`**의 끝) 앞에서 끝나야 합니다:

```py
example[33:45]
```

```python out
Hugging Face
```

개체를 그룹화하는 동안 예측을 후처리하는 코드를 작성하기 위해, 우리는 연속적이고 **`I-XXX`**로 레이블이 지정된 개체들을 함께 그룹화할 것입니다. 단, 첫 번째 개체는 **`B-XXX`** 또는 **`I-XXX`**로 레이블이 지정될 수 있습니다(즉, **`O`**, 새 유형의 개체, 또는 동일한 유형의 개체가 시작되고 있음을 알려주는 **`B-XXX`**가 나올 때 그룹화를 중지합니다):

```py
import numpy as np

results = []
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
tokens = inputs_with_offsets.tokens()
offsets = inputs_with_offsets["offset_mapping"]

idx = 0
while idx < len(predictions):
    pred = predictions[idx]
    label = model.config.id2label[pred]
    if label != "O":
        # B- 또는 I- 제거
        label = label[2:]
        start, _ = offsets[idx]

        # I-label로 레이블이 지정된 모든 토큰 가져오기
        all_scores = []
        while (
            idx < len(predictions)
            and model.config.id2label[predictions[idx]] == f"I-{label}"
        ):
            all_scores.append(probabilities[idx][pred])
            _, end = offsets[idx]
            idx += 1

        # 점수는 해당 그룹화된 개체의 모든 토큰 점수의 평균입니다.
        score = np.mean(all_scores).item()
        word = example[start:end]
        results.append(
            {
                "entity_group": label,
                "score": score,
                "word": word,
                "start": start,
                "end": end,
            }
        )
    idx += 1

print(results)
```

그리고 두 번째 파이프라인과 동일한 결과를 얻습니다!

```python out
[{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18},
 {'entity_group': 'ORG', 'score': 0.97960204, 'word': 'Hugging Face', 'start': 33, 'end': 45},
 {'entity_group': 'LOC', 'score': 0.99321055, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

이러한 오프셋이 매우 유용한 작업의 또 다른 예는 질문 답변입니다. 다음 섹션에서 다룰 이 파이프라인에 대해 자세히 살펴보면, 🤗 Transformers 라이브러리의 마지막 기능 중 하나를 살펴볼 수도 있습니다. 즉, 입력을 주어진 길이로 자를 때 **오버플로되는 토큰**을 처리하는 것입니다.


<EditOnGithub source="https://github.com/huggingface/course/blob/main/chapters/en/chapter6/3.mdx" />

# 챕터 마무리 퀴즈

이 챕터에서 배운 내용을 테스트해 봅시다!

1. **언제 새 토크나이저를 훈련해야 합니까?**
    1. 당신의 데이터셋이 기존의 사전 훈련된 모델이 사용한 데이터셋과 유사하고, 새 모델을 사전 훈련하려고 할 때
    2. 당신의 데이터셋이 기존의 사전 훈련된 모델이 사용한 데이터셋과 유사하고, 이 사전 훈련된 모델을 사용하여 새 모델을 미세 조정하려고 할 때
    3. 당신의 데이터셋이 기존의 사전 훈련된 모델이 사용한 데이터셋과 다르고, 새 모델을 사전 훈련하려고 할 때
    4. 당신의 데이터셋이 기존의 사전 훈련된 모델이 사용한 데이터셋과 다르지만, 이 사전 훈련된 모델을 사용하여 새 모델을 미세 조정하려고 할 때

2. **train_new_from_iterator()를 사용할 때 텍스트 목록의 목록과 비교하여 텍스트 목록의 제너레이터를 사용하는 이점은 무엇입니까?**
    1. 이 메서드(`train_new_from_iterator()`)가 허용하는 유일한 유형입니다.
    2. 전체 데이터셋을 한 번에 메모리에 로드하는 것을 방지할 수 있습니다.
    3. 이는 🤗 Tokenizers 라이브러리가 멀티프로세싱을 사용하도록 허용합니다.
    4. 당신이 훈련하는 토크나이저가 더 나은 텍스트를 생성할 것입니다.

3. **"빠른" 토크나이저를 사용하는 이점은 무엇입니까?**
    1. 많은 입력을 함께 배치할 때 느린 토크나이저보다 더 빠르게 입력을 처리할 수 있습니다.
    2. 빠른 토크나이저는 항상 느린 토크나이저보다 더 빠르게 토큰화합니다.
    3. 패딩과 잘라내기를 적용할 수 있습니다.
    4. 토큰을 생성한 텍스트 범위에 매핑할 수 있는 몇 가지 추가 기능이 있습니다.

4. **token-classification 파이프라인은 여러 토큰에 걸쳐 있는 개체를 어떻게 처리합니까?**
    1. 동일한 레이블을 가진 개체는 하나의 개체로 병합됩니다.
    2. 개체의 시작을 위한 레이블과 개체의 연속을 위한 레이블이 있습니다.
    3. 주어진 단어에서 첫 번째 토큰이 개체의 레이블을 가지고 있는 한, 전체 단어는 해당 개체로 레이블이 지정된 것으로 간주됩니다.
    4. 토큰이 주어진 개체의 레이블을 가질 때, 새로운 개체의 시작으로 레이블이 지정되지 않는 한, 동일한 레이블을 가진 다른 후속 토큰은 동일한 개체의 일부로 간주됩니다.

5. **question-answering 파이프라인은 긴 컨텍스트를 어떻게 처리합니까?**
    1. 모델이 허용하는 최대 길이로 긴 컨텍스트를 잘라내므로 실제로 처리하지 않습니다.
    2. 컨텍스트를 여러 부분으로 분할하고 얻은 결과를 평균화합니다.
    3. 컨텍스트를 여러 부분으로 분할하고(오버랩을 포함하여) 각 부분에서 답변에 대한 최대 점수를 찾습니다.
    4. 컨텍스트를 여러 부분으로 분할하고(효율성을 위해 오버랩 없이) 각 부분에서 답변에 대한 최대 점수를 찾습니다.

6. **정규화란 무엇입니까?**
    1. 토크나이저가 초기 단계에서 텍스트에 수행하는 모든 정리입니다.
    2. 희귀한 단어를 제거하여 텍스트를 더 정상적으로 만드는 데이터 증강 기술입니다.
    3. 토크나이저가 특수 토큰을 추가하는 최종 후처리 단계입니다.
    4. 평균을 빼고 표준 편차로 나누어 임베딩이 평균 0, 표준 편차 1이 되도록 만드는 것입니다.

7. **서브워드 토크나이저에 대한 사전 토큰화란 무엇입니까?**
    1. 데이터 증강(예: 무작위 마스킹)이 적용되는 토큰화 이전 단계입니다.
    2. 텍스트에 원하는 정리 작업이 적용되는 토큰화 이전 단계입니다.
    3. 입력을 단어로 분할하기 위해 토크나이저 모델이 적용되기 이전 단계입니다.
    4. 입력을 토큰으로 분할하기 위해 토크나이저 모델이 적용되기 이전 단계입니다.

8. **BPE 토큰화 모델에 적용되는 문장을 선택하십시오.**
    1. BPE는 작은 어휘로 시작하여 병합 규칙을 학습하는 서브워드 토큰화 알고리즘입니다.
    2. BPE는 큰 어휘로 시작하여 점진적으로 토큰을 제거하는 서브워드 토큰화 알고리즘입니다.
    3. BPE 토크나이저는 가장 자주 발생하는 토큰 쌍을 병합하여 병합 규칙을 학습합니다.
    4. BPE 토크나이저는 개별 부분의 빈도가 낮은 자주 발생하는 쌍을 우대하는 점수를 최대화하는 토큰 쌍을 병합하여 병합 규칙을 학습합니다.
    5. BPE는 단어를 문자로 분할한 다음 병합 규칙을 적용하여 서브워드로 토큰화합니다.
    6. BPE는 어휘에 있는 시작 부분부터 가장 긴 서브워드를 찾은 다음 텍스트의 나머지 부분에 대해 프로세스를 반복하여 단어를 서브워드로 토큰화합니다.

9. **WordPiece 토큰화 모델에 적용되는 문장을 선택하십시오.**
    1. WordPiece는 작은 어휘로 시작하여 병합 규칙을 학습하는 서브워드 토큰화 알고리즘입니다.
    2. WordPiece는 큰 어휘로 시작하여 점진적으로 토큰을 제거하는 서브워드 토큰화 알고리즘입니다.
    3. WordPiece 토크나이저는 가장 자주 발생하는 토큰 쌍을 병합하여 병합 규칙을 학습합니다.
    4. WordPiece 토크나이저는 개별 부분의 빈도가 낮은 자주 발생하는 쌍을 우대하는 점수를 최대화하는 토큰 쌍을 병합하여 병합 규칙을 학습합니다.
    5. WordPiece는 모델에 따라 토큰으로의 가장 가능성이 높은 분할을 찾아 단어를 서브워드로 토큰화합니다.
    6. WordPiece는 어휘에 있는 시작 부분부터 가장 긴 서브워드를 찾은 다음 텍스트의 나머지 부분에 대해 프로세스를 반복하여 단어를 서브워드로 토큰화합니다.

10. **Unigram 토큰화 모델에 적용되는 문장을 선택하십시오.**
    1. Unigram은 작은 어휘로 시작하여 병합 규칙을 학습하는 서브워드 토큰화 알고리즘입니다.
    2. Unigram은 큰 어휘로 시작하여 점진적으로 토큰을 제거하는 서브워드 토큰화 알고리즘입니다.
    3. Unigram은 전체 코퍼스에서 계산된 손실을 최소화하여 어휘를 조정합니다.
    4. Unigram은 가장 자주 발생하는 서브워드를 유지하여 어휘를 조정합니다.
    5. Unigram은 모델에 따라 토큰으로의 가장 가능성이 높은 분할을 찾아 단어를 서브워드로 토큰화합니다.
    6. Unigram은 단어를 문자로 분할한 다음 병합 규칙을 적용하여 서브워드로 토큰화합니다.

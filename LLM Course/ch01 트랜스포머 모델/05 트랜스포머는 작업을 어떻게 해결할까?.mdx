# How 🤗 Transformers solve tasks

<Youtube id="zsfR7eY9Uho" />

[트랜스포머, 무엇을 할 수 있나요?](/course/chapter1/3)에서 당신은 자연어 처리(NLP), 음성 및 오디오, 컴퓨터 비전 작업과 이들의 몇 가지 중요한 응용 프로그램에 대해 배웠습니다.
이 페이지에서는 모델이 이러한 작업을 어떻게 해결하는지 자세히 살펴보고 내부에서 무슨 일이 일어나고 있는지 설명할 것입니다.
주어진 작업을 해결하는 방법은 많으며, 일부 모델은 특정 기술을 구현하거나 새로운 각도로 작업에 접근할 수도 있지만, 트랜스포머 모델의 일반적인 아이디어는 동일합니다.
유연한 아키텍처 덕분에 대부분의 모델은 인코더, 디코더 또는 인코더-디코더 구조의 변형입니다.
> [!TIP]
> 특정 아키텍처 변형에 대해 자세히 알아보기 전에, 대부분의 작업이 비슷한 패턴을 따른다는 것을 이해하는 것이 도움이 됩니다: 입력 데이터는 모델을 통해 처리되고, 출력은 특정 작업을 위해 해석됩니다.
차이점은 데이터 준비 방식, 사용되는 모델 아키텍처 변형, 출력이 처리되는 방식에 있습니다.
작업이 어떻게 해결되는지 설명하기 위해, 유용한 예측을 출력하기 위해 모델 내부에서 일어나는 과정을 단계별로 살펴볼 것입니다.
우리는 다음 모델과 해당 작업을 다룰 것입니다:

- [Wav2Vec2](https://huggingface.co/docs/transformers/model_doc/wav2vec2) (오디오 분류 및 자동 음성 인식(ASR)용)
- [Vision Transformer (ViT)](https://huggingface.co/docs/transformers/model_doc/vit) 및 [ConvNeXT](https://huggingface.co/docs/transformers/model_doc/convnext) (이미지 분류용)
- [DETR](https://huggingface.co/docs/transformers/model_doc/detr) (객체 감지용)
- [Mask2Former](https://huggingface.co/docs/transformers/model_doc/mask2former) (이미지 분할용)
- [GLPN](https://huggingface.co/docs/transformers/model_doc/glpn) (깊이 추정용)
- [BERT](https://huggingface.co/docs/transformers/model_doc/bert) (텍스트 분류, 토큰 분류, 질문 답변과 같이 인코더를 사용하는 NLP 작업용)
- [GPT2](https://huggingface.co/docs/transformers/model_doc/gpt2) (텍스트 생성과 같이 디코더를 사용하는 NLP 작업용)
- [BART](https://huggingface.co/docs/transformers/model_doc/bart) (요약 및 번역과 같이 인코더-디코더를 사용하는 NLP 작업용)

> [!TIP]
> 더 나아가기 전에, 오리지널 트랜스포머 아키텍처에 대한 기본적인 지식을 가지고 있는 것이 좋습니다.
인코더, 디코더 및 어텐션이 어떻게 작동하는지 아는 것은 다양한 트랜스포머 모델이 어떻게 작동하는지 이해하는 데 도움이 될 것입니다.
더 많은 정보를 위해 [이전 섹션](https://huggingface.co/course/chapter1/4?fw=pt)을 꼭 확인하세요!
## Transformer models for language 

언어 모델은 현대 NLP의 핵심입니다.
이들은 텍스트 내에서 단어나 토큰 간의 통계적 패턴과 관계를 학습하여 인간의 언어를 이해하고 생성하도록 설계되었습니다.
트랜스포머는 원래 기계 번역을 위해 설계되었으며, 그 이후로 모든 AI 작업을 해결하는 기본 아키텍처가 되었습니다.
일부 작업은 트랜스포머의 인코더 구조에 적합한 반면, 다른 작업은 디코더에 더 적합합니다.
또 다른 작업들은 트랜스포머의 인코더-디코더 구조를 모두 사용합니다.
### How language models work

언어 모델은 주변 단어의 문맥을 고려하여 단어의 확률을 예측하도록 훈련됨으로써 작동합니다.
이것은 언어에 대한 기본적인 이해를 제공하며, 다른 작업으로 일반화될 수 있습니다.
트랜스포머 모델을 훈련하는 주요 접근 방식은 두 가지가 있습니다:

1. **마스크 언어 모델링(MLM)**: BERT와 같은 인코더 모델에서 사용되는 이 접근 방식은 입력의 일부 토큰을 무작위로 마스킹하고, 주변 문맥을 기반으로 원래 토큰을 예측하도록 모델을 훈련합니다.
이를 통해 모델은 양방향 문맥(마스킹된 단어 이전과 이후의 단어 모두를 보는 것)을 학습할 수 있습니다.
2. **인과적 언어 모델링(CLM)**: GPT와 같은 디코더 모델에서 사용되는 이 접근 방식은 시퀀스의 모든 이전 토큰을 기반으로 다음 토큰을 예측합니다.
모델은 다음 토큰을 예측하기 위해 왼쪽(이전 토큰)의 문맥만 사용할 수 있습니다.
### Types of language models

트랜스포머 라이브러리에서 언어 모델은 일반적으로 세 가지 아키텍처 범주로 나뉩니다:

1. **인코더 전용 모델** (BERT와 같은): 이 모델들은 양방향 접근 방식을 사용하여 양쪽 방향의 문맥을 이해합니다.
이들은 분류, 개체명 인식, 질문 답변과 같이 텍스트에 대한 깊은 이해를 요구하는 작업에 가장 적합합니다.
2. **디코더 전용 모델** (GPT, Llama와 같은): 이 모델들은 텍스트를 왼쪽에서 오른쪽으로 처리하며 특히 텍스트 생성 작업에 능숙합니다.
이들은 프롬프트를 기반으로 문장을 완성하거나, 에세이를 작성하거나, 심지어 코드를 생성할 수 있습니다.
3. **인코더-디코더 모델** (T5, BART와 같은): 이 모델들은 두 접근 방식을 모두 결합하여, 인코더를 사용하여 입력을 이해하고 디코더를 사용하여 출력을 생성합니다.
이들은 번역, 요약, 질문 답변과 같은 시퀀스-투-시퀀스 작업에서 뛰어납니다.
![transformer-models-for-language](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_architecture.png)

이전 섹션에서 다룬 것처럼, 언어 모델은 일반적으로 대규모 텍스트 데이터에서 자기 지도 학습 방식으로 사전 훈련된 다음(사람의 주석 없이), 특정 작업에 대해 **미세 조정**됩니다.
**전이 학습**이라고 알려진 이 접근 방식을 통해 이 모델들은 비교적 적은 양의 작업별 데이터로 많은 다양한 NLP 작업에 적응할 수 있습니다.
다음 섹션에서는 특정 모델 아키텍처와 이들이 음성, 비전, 텍스트 도메인 전반의 다양한 작업에 어떻게 적용되는지 살펴보겠습니다.
> [!TIP]
> 트랜스포머 아키텍처의 어떤 부분(인코더, 디코더 또는 둘 다)이 특정 NLP 작업에 가장 적합한지 이해하는 것이 올바른 모델을 선택하는 데 핵심입니다.
일반적으로 양방향 문맥을 요구하는 작업은 인코더를 사용하고, 텍스트를 생성하는 작업은 디코더를 사용하며, 한 시퀀스를 다른 시퀀스로 변환하는 작업은 인코더-디코더를 사용합니다.
### Text generation

텍스트 생성은 프롬프트나 입력을 기반으로 일관성 있고 문맥적으로 관련성 있는 텍스트를 만드는 것을 포함합니다.
[GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2)는 대규모 텍스트 데이터로 사전 훈련된 디코더 전용 모델입니다.
이 모델은 프롬프트가 주어지면 설득력 있는(항상 사실은 아닐지라도!) 텍스트를 생성할 수 있으며, 명시적으로 훈련되지 않았음에도 질문 답변과 같은 다른 NLP 작업도 완료할 수 있습니다.
<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gpt2_architecture.png"/>
</div>

1. GPT-2는 [바이트 쌍 인코딩(BPE)](https://huggingface.co/docs/transformers/tokenizer_summary#bytepair-encoding-bpe)을 사용하여 단어를 토큰화하고 **토큰 임베딩**을 생성합니다.
시퀀스에서 각 토큰의 위치를 나타내기 위해 토큰 임베딩에 **위치 임베딩**이 추가됩니다.
입력 임베딩은 여러 **디코더 블록**을 통과하여 최종 은닉 상태를 출력합니다.
각 디코더 블록 내에서 GPT-2는 *마스크된 셀프 어텐션* 레이어를 사용하는데, 이는 GPT-2가 미래 토큰에 주의를 기울일 수 없음을 의미합니다.
오직 왼쪽에 있는 토큰에만 주의를 기울일 수 있습니다.
이는 BERT의 [`mask`] 토큰과 다른데, 마스크된 셀프 어텐션에서는 어텐션 마스크가 사용되어 미래 토큰에 대한 점수를 `0`으로 설정하기 때문입니다.
2. 디코더의 출력은 **언어 모델링 헤드**로 전달되며, 이는 선형 변환을 수행하여 은닉 상태를 **로짓**으로 변환합니다.
레이블은 시퀀스에서 다음 토큰이며, 로짓을 오른쪽으로 한 칸 이동하여 생성됩니다.
이동된 로짓과 레이블 간의 **크로스 엔트로피 손실**이 계산되어 다음으로 가장 가능성이 높은 토큰을 출력합니다.
GPT-2의 사전 훈련 목표는 전적으로 [인과적 언어 모델링](https://huggingface.co/docs/transformers/glossary#causal-language-modeling)에 기반하며, 시퀀스에서 다음 단어를 예측하는 것입니다.
이것은 GPT-2를 텍스트 생성을 포함하는 작업에 특히 능숙하게 만듭니다.

텍스트 생성을 직접 시도해 볼 준비가 되셨나요?
DistilGPT-2를 미세 조정하고 추론에 사용하는 방법을 배우려면 완전한 [인과적 언어 모델링 가이드](https://huggingface.co/docs/transformers/tasks/language_modeling#causal-language-modeling)를 확인하세요!
> [!TIP]
> 텍스트 생성에 대한 더 많은 정보는 [텍스트 생성 전략](https://huggingface.co/docs/transformers/generation_strategies#generation-strategies) 가이드를 확인하세요!
### Text classification

텍스트 분류는 감성 분석, 주제 분류, 스팸 감지와 같이 텍스트 문서에 미리 정의된 범주를 할당하는 것을 포함합니다.
[BERT](https://huggingface.co/docs/transformers/model_doc/bert)는 **인코더 전용** 모델이며, 양쪽의 단어에 주의를 기울여 텍스트의 더 풍부한 표현을 학습하기 위해 **깊은 양방향성**을 효과적으로 구현한 첫 번째 모델입니다.
1. BERT는 [WordPiece](https://huggingface.co/docs/transformers/tokenizer_summary#wordpiece) 토큰화를 사용하여 텍스트의 **토큰 임베딩**을 생성합니다.
단일 문장과 한 쌍의 문장을 구별하기 위해 특별한 `[SEP]` 토큰이 추가되어 이들을 구분합니다.
특별한 `[CLS]` 토큰이 모든 텍스트 시퀀스의 시작 부분에 추가됩니다.
`[CLS]` 토큰을 포함하는 최종 출력은 분류 작업을 위한 **분류 헤드**의 입력으로 사용됩니다.
BERT는 또한 토큰이 한 쌍의 문장에서 첫 번째 문장에 속하는지 두 번째 문장에 속하는지를 나타내기 위해 **세그먼트 임베딩**을 추가합니다.
2. BERT는 두 가지 목표로 사전 훈련됩니다: **마스크 언어 모델링**과 **다음 문장 예측**입니다.
마스크 언어 모델링에서는 입력 토큰의 일부 비율이 무작위로 마스킹되며, 모델은 이들을 예측해야 합니다.
이것은 모델이 속임수를 써서 모든 단어를 보고 다음 단어를 "예측"할 수 있는 양방향성 문제를 해결합니다.
예측된 마스크 토큰의 최종 은닉 상태는 어휘에 대한 **소프트맥스**를 가진 **피드포워드 네트워크**로 전달되어 마스크된 단어를 예측합니다.
두 번째 사전 훈련 목표는 **다음 문장 예측**입니다. 모델은 문장 B가 문장 A를 따르는지 여부를 예측해야 합니다. 절반은 문장 B가 다음 문장이고, 나머지 절반은 문장 B가 임의의 문장입니다.
다음 문장인지 아닌지에 대한 예측은 두 클래스(`IsNext` 및 `NotNext`)에 대한 소프트맥스를 가진 피드포워드 네트워크로 전달됩니다.
3. 입력 임베딩은 여러 **인코더 레이어**를 통과하여 최종 은닉 상태를 출력합니다.
사전 훈련된 모델을 텍스트 분류에 사용하기 위해서는 기본 BERT 모델 위에 **시퀀스 분류 헤드**를 추가합니다.
시퀀스 분류 헤드는 최종 은닉 상태를 받아들이고 선형 변환을 수행하여 이들을 로짓으로 변환하는 선형 레이어입니다.
로짓과 타겟 간의 **크로스 엔트로피 손실**이 계산되어 가장 가능성이 높은 레이블을 찾습니다.
텍스트 분류를 직접 시도해 볼 준비가 되셨나요? DistilBERT를 미세 조정하고 추론에 사용하는 방법을 배우려면 완전한 [텍스트 분류 가이드](https://huggingface.co/docs/transformers/tasks/sequence_classification)를 확인하세요!
### Token classification

토큰 분류는 개체명 인식 또는 품사 태깅과 같이 시퀀스의 각 토큰에 레이블을 할당하는 것을 포함합니다.
개체명 인식(NER)과 같은 토큰 분류 작업에 BERT를 사용하려면 기본 BERT 모델 위에 **토큰 분류 헤드**를 추가합니다.
토큰 분류 헤드는 최종 은닉 상태를 받아들이고 선형 변환을 수행하여 이들을 로짓으로 변환하는 선형 레이어입니다.
로짓과 각 토큰 간의 **크로스 엔트로피 손실**이 계산되어 가장 가능성이 높은 레이블을 찾습니다.
토큰 분류를 직접 시도해 볼 준비가 되셨나요? DistilBERT를 미세 조정하고 추론에 사용하는 방법을 배우려면 완전한 [토큰 분류 가이드](https://huggingface.co/docs/transformers/tasks/token_classification)를 확인하세요!
### Question answering

질문 답변은 주어진 문맥이나 단락 내에서 질문에 대한 답을 찾는 것을 포함합니다.
질문 답변에 BERT를 사용하려면 기본 BERT 모델 위에 **스팬 분류 헤드**를 추가합니다.
이 선형 레이어는 최종 은닉 상태를 받아들이고 선형 변환을 수행하여 답변에 해당하는 **스팬**의 시작 및 끝 로짓을 계산합니다.
로짓과 레이블 위치 간의 **크로스 엔트로피 손실**이 계산되어 답변에 해당하는 텍스트의 가장 가능성이 높은 스팬을 찾습니다.
질문 답변을 직접 시도해 볼 준비가 되셨나요? DistilBERT를 미세 조정하고 추론에 사용하는 방법을 배우려면 완전한 [질문 답변 가이드](https://huggingface.co/docs/transformers/tasks/question_answering)를 확인하세요!
> [!TIP]
> 💡 BERT가 사전 훈련된 후에는 다른 작업에 얼마나 쉽게 사용될 수 있는지 주목하세요.
단지 특정 **헤드**를 사전 훈련된 모델에 추가하여 은닉 상태를 원하는 출력으로 조작하기만 하면 됩니다!
### Summarization

요약은 긴 텍스트를 핵심 정보와 의미를 보존하면서 더 짧은 버전으로 압축하는 것을 포함합니다.
[BART](https://huggingface.co/docs/transformers/model_doc/bart) 및 [T5](model_doc/t5)와 같은 **인코더-디코더 모델**은 요약 작업의 시퀀스-투-시퀀스 패턴을 위해 설계되었습니다.
이 섹션에서는 BART가 어떻게 작동하는지 설명하고, 마지막에 T5를 미세 조정하는 것을 시도해 볼 수 있습니다.
<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bart_architecture.png"/>
</div>

1. BART의 인코더 아키텍처는 BERT와 매우 유사하며, 텍스트의 토큰 및 위치 임베딩을 받아들입니다.
BART는 입력을 손상시킨 다음 디코더로 재구성하여 사전 훈련됩니다.
특정 손상 전략을 사용하는 다른 인코더와 달리, BART는 모든 유형의 손상을 적용할 수 있습니다.
하지만 *텍스트 채우기(text infilling)* 손상 전략이 가장 잘 작동합니다. 텍스트 채우기에서는 여러 텍스트 스팬이 **단일** [`mask`] 토큰으로 대체됩니다.
이것은 모델이 마스크된 토큰을 예측해야 하고, 누락된 토큰의 개수를 예측하도록 모델을 학습시키기 때문에 중요합니다.
입력 임베딩과 마스크된 스팬은 인코더를 통과하여 최종 은닉 상태를 출력하지만, BERT와 달리 BART는 끝에 단어를 예측하기 위한 최종 피드포워드 네트워크를 추가하지 않습니다.
2. 인코더의 출력은 **디코더**로 전달되며, 디코더는 마스크된 토큰과 인코더 출력에서 손상되지 않은 토큰을 예측해야 합니다.
이것은 디코더가 원래 텍스트를 복원하는 데 도움이 되는 추가적인 문맥을 제공합니다.
디코더의 출력은 **언어 모델링 헤드**로 전달되며, 이는 선형 변환을 수행하여 은닉 상태를 로짓으로 변환합니다.
로짓과 레이블(단순히 오른쪽으로 이동된 토큰) 간의 **크로스 엔트로피 손실**이 계산됩니다.
요약을 직접 시도해 볼 준비가 되셨나요? T5를 미세 조정하고 추론에 사용하는 방법을 배우려면 완전한 [요약 가이드](https://huggingface.co/docs/transformers/tasks/summarization)를 확인하세요!
> [!TIP]
> 텍스트 생성에 대한 더 많은 정보는 [텍스트 생성 전략](https://huggingface.co/docs/transformers/generation_strategies) 가이드를 확인하세요!
### Translation

번역은 의미를 보존하면서 한 언어의 텍스트를 다른 언어로 변환하는 것을 포함합니다.
번역은 또 다른 시퀀스-투-시퀀스 작업의 예이며, 이는 [BART](https://huggingface.co/docs/transformers/model_doc/bart) 또는 [T5](model_doc/t5)와 같은 **인코더-디코더 모델**을 사용하여 수행할 수 있음을 의미합니다.
이 섹션에서는 BART가 어떻게 작동하는지 설명하고, 마지막에 T5를 미세 조정하는 것을 시도해 볼 수 있습니다.
BART는 **별도의 무작위로 초기화된 인코더**를 추가하여 번역에 적응합니다. 이 인코더는 소스 언어를 타겟 언어로 디코딩될 수 있는 입력으로 매핑합니다.
이 새로운 인코더의 임베딩은 원래 단어 임베딩 대신 사전 훈련된 인코더로 전달됩니다.
소스 인코더는 모델 출력의 **크로스 엔트로피 손실**로 소스 인코더, 위치 임베딩 및 입력 임베딩을 업데이트하여 훈련됩니다.
이 첫 번째 단계에서는 모델 매개변수가 **고정(frozen)**되며, 두 번째 단계에서는 모든 모델 매개변수가 함께 훈련됩니다.
BART는 이후 다국어 버전인 **mBART**로 이어졌으며, 번역을 위해 의도되었고 많은 다른 언어로 사전 훈련되었습니다.
번역을 직접 시도해 볼 준비가 되셨나요? T5를 미세 조정하고 추론에 사용하는 방법을 배우려면 완전한 [번역 가이드](https://huggingface.co/docs/transformers/tasks/translation)를 확인하세요!
> [!TIP]
> 이 가이드 전체에서 보셨듯이, 많은 모델들은 다른 작업을 다루고 있음에도 불구하고 유사한 패턴을 따릅니다.
이러한 공통 패턴을 이해하면 새로운 모델이 어떻게 작동하는지, 그리고 기존 모델을 특정 요구 사항에 어떻게 맞출 수 있는지 빠르게 파악하는 데 도움이 될 수 있습니다.
## Modalities beyond text

트랜스포머는 텍스트에만 국한되지 않습니다. 이들은 음성 및 오디오, 이미지, 비디오와 같은 다른 양식에도 적용될 수 있습니다.
물론 이 과정에서는 텍스트에 초점을 맞추겠지만, 다른 양식들을 간략하게 소개할 수 있습니다.
### Speech and audio

트랜스포머 모델이 텍스트나 이미지와 비교하여 고유한 도전 과제를 제시하는 음성 및 오디오 데이터를 어떻게 처리하는지 살펴보는 것으로 시작하겠습니다.
[Whisper](https://huggingface.co/docs/transformers/main/en/model_doc/whisper)는 680,000시간의 레이블이 지정된 오디오 데이터로 사전 훈련된 **인코더-디코더(시퀀스-투-시퀀스)** 트랜스포머입니다.
이러한 사전 훈련 데이터의 양은 영어 및 다른 많은 언어에서 오디오 작업에 대한 **제로 샷 성능**을 가능하게 합니다.
디코더를 통해 Whisper는 인코더가 학습한 음성 표현을 추가적인 미세 조정 없이 텍스트와 같은 유용한 출력으로 매핑할 수 있습니다.
Whisper는 바로 사용 가능합니다.

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/whisper_architecture.png"/>
</div>

다이어그램은 [Whisper 논문](https://huggingface.co/papers/2212.04356)에서 가져왔습니다.
이 모델에는 두 가지 주요 구성 요소가 있습니다:

1. **인코더**는 입력 오디오를 처리합니다.
원시 오디오는 먼저 **로그-멜 스펙트로그램**으로 변환됩니다. 이 스펙트로그램은 이후 트랜스포머 인코더 네트워크를 통과합니다.
2. **디코더**는 인코딩된 오디오 표현을 가져와 이전 토큰과 인코더 출력이 주어졌을 때 해당하는 **텍스트 토큰**을 **자기 회귀적으로** 예측합니다.
이는 이전 토큰과 인코더 출력이 주어졌을 때 다음 텍스트 토큰을 예측하도록 훈련된 표준 트랜스포머 디코더입니다.
디코더 입력의 시작 부분에 **특수 토큰**이 사용되어 모델을 전사(transcription), 번역 또는 언어 식별과 같은 특정 작업으로 유도합니다.
Whisper는 웹에서 수집된 680,000시간의 레이블이 지정된 방대한 양의 다양한 오디오 데이터셋으로 사전 훈련되었습니다.
이러한 대규모의 **약한 지도 학습 사전 훈련**은 많은 언어와 작업에 걸쳐 강력한 제로 샷 성능을 보이는 핵심입니다.
이제 Whisper가 사전 훈련되었으므로, **제로 샷 추론**을 위해 직접 사용하거나 특정 작업(예: 자동 음성 인식 또는 음성 번역)에서 개선된 성능을 위해 데이터에 대해 **미세 조정**할 수 있습니다!
> [!TIP]
> Whisper의 핵심 혁신은 인터넷에서 얻은 전례 없는 규모의 다양하고 약하게 지도 학습된 오디오 데이터로 훈련되었다는 것입니다.
이를 통해 작업별 미세 조정 없이도 다양한 언어, 억양 및 작업에 놀라울 정도로 잘 일반화할 수 있습니다.
### Automatic speech recognition

사전 훈련된 모델을 자동 음성 인식에 사용하려면 전체 **인코더-디코더 구조**를 활용합니다.
인코더는 오디오 입력을 처리하고, 디코더는 전사(transcript)를 토큰별로 **자기 회귀적으로** 생성합니다.
미세 조정할 때, 모델은 일반적으로 표준 시퀀스-투-시퀀스 손실(예: 크로스 엔트로피)을 사용하여 오디오 입력에 기반한 올바른 텍스트 토큰을 예측하도록 훈련됩니다.
미세 조정된 모델을 추론에 사용하는 가장 쉬운 방법은 **`pipeline`** 내에서 사용하는 것입니다.
```python
from transformers import pipeline

transcriber = pipeline(
    task="automatic-speech-recognition", model="openai/whisper-base.en"
)
transcriber("[https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac](https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac)")
# Output: {'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}
```

자동 음성 인식을 직접 시도해 볼 준비가 되셨나요?
Whisper를 미세 조정하고 추론에 사용하는 방법을 배우려면 완전한 [자동 음성 인식 가이드](https://huggingface.co/docs/transformers/tasks/asr)를 확인하세요!
### Computer vision

이제 이미지나 비디오에서 시각적 정보를 이해하고 해석하는 것을 다루는 **컴퓨터 비전** 작업으로 넘어가 보겠습니다.
컴퓨터 비전 작업에 접근하는 방법은 두 가지가 있습니다:

1. 이미지를 일련의 패치로 분할하고 트랜스포머로 병렬 처리합니다.
2. [ConvNeXT](https://huggingface.co/docs/transformers/model_doc/convnext)와 같은 현대적인 CNN을 사용합니다. 이 모델은 **합성곱 레이어**에 의존하지만 현대적인 네트워크 설계를 채택합니다.
> [!TIP]
> 세 번째 접근 방식은 트랜스포머와 합성곱을 혼합하는 것입니다(예: [Convolutional Vision Transformer](https://huggingface.co/docs/transformers/model_doc/cvt) 또는 [LeViT](https://huggingface.co/docs/transformers/model_doc/levit)).
우리는 여기서 다루는 두 가지 접근 방식을 단순히 결합하기 때문에 이에 대해서는 논의하지 않을 것입니다.
ViT와 ConvNeXT는 이미지 분류에 일반적으로 사용되지만, 객체 감지, 분할 및 깊이 추정과 같은 다른 비전 작업에는 각각 DETR, Mask2Former 및 GLPN을 살펴볼 것입니다.
이 모델들은 해당 작업에 더 적합합니다.

### Image classification

이미지 분류는 기본적인 컴퓨터 비전 작업 중 하나입니다.
다양한 모델 아키텍처가 이 문제에 어떻게 접근하는지 살펴보겠습니다.

ViT와 ConvNeXT는 모두 이미지 분류에 사용될 수 있습니다.
주요 차이점은 ViT는 **어텐션 메커니즘**을 사용하는 반면 ConvNeXT는 **합성곱**을 사용한다는 것입니다.
[ViT](https://huggingface.co/docs/transformers/model_doc/vit)는 합성곱을 **순수 트랜스포머 아키텍처**로 완전히 대체합니다. 오리지널 트랜스포머에 익숙하다면 이미 ViT를 이해하는 대부분의 단계를 거친 것입니다.
<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/vit_architecture.jpg"/>
</div>

ViT가 도입한 주요 변경 사항은 이미지가 트랜스포머에 공급되는 방식입니다:

1. 이미지는 **정사각형의 겹치지 않는 패치**로 분할되며, 각 패치는 벡터 또는 *패치 임베딩*으로 변환됩니다.
패치 임베딩은 적절한 입력 차원(기본 트랜스포머의 경우 각 패치 임베딩에 대해 768 값)을 생성하는 **합성곱 2D 레이어**로부터 생성됩니다.
224x224 픽셀 이미지가 있다면, 이를 196개의 16x16 이미지 패치로 분할할 수 있습니다.
텍스트가 단어로 토큰화되는 것처럼, 이미지는 일련의 패치로 "토큰화"됩니다.
2. BERT와 마찬가지로 *학습 가능한 임베딩* - 특별한 `[CLS]` 토큰 -이 패치 임베딩의 시작 부분에 추가됩니다.
`[CLS]` 토큰의 최종 은닉 상태는 연결된 **분류 헤드**의 입력으로 사용되며, 다른 출력은 무시됩니다. 이 토큰은 모델이 이미지의 표현을 인코딩하는 방법을 배우는 데 도움이 됩니다.
3. 패치 및 학습 가능한 임베딩에 추가할 마지막 것은 *위치 임베딩*입니다. 왜냐하면 모델은 이미지 패치가 어떻게 정렬되어 있는지 모르기 때문입니다.
위치 임베딩 역시 학습 가능하며 패치 임베딩과 크기가 같습니다.
마지막으로, 모든 임베딩은 **트랜스포머 인코더**로 전달됩니다.
4. 출력, 특히 `[CLS]` 토큰을 포함하는 출력만 **다층 퍼셉트론 헤드(MLP)**로 전달됩니다.
ViT의 사전 훈련 목표는 단순히 **분류**입니다. 다른 분류 헤드와 마찬가지로 MLP 헤드는 출력을 클래스 레이블에 대한 **로짓**으로 변환하고 **크로스 엔트로피 손실**을 계산하여 가장 가능성이 높은 클래스를 찾습니다.
이미지 분류를 직접 시도해 볼 준비가 되셨나요? ViT를 미세 조정하고 추론에 사용하는 방법을 배우려면 완전한 [이미지 분류 가이드](https://huggingface.co/docs/transformers/tasks/image_classification)를 확인하세요!
> [!TIP]
> ViT와 BERT 사이의 유사점에 주목하세요: 둘 다 특별한 토큰(<code>[CLS]</code>)을 사용하여 전체 표현을 포착하고, 둘 다 임베딩에 위치 정보를 추가하며, 둘 다 트랜스포머 인코더를 사용하여 토큰/패치의 시퀀스를 처리합니다.

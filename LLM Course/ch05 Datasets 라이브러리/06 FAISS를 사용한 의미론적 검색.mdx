# FAISSë¥¼ ì´ìš©í•œ ì‹œë§¨í‹± ê²€ìƒ‰ (Semantic search with FAISS)

[ì„¹ì…˜ 5](/course/chapter5/5)ì—ì„œ ğŸ¤— Datasets ì €ì¥ì†Œì˜ GitHub ì´ìŠˆ ë° ëŒ“ê¸€ ë°ì´í„°ì…‹ì„ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤. ì´ ì„¹ì…˜ì—ì„œëŠ” ì´ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ëŒ€í•œ ê°€ì¥ ì‹œê¸‰í•œ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì„ ì°¾ëŠ” ë° ë„ì›€ì´ ë˜ëŠ” ê²€ìƒ‰ ì—”ì§„ì„ êµ¬ì¶•í•  ê²ƒì…ë‹ˆë‹¤!

<Youtube id="OATCgQtNX2o"/>

## ì‹œë§¨í‹± ê²€ìƒ‰ì„ ìœ„í•œ ì„ë² ë”© ì‚¬ìš© (Using embeddings for semantic search)

[ì±•í„° 1](/course/chapter1)ì—ì„œ ë³´ì•˜ë“¯ì´, íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ì–¸ì–´ ëª¨ë¸ì€ í…ìŠ¤íŠ¸ ë²”ìœ„ì˜ ê° í† í°ì„ *ì„ë² ë”© ë²¡í„°*ë¡œ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ê°œë³„ ì„ë² ë”©ì„ "í’€ë§(pool)"í•˜ì—¬ ì „ì²´ ë¬¸ì¥, ë‹¨ë½ ë˜ëŠ” (ì¼ë¶€ ê²½ìš°) ë¬¸ì„œì— ëŒ€í•œ ë²¡í„° í‘œí˜„ì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ ì´ëŸ¬í•œ ì„ë² ë”©ì„ ì‚¬ìš©í•˜ì—¬ ê° ì„ë² ë”©ê³¼ ì  ê³± ìœ ì‚¬ë„(dot-product similarity, ë˜ëŠ” ë‹¤ë¥¸ ìœ ì‚¬ë„ ë©”íŠ¸ë¦­)ë¥¼ ê³„ì‚°í•˜ê³  ê°€ì¥ í° ê²¹ì¹¨(overlap)ì´ ìˆëŠ” ë¬¸ì„œë¥¼ ë°˜í™˜í•˜ì—¬ ì½”í¼ìŠ¤ì—ì„œ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì´ ì„¹ì…˜ì—ì„œëŠ” ì„ë² ë”©ì„ ì‚¬ìš©í•˜ì—¬ **ì‹œë§¨í‹± ê²€ìƒ‰ ì—”ì§„**ì„ ê°œë°œí•  ê²ƒì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ê²€ìƒ‰ ì—”ì§„ì€ ì¿¼ë¦¬ì˜ í‚¤ì›Œë“œë¥¼ ë¬¸ì„œì™€ ì¼ì¹˜ì‹œí‚¤ëŠ” ê¸°ì¡´ ì ‘ê·¼ ë°©ì‹ë³´ë‹¤ ëª‡ ê°€ì§€ ì´ì ì„ ì œê³µí•©ë‹ˆë‹¤.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter5/semantic-search.svg" alt="Semantic search."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter5/semantic-search-dark.svg" alt="Semantic search."/>
</div>

## ë°ì´í„°ì…‹ ë¡œë“œ ë° ì¤€ë¹„ (Loading and preparing the dataset)

ê°€ì¥ ë¨¼ì € í•´ì•¼ í•  ì¼ì€ GitHub ì´ìŠˆ ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. í‰ì†Œì²˜ëŸ¼ **`load_dataset()`** í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ì‹œë‹¤:

```py
from datasets import load_dataset

issues_dataset = load_dataset("lewtun/github-issues", split="train")
issues_dataset
```

```python out
Dataset({
Â  Â  features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'performed_via_github_app', 'is_pull_request'],
Â  Â  num_rows: 2855
})
```

ì—¬ê¸°ì„œ **`load_dataset()`**ì— ê¸°ë³¸ **`train`** ë¶„í• ì„ ì§€ì •í–ˆìœ¼ë¯€ë¡œ **`DatasetDict`** ëŒ€ì‹  **`Dataset`**ì„ ë°˜í™˜í•©ë‹ˆë‹¤. ê°€ì¥ ë¨¼ì € í•  ì¼ì€ í’€ ë¦¬í€˜ìŠ¤íŠ¸ë¥¼ í•„í„°ë§í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ëŠ” ì‚¬ìš©ì ì¿¼ë¦¬ì— ì‘ë‹µí•˜ëŠ” ë° ê±°ì˜ ì‚¬ìš©ë˜ì§€ ì•Šìœ¼ë©° ê²€ìƒ‰ ì—”ì§„ì— ë…¸ì´ì¦ˆë¥¼ ìœ ë°œí•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì´ì œ ìµìˆ™í•´ì¡Œë“¯ì´, **`Dataset.filter()`** í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ì…‹ì—ì„œ ì´ëŸ¬í•œ í–‰ì„ ì œì™¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ì™€ ë™ì‹œì—, ì‚¬ìš©ì ì¿¼ë¦¬ì— ëŒ€í•œ ë‹µë³€ì„ ì œê³µí•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ëŒ“ê¸€ì´ ì—†ëŠ” í–‰ë„ í•„í„°ë§í•´ ë´…ì‹œë‹¤:

```py
issues_dataset = issues_dataset.filter(
Â  Â  lambda x: (x["is_pull_request"] == False and len(x["comments"]) > 0)
)
issues_dataset
```

```python out
Dataset({
Â  Â  features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'performed_via_github_app', 'is_pull_request'],
Â  Â  num_rows: 771
})
```

ë°ì´í„°ì…‹ì— ë§ì€ ì—´ì´ ìˆìŒì„ ì•Œ ìˆ˜ ìˆìœ¼ë©°, ê·¸ ì¤‘ ëŒ€ë¶€ë¶„ì€ ê²€ìƒ‰ ì—”ì§„ì„ êµ¬ì¶•í•˜ëŠ” ë° í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ê²€ìƒ‰ ê´€ì ì—ì„œ ê°€ì¥ ìœ ìµí•œ ì—´ì€ **`title`**, **`body`**, ë° **`comments`**ì´ë©°, **`html_url`**ì€ ì†ŒìŠ¤ ì´ìŠˆì— ëŒ€í•œ ë§í¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤. **`Dataset.remove_columns()`** í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‚˜ë¨¸ì§€ë¥¼ ì‚­ì œí•´ ë´…ì‹œë‹¤:

```py
columns = issues_dataset.column_names
columns_to_keep = ["title", "body", "html_url", "comments"]
columns_to_remove = set(columns_to_keep).symmetric_difference(columns)
issues_dataset = issues_dataset.remove_columns(columns_to_remove)
issues_dataset
```

```python out
Dataset({
Â  Â  features: ['html_url', 'title', 'comments', 'body'],
Â  Â  num_rows: 771
})
```

ì„ë² ë”©ì„ ë§Œë“¤ê¸° ìœ„í•´ ê° ëŒ“ê¸€ì— ì´ìŠˆì˜ ì œëª©ê³¼ ë³¸ë¬¸ì„ ì¶”ê°€í•  ê²ƒì…ë‹ˆë‹¤. ì´ëŸ¬í•œ í•„ë“œì—ëŠ” ì¢…ì¢… ìœ ìš©í•œ **ë¬¸ë§¥ ì •ë³´**ê°€ í¬í•¨ë˜ì–´ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. **`comments`** ì—´ì€ í˜„ì¬ ê° ì´ìŠˆì— ëŒ€í•œ ëŒ“ê¸€ ëª©ë¡ì´ë¯€ë¡œ, ê° í–‰ì´ `(html_url, title, body, comment)` íŠœí”Œë¡œ êµ¬ì„±ë˜ë„ë¡ ì—´ì„ "í­ë°œ(explode)"í•´ì•¼ í•©ë‹ˆë‹¤. Pandasì—ì„œëŠ” [`DataFrame.explode()` í•¨ìˆ˜](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.explode.html)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ í•¨ìˆ˜ëŠ” ëª©ë¡ê³¼ ê°™ì€ ì—´ì˜ ê° ìš”ì†Œì— ëŒ€í•´ ìƒˆ í–‰ì„ ë§Œë“¤ê³  ë‹¤ë¥¸ ëª¨ë“  ì—´ ê°’ì„ ë³µì œí•©ë‹ˆë‹¤. ì´ê²ƒì´ ì‹¤ì œë¡œ ì‘ë™í•˜ëŠ” ê²ƒì„ ë³´ê¸° ìœ„í•´, ë¨¼ì € Pandas **`DataFrame`** í˜•ì‹ìœ¼ë¡œ ì „í™˜í•´ ë´…ì‹œë‹¤:

```py
issues_dataset.set_format("pandas")
df = issues_dataset[:]
```

ì´ **`DataFrame`**ì˜ ì²« ë²ˆì§¸ í–‰ì„ ê²€ì‚¬í•˜ë©´ ì´ ì´ìŠˆì™€ ê´€ë ¨ëœ 4ê°œì˜ ëŒ“ê¸€ì´ ìˆìŒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```py
df["comments"][0].tolist()
```

```python out
['the bug code locate in ï¼š\r\nÂ  Â  if data_args.task_name is not None:\r\nÂ  Â  Â  Â  # Downloading and loading a dataset from the hub.\r\nÂ  Â  Â  Â  datasets = load_dataset("glue", data_args.task_name, cache_dir=model_args.cache_dir)',
Â 'Hi @jinec,\r\n\r\nFrom time to time we get this kind of `ConnectionError` coming from the github.com website: [https://raw.githubusercontent.com](https://raw.githubusercontent.com)\r\n\r\nNormally, it should work if you wait a little and then retry.\r\n\r\nCould you please confirm if the problem persists?',
Â 'cannot connectï¼Œeven by Web browserï¼Œplease check thatÂ  there is someÂ  problemsã€‚',
Â 'I can access [https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py](https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py) without problem...']
```

**`df`**ë¥¼ í­ë°œì‹œí‚¤ë©´(explode) ì´ ëŒ“ê¸€ ê°ê°ì— ëŒ€í•´ í•˜ë‚˜ì˜ í–‰ì´ ìƒê¸¸ ê²ƒìœ¼ë¡œ ì˜ˆìƒí•©ë‹ˆë‹¤. ê·¸ê²ƒì´ ì‚¬ì‹¤ì¸ì§€ í™•ì¸í•´ ë´…ì‹œë‹¤:

```py
comments_df = df.explode("comments", ignore_index=True)
comments_df.head(4)
```

<table border="1" class="dataframe" style="table-layout: fixed; word-wrap:break-word; width: 100%;">
Â  <thead>
Â  Â  <tr style="text-align: right;">
Â  Â  Â  <th></th>
Â  Â  Â  <th>html_url</th>
Â  Â  Â  <th>title</th>
Â  Â  Â  <th>comments</th>
Â  Â  Â  <th>body</th>
Â  Â  </tr>
Â  </thead>
Â  <tbody>
Â  Â  <tr>
Â  Â  Â  <th>0</th>
Â  Â  Â  <td>https://github.com/huggingface/datasets/issues/2787</td>
Â  Â  Â  <td>ConnectionError: Couldn't reach https://raw.githubusercontent.com</td>
Â  Â  Â  <td>the bug code locate in ï¼š\r\nÂ  Â  if data_args.task_name is not None...</td>
Â  Â  Â  <td>Hello,\r\nI am trying to run run_glue.py and it gives me this error...</td>
Â  Â  </tr>
Â  Â  <tr>
Â  Â  Â  <th>1</th>
Â  Â  Â  <td>https://github.com/huggingface/datasets/issues/2787</td>
Â  Â  Â  <td>ConnectionError: Couldn't reach https://raw.githubusercontent.com</td>
Â  Â  Â  <td>Hi @jinec,\r\n\r\nFrom time to time we get this kind of `ConnectionError` coming from the github.com website: https://raw.githubusercontent.com...</td>
Â  Â  Â  <td>Hello,\r\nI am trying to run run_glue.py and it gives me this error...</td>
Â  Â  </tr>
Â  Â  <tr>
Â  Â  Â  <th>2</th>
Â  Â  Â  <td>https://github.com/huggingface/datasets/issues/2787</td>
Â  Â  Â  <td>ConnectionError: Couldn't reach https://raw.githubusercontent.com</td>
Â  Â  Â  <td>cannot connectï¼Œeven by Web browserï¼Œplease check thatÂ  there is someÂ  problemsã€‚</td>
Â  Â  Â  <td>Hello,\r\nI am trying to run run_glue.py and it gives me this error...</td>
Â  Â  </tr>
Â  Â  <tr>
Â  Â  Â  <th>3</th>
Â  Â  Â  <td>https://github.com/huggingface/datasets/issues/2787</td>
Â  Â  Â  <td>ConnectionError: Couldn't reach https://raw.githubusercontent.com</td>
Â  Â  Â  <td>I can access https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py without problem...</td>
Â  Â  Â  <td>Hello,\r\nI am trying to run run_glue.py and it gives me this error...</td>
Â  Â  </tr>
Â  </tbody>
</table>

í›Œë¥­í•©ë‹ˆë‹¤. í–‰ì´ ë³µì œë˜ì—ˆìœ¼ë©° **`comments`** ì—´ì—ëŠ” ê°œë³„ ëŒ“ê¸€ì´ í¬í•¨ë˜ì–´ ìˆìŒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤! ì´ì œ Pandas ì‘ì—…ì„ ë§ˆì³¤ìœ¼ë¯€ë¡œ, ë©”ëª¨ë¦¬ì— **`DataFrame`**ì„ ë¡œë“œí•˜ì—¬ **`Dataset`**ìœ¼ë¡œ ë¹ ë¥´ê²Œ ë‹¤ì‹œ ì „í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```py
from datasets import Dataset

comments_dataset = Dataset.from_pandas(comments_df)
comments_dataset
```

```python out
Dataset({
Â  Â  features: ['html_url', 'title', 'comments', 'body'],
Â  Â  num_rows: 2842
})
```

ì¢‹ìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì‘ì—…í•  ìˆ˜ ìˆëŠ” ìˆ˜ì²œ ê°œì˜ ëŒ“ê¸€ì„ ì–»ì—ˆìŠµë‹ˆë‹¤!


<Tip>

âœï¸ **ì§ì ‘ í•´ë³´ì„¸ìš”!** Pandasë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  **`issues_dataset`**ì˜ **`comments`** ì—´ì„ í­ë°œ(explode)ì‹œí‚¤ê¸° ìœ„í•´ **`Dataset.map()`**ì„ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ”ì§€ í™•ì¸í•´ ë³´ì„¸ìš”. ì´ê²ƒì€ ì•½ê°„ ê¹Œë‹¤ë¡­ìŠµë‹ˆë‹¤. ì´ ì‘ì—…ì— ëŒ€í•´ ğŸ¤— Datasets ë¬¸ì„œì˜ ["ë°°ì¹˜ ë§¤í•‘ (Batch mapping)"](https://huggingface.co/docs/datasets/about_map_batch#batch-mapping) ì„¹ì…˜ì´ ìœ ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

</Tip>

ì´ì œ í–‰ë‹¹ í•˜ë‚˜ì˜ ëŒ“ê¸€ì´ ìˆìœ¼ë¯€ë¡œ, ëŒ“ê¸€ë‹¹ ë‹¨ì–´ ìˆ˜ë¥¼ í¬í•¨í•˜ëŠ” ìƒˆ **`comments_length`** ì—´ì„ ë§Œë“¤ì–´ ë´…ì‹œë‹¤:

```py
comments_dataset = comments_dataset.map(
Â  Â  lambda x: {"comment_length": len(x["comments"].split())}
)
```

ì´ ìƒˆ ì—´ì„ ì‚¬ìš©í•˜ì—¬ ì§§ì€ ëŒ“ê¸€ì„ í•„í„°ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì§§ì€ ëŒ“ê¸€ì€ ì¼ë°˜ì ìœ¼ë¡œ ê²€ìƒ‰ ì—”ì§„ê³¼ ê´€ë ¨ì´ ì—†ëŠ” "cc @lewtun" ë˜ëŠ” "Thanks!"ì™€ ê°™ì€ ê²ƒë“¤ì„ í¬í•¨í•©ë‹ˆë‹¤. í•„í„°ì— ëŒ€í•´ ì„ íƒí•  ì •í™•í•œ ìˆ«ìëŠ” ì—†ì§€ë§Œ, ì•½ 15ë‹¨ì–´ê°€ ì¢‹ì€ ì‹œì‘ìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤:

```py
comments_dataset = comments_dataset.filter(lambda x: x["comment_length"] > 15)
comments_dataset
```

```python out
Dataset({
Â  Â  features: ['html_url', 'title', 'comments', 'body', 'comment_length'],
Â  Â  num_rows: 2098
})
```

ë°ì´í„°ì…‹ì„ ì•½ê°„ ì •ë¦¬í–ˆìœ¼ë¯€ë¡œ, ì´ìŠˆ ì œëª©, ì„¤ëª… ë° ëŒ“ê¸€ì„ ìƒˆ **`text`** ì—´ì— ì—°ê²°í•´ ë´…ì‹œë‹¤. í‰ì†Œì²˜ëŸ¼, **`Dataset.map()`**ì— ì „ë‹¬í•  ìˆ˜ ìˆëŠ” ê°„ë‹¨í•œ í•¨ìˆ˜ë¥¼ ì‘ì„±í•  ê²ƒì…ë‹ˆë‹¤:

```py
def concatenate_text(examples):
Â  Â  return {
Â  Â  Â  Â  "text": examples["title"]
Â  Â  Â  Â  + " \n "
Â  Â  Â  Â  + examples["body"]
Â  Â  Â  Â  + " \n "
Â  Â  Â  Â  + examples["comments"]
Â  Â  }


comments_dataset = comments_dataset.map(concatenate_text)
```

ë“œë””ì–´ ì„ë² ë”©ì„ ë§Œë“¤ ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤! ì‚´í´ë´…ì‹œë‹¤.

## í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±í•˜ê¸° (Creating text embeddings)

[ì±•í„° 2](/course/chapter2)ì—ì„œ **`AutoModel`** í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ í† í° ì„ë² ë”©ì„ ì–»ì„ ìˆ˜ ìˆìŒì„ ë³´ì•˜ìŠµë‹ˆë‹¤. ìš°ë¦¬ê°€ í•´ì•¼ í•  ì¼ì€ ëª¨ë¸ì„ ë¡œë“œí•  ì ì ˆí•œ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì„ íƒí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ë‹¤í–‰íˆ ì„ë² ë”© ìƒì„± ì „ìš©ì¸ **`sentence-transformers`**ë¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ìˆìŠµë‹ˆë‹¤. ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ [ë¬¸ì„œ](https://www.sbert.net/examples/applications/semantic-search/README.html#symmetric-vs-asymmetric-semantic-search)ì— ì„¤ëª…ëœ ëŒ€ë¡œ, ìš°ë¦¬ì˜ ì‚¬ìš© ì‚¬ë¡€ëŠ” ì§§ì€ ì¿¼ë¦¬ê°€ ìˆê³  ê·¸ ë‹µë³€ì„ ì´ìŠˆ ëŒ“ê¸€ê³¼ ê°™ì€ ë” ê¸´ ë¬¸ì„œì—ì„œ ì°¾ê³ ì í•˜ëŠ” *ë¹„ëŒ€ì¹­ ì‹œë§¨í‹± ê²€ìƒ‰(asymmetric semantic search)*ì˜ ì˜ˆì…ë‹ˆë‹¤. ë¬¸ì„œì˜ í¸ë¦¬í•œ [ëª¨ë¸ ê°œìš” í…Œì´ë¸”](https://www.sbert.net/docs/pretrained_models.html#model-overview)ì€ **`multi-qa-mpnet-base-dot-v1`** ì²´í¬í¬ì¸íŠ¸ê°€ ì‹œë§¨í‹± ê²€ìƒ‰ì— ëŒ€í•œ ìµœìƒì˜ ì„±ëŠ¥ì„ ê°€ì§€ê³  ìˆìŒì„ ë‚˜íƒ€ë‚´ë¯€ë¡œ, ì´ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì• í”Œë¦¬ì¼€ì´ì…˜ì— ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤. ë˜í•œ ë™ì¼í•œ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•  ê²ƒì…ë‹ˆë‹¤:

```py
from transformers import AutoTokenizer, AutoModel

model_ckpt = "sentence-transformers/multi-qa-mpnet-base-dot-v1"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
model = AutoModel.from_pretrained(model_ckpt)
```

ì„ë² ë”© í”„ë¡œì„¸ìŠ¤ ì†ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ ëª¨ë¸ê³¼ ì…ë ¥ì„ GPU ì¥ì¹˜ì— ë°°ì¹˜í•˜ëŠ” ê²ƒì´ ë„ì›€ì´ ë˜ë¯€ë¡œ, ì§€ê¸ˆ ë°”ë¡œ ê·¸ë ‡ê²Œ í•©ì‹œë‹¤:

```py
import torch

device = torch.device("cuda")
model.to(device)
```

ì´ì „ì— ì–¸ê¸‰í–ˆë“¯ì´, GitHub ì´ìŠˆ ì½”í¼ìŠ¤ì˜ ê° í•­ëª©ì„ ë‹¨ì¼ ë²¡í„°ë¡œ ë‚˜íƒ€ë‚´ê³ ì í•˜ë¯€ë¡œ, í† í° ì„ë² ë”©ì„ ì–´ë–¤ ë°©ì‹ìœ¼ë¡œë“  "í’€ë§"í•˜ê±°ë‚˜ í‰ê· í™”í•´ì•¼ í•©ë‹ˆë‹¤. í•œ ê°€ì§€ ì¸ê¸° ìˆëŠ” ì ‘ê·¼ ë°©ì‹ì€ ëª¨ë¸ ì¶œë ¥ì— **CLS í’€ë§**ì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì—¬ê¸°ì„œ íŠ¹ë³„í•œ **`[CLS]`** í† í°ì— ëŒ€í•œ ë§ˆì§€ë§‰ ì€ë‹‰ ìƒíƒœë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤. ë‹¤ìŒ í•¨ìˆ˜ê°€ ìš°ë¦¬ë¥¼ ìœ„í•´ ì´ íŠ¸ë¦­ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:

```py
def cls_pooling(model_output):
Â  Â  return model_output.last_hidden_state[:, 0]
```

ë‹¤ìŒìœ¼ë¡œ, í…ìŠ¤íŠ¸ ëª©ë¡ì„ í† í°í™”í•˜ê³ , í…ì„œë¥¼ GPUì— ë°°ì¹˜í•˜ê³ , ëª¨ë¸ì— ê³µê¸‰í•˜ê³ , ë§ˆì§€ë§‰ìœ¼ë¡œ ì¶œë ¥ì— CLS í’€ë§ì„ ì ìš©í•˜ëŠ” ë„ìš°ë¯¸ í•¨ìˆ˜ë¥¼ ë§Œë“¤ ê²ƒì…ë‹ˆë‹¤:

```py
def get_embeddings(text_list):
Â  Â  encoded_input = tokenizer(
Â  Â  Â  Â  text_list, padding=True, truncation=True, return_tensors="pt"
Â  Â  )
Â  Â  encoded_input = {k: v.to(device) for k, v in encoded_input.items()}
Â  Â  model_output = model(**encoded_input)
Â  Â  return cls_pooling(model_output)
```

ì½”í¼ìŠ¤ì˜ ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸ í•­ëª©ì„ ê³µê¸‰í•˜ê³  ì¶œë ¥ ëª¨ì–‘ì„ ê²€ì‚¬í•˜ì—¬ í•¨ìˆ˜ê°€ ì‘ë™í•˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```py
embedding = get_embeddings(comments_dataset["text"][0])
embedding.shape
```

```python out
torch.Size([1, 768])
```

í›Œë¥­í•©ë‹ˆë‹¤. ì½”í¼ìŠ¤ì˜ ì²« ë²ˆì§¸ í•­ëª©ì„ 768ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜í–ˆìŠµë‹ˆë‹¤! **`Dataset.map()`**ì„ ì‚¬ìš©í•˜ì—¬ **`get_embeddings()`** í•¨ìˆ˜ë¥¼ ì½”í¼ìŠ¤ì˜ ê° í–‰ì— ì ìš©í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ë‹¤ìŒê³¼ ê°™ì´ ìƒˆ **`embeddings`** ì—´ì„ ë§Œë“¤ì–´ ë´…ì‹œë‹¤:

```py
embeddings_dataset = comments_dataset.map(
Â  Â  lambda x: {"embeddings": get_embeddings(x["text"]).detach().cpu().numpy()[0]}
)
```

ìš°ë¦¬ê°€ ì„ë² ë”©ì„ NumPy ë°°ì—´ë¡œ ë³€í™˜í–ˆë‹¤ëŠ” ì ì— ìœ ì˜í•˜ì‹­ì‹œì˜¤. ì´ëŠ” ë‹¤ìŒìœ¼ë¡œ ìˆ˜í–‰í•  FAISSë¡œ ì„ë² ë”©ì„ ì¸ë±ì‹±í•˜ë ¤ê³  í•  ë•Œ ğŸ¤— Datasetsê°€ ì´ í˜•ì‹ì„ ìš”êµ¬í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.


## íš¨ìœ¨ì ì¸ ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìœ„í•œ FAISS ì‚¬ìš© (Using FAISS for efficient similarity search)

ì´ì œ ì„ë² ë”© ë°ì´í„°ì…‹ì´ ìˆìœ¼ë¯€ë¡œ, ì´ë¥¼ ê²€ìƒ‰í•  ë°©ë²•ì´ í•„ìš”í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ğŸ¤— Datasetsì˜ *FAISS ì¸ë±ìŠ¤*ë¼ëŠ” íŠ¹ìˆ˜ ë°ì´í„° êµ¬ì¡°ë¥¼ ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤. [FAISS](https://faiss.ai/) (Facebook AI Similarity Searchì˜ ì•½ì)ëŠ” ì„ë² ë”© ë²¡í„°ë¥¼ ë¹ ë¥´ê²Œ ê²€ìƒ‰í•˜ê³  í´ëŸ¬ìŠ¤í„°ë§í•˜ê¸° ìœ„í•œ íš¨ìœ¨ì ì¸ ì•Œê³ ë¦¬ì¦˜ì„ ì œê³µí•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.

FAISSì˜ ê¸°ë³¸ ì•„ì´ë””ì–´ëŠ” ì…ë ¥ ì„ë² ë”©ê³¼ ìœ ì‚¬í•œ ì„ë² ë”©ì„ ì°¾ì„ ìˆ˜ ìˆë„ë¡ í•˜ëŠ” *ì¸ë±ìŠ¤*ë¼ëŠ” íŠ¹ìˆ˜ ë°ì´í„° êµ¬ì¡°ë¥¼ ë§Œë“œëŠ” ê²ƒì…ë‹ˆë‹¤. ğŸ¤— Datasetsì—ì„œ FAISS ì¸ë±ìŠ¤ë¥¼ ë§Œë“œëŠ” ê²ƒì€ ê°„ë‹¨í•©ë‹ˆë‹¤. **`Dataset.add_faiss_index()`** í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ê³  ë°ì´í„°ì…‹ì˜ ì–´ë–¤ ì—´ì„ ì¸ë±ì‹±í• ì§€ ì§€ì •í•©ë‹ˆë‹¤:

```py
embeddings_dataset.add_faiss_index(column="embeddings")
```

ì´ì œ **`Dataset.get_nearest_examples()`** í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœê·¼ì ‘ ì´ì›ƒ ì¡°íšŒë¥¼ ìˆ˜í–‰í•˜ì—¬ ì´ ì¸ë±ìŠ¤ì— ëŒ€í•œ ì¿¼ë¦¬ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ìŒê³¼ ê°™ì´ ì§ˆë¬¸ì„ ë¨¼ì € ì„ë² ë”©í•˜ì—¬ í…ŒìŠ¤íŠ¸í•´ ë´…ì‹œë‹¤:

```py
question = "How can I load a dataset offline?"
question_embedding = get_embeddings([question]).cpu().detach().numpy()
question_embedding.shape
```

```python out
torch.Size([1, 768])
```

ë¬¸ì„œì—ì„œì™€ ë§ˆì°¬ê°€ì§€ë¡œ, ì´ì œ ì¿¼ë¦¬ë¥¼ ë‚˜íƒ€ë‚´ëŠ” 768ì°¨ì› ë²¡í„°ê°€ ìˆìœ¼ë©°, ì´ë¥¼ ì „ì²´ ì½”í¼ìŠ¤ì™€ ë¹„êµí•˜ì—¬ ê°€ì¥ ìœ ì‚¬í•œ ì„ë² ë”©ì„ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```py
scores, samples = embeddings_dataset.get_nearest_examples(
Â  Â  "embeddings", question_embedding, k=5
)
```

**`Dataset.get_nearest_examples()`** í•¨ìˆ˜ëŠ” ì¿¼ë¦¬ì™€ ë¬¸ì„œ ê°„ì˜ ê²¹ì¹¨ì„ ìˆœìœ„ ì§€ì •í•˜ëŠ” ì ìˆ˜ì™€ í•´ë‹¹ ìƒ˜í”Œ ì„¸íŠ¸(ì—¬ê¸°ì„œëŠ” ê°€ì¥ ì¼ì¹˜í•˜ëŠ” 5ê°œ)ì˜ íŠœí”Œì„ ë°˜í™˜í•©ë‹ˆë‹¤. ì´ë“¤ì„ **`pandas.DataFrame`**ì— ìˆ˜ì§‘í•˜ì—¬ ì‰½ê²Œ ì •ë ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```py
import pandas as pd

samples_df = pd.DataFrame.from_dict(samples)
samples_df["scores"] = scores
samples_df.sort_values("scores", ascending=False, inplace=True)
```

ì´ì œ ì²˜ìŒ ëª‡ í–‰ì„ ë°˜ë³µí•˜ì—¬ ì¿¼ë¦¬ê°€ ì‚¬ìš© ê°€ëŠ¥í•œ ëŒ“ê¸€ê³¼ ì–¼ë§ˆë‚˜ ì˜ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```py
for _, row in samples_df.iterrows():
Â  Â  print(f"COMMENT: {row.comments}")
Â  Â  print(f"SCORE: {row.scores}")
Â  Â  print(f"TITLE: {row.title}")
Â  Â  print(f"URL: {row.html_url}")
Â  Â  print("=" * 50)
Â  Â  print()
```

```python out
"""
COMMENT: Requiring online connection is a deal breaker in some cases unfortunately so it'd be great if offline mode is added similar to how `transformers` loads models offline fine.

@mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?
SCORE: 25.505046844482422
TITLE: Discussion using datasets in offline mode
URL: [https://github.com/huggingface/datasets/issues/824](https://github.com/huggingface/datasets/issues/824)
==================================================

COMMENT: The local dataset builders (csv, text , json and pandas) are now part of the `datasets` package since #1726 :)
You can now use them offline
\`\`\`python
datasets = load_dataset("text", data_files=data_files)
\`\`\`

We'll do a new release soon
SCORE: 24.555509567260742
TITLE: Discussion using datasets in offline mode
URL: [https://github.com/huggingface/datasets/issues/824](https://github.com/huggingface/datasets/issues/824)
==================================================

COMMENT: I opened a PR that allows to reload modules that have already been loaded once even if there's no internet.

Let me know if you know other ways that can make the offline mode experience better. I'd be happy to add them :)

I already note the "freeze" modules option, to prevent local modules updates. It would be a cool feature.

----------

> @mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?

Indeed `load_dataset` allows to load remote dataset script (squad, glue, etc.) but also you own local ones.
For example if you have a dataset script at `./my_dataset/my_dataset.py` then you can do
\`\`\`python
load_dataset("./my_dataset")
\`\`\`
and the dataset script will generate your dataset once and for all.

----------

About I'm looking into having `csv`, `json`, `text`, `pandas` dataset builders already included in the `datasets` package, so that they are available offline by default, as opposed to the other datasets that require the script to be downloaded.
cf #1724
SCORE: 24.14896583557129
TITLE: Discussion using datasets in offline mode
URL: [https://github.com/huggingface/datasets/issues/824](https://github.com/huggingface/datasets/issues/824)
==================================================

COMMENT: > here is my way to load a dataset offline, but it **requires** an online machine
>
> 1. (online machine)
>
> ```
>
> import datasets
>
> data = datasets.load_dataset(...)
>
> data.save_to_disk(/YOUR/DATASET/DIR)
>
> ```
>
> 2. copy the dir from online to the offline machine
>
> 3. (offline machine)
>
> ```
>
> import datasets
>
> data = datasets.load_from_disk(/SAVED/DATA/DIR)
>
> ```
>
>
>
> HTH.


SCORE: 22.893993377685547
TITLE: Discussion using datasets in offline mode
URL: [https://github.com/huggingface/datasets/issues/824](https://github.com/huggingface/datasets/issues/824)
==================================================

COMMENT: here is my way to load a dataset offline, but it **requires** an online machine
1. (online machine)
\`\`\`
import datasets
data = datasets.load_dataset(...)
data.save_to_disk(/YOUR/DATASET/DIR)
\`\`\`
2. copy the dir from online to the offline machine
3. (offline machine)
\`\`\`
import datasets
data = datasets.load_from_disk(/SAVED/DATA/DIR)
\`\`\`

HTH.
SCORE: 22.406635284423828
TITLE: Discussion using datasets in offline mode
URL: [https://github.com/huggingface/datasets/issues/824](https://github.com/huggingface/datasets/issues/824)
==================================================
"""
```

ë‚˜ì˜ì§€ ì•ŠìŠµë‹ˆë‹¤! ìš°ë¦¬ì˜ ë‘ ë²ˆì§¸ íˆíŠ¸ëŠ” ì¿¼ë¦¬ì™€ ì¼ì¹˜í•˜ëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤.

<Tip>

âœï¸ **ì§ì ‘ í•´ë³´ì„¸ìš”!** ìì‹ ë§Œì˜ ì¿¼ë¦¬ë¥¼ ë§Œë“¤ê³  ê²€ìƒ‰ëœ ë¬¸ì„œì—ì„œ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ìˆëŠ”ì§€ í™•ì¸í•´ ë³´ì„¸ìš”. ê²€ìƒ‰ì„ ë„“íˆê¸° ìœ„í•´ **`Dataset.get_nearest_examples()`**ì˜ **`k`** ë§¤ê°œë³€ìˆ˜ë¥¼ ëŠ˜ë ¤ì•¼ í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

</Tip>


<EditOnGithub source="https://github.com/huggingface/course/blob/main/chapters/en/chapter5/6.mdx" />
